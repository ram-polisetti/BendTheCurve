{
  "hash": "78c83747523ea6f26dff888e1bb70551",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Causal Inference and Structural Learning\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, causality, statistics, mathematics]\nimage: \"causal_inference.jpg\"\ndescription: \"A rigorous exploration of causal inference and structural learning, covering identification, estimation, and structural causal models.\"\njupyter: python3\n---\n\n\n\n\n# Causal Inference and Structural Learning\n\n## Structural Causal Models\n\n### 1. Basic Framework\n\nDefinition of SCM:\n\n$$\n\\begin{aligned}\nX_i &= f_i(\\text{PA}_i, U_i) \\\\\nU_i &\\sim P(U_i)\n\\end{aligned}\n$$\n\nWhere:\n- $X_i$ are endogenous variables\n- $\\text{PA}_i$ are parents of $X_i$\n- $U_i$ are exogenous variables\n- $f_i$ are structural equations\n\n### 2. Intervention Calculus\n\nDo-operator formalization:\n\n$$\nP(Y|\\text{do}(X=x)) = \\sum_z P(Y|X=x,Z=z)P(Z=z)\n$$\n\nBackdoor adjustment:\n\n$$\nP(Y|\\text{do}(X=x)) = \\sum_z P(Y|X=x,Z=z)P(Z=z)\n$$\n\nWhere Z satisfies the backdoor criterion.\n\n## Identification Methods\n\n### 1. Backdoor Criterion\n\nA set Z satisfies the backdoor criterion relative to (X,Y) if:\n1. No node in Z is a descendant of X\n2. Z blocks all backdoor paths from X to Y\n\nFormal criterion:\n\n$$\nP(Y|\\text{do}(X)) = \\sum_z P(Y|X,Z)P(Z)\n$$\n\n### 2. Front-door Criterion\n\nThree conditions:\n1. M blocks all directed paths from X to Y\n2. No unblocked backdoor path from X to M\n3. All backdoor paths from M to Y are blocked by X\n\nFormula:\n\n$$\nP(Y|\\text{do}(X)) = \\sum_m P(m|X)\\sum_{x'}P(Y|m,x')P(x')\n$$\n\n### 3. Do-Calculus Rules\n\nRule 1 (Insertion/deletion of observations):\n\n$$\nP(y|\\text{do}(x),z,w) = P(y|\\text{do}(x),w)\n$$\n\nif (Y ⊥⊥ Z|X,W)$_{G_{\\overline{X}}}$\n\nRule 2 (Action/observation exchange):\n\n$$\nP(y|\\text{do}(x),\\text{do}(z),w) = P(y|\\text{do}(x),z,w)\n$$\n\nif (Y ⊥⊥ Z|X,W)$_{G_{\\overline{X}\\underline{Z}}}$\n\nRule 3 (Insertion/deletion of actions):\n\n$$\nP(y|\\text{do}(x),\\text{do}(z),w) = P(y|\\text{do}(x),w)\n$$\n\nif (Y ⊥⊥ Z|X,W)$_{G_{\\overline{X}\\overline{Z(W)}}}$\n\n## Estimation Methods\n\n### 1. Propensity Score Matching\n\nPropensity score:\n\n$$\ne(X) = P(T=1|X)\n$$\n\nAverage Treatment Effect (ATE):\n\n$$\n\\text{ATE} = \\mathbb{E}[Y(1) - Y(0)] = \\mathbb{E}\\left[\\frac{TY}{e(X)} - \\frac{(1-T)Y}{1-e(X)}\\right]\n$$\n\n### 2. Instrumental Variables\n\nTwo-stage least squares (2SLS):\n\nFirst stage:\n$$\nX = \\gamma_0 + \\gamma_1Z + \\eta\n$$\n\nSecond stage:\n$$\nY = \\beta_0 + \\beta_1\\hat{X} + \\epsilon\n$$\n\n### 3. Regression Discontinuity\n\nSharp RD estimator:\n\n$$\n\\tau_{SRD} = \\lim_{x \\downarrow c} \\mathbb{E}[Y|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y|X=x]\n$$\n\nFuzzy RD estimator:\n\n$$\n\\tau_{FRD} = \\frac{\\lim_{x \\downarrow c} \\mathbb{E}[Y|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y|X=x]}{\\lim_{x \\downarrow c} \\mathbb{E}[D|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[D|X=x]}\n$$\n\n## Structural Learning\n\n### 1. Constraint-Based Methods\n\nPC Algorithm steps:\n1. Start with complete undirected graph\n2. Remove edges based on conditional independence\n3. Orient v-structures\n4. Orient remaining edges\n\nIndependence test statistic:\n\n$$\n\\chi^2 = n\\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n$$\n\n### 2. Score-Based Methods\n\nBIC score:\n\n$$\n\\text{BIC}(G) = \\ell(D|G) - \\frac{\\log n}{2}|G|\n$$\n\nWhere:\n- $\\ell(D|G)$ is log-likelihood\n- $|G|$ is model complexity\n- $n$ is sample size\n\n### 3. Hybrid Methods\n\nMMHC algorithm:\n1. Learn skeleton using constraint-based method\n2. Orient edges using score-based method\n\nScore function:\n\n$$\n\\text{Score}(G) = \\text{BIC}(G) + \\lambda \\text{Sparsity}(G)\n$$\n\n## Advanced Topics\n\n### 1. Counterfactual Analysis\n\nFundamental problem of causal inference:\n\n$$\n\\text{ACE} = \\mathbb{E}[Y(1) - Y(0)]\n$$\n\nBut we only observe:\n\n$$\nY = TY(1) + (1-T)Y(0)\n$$\n\n### 2. Mediation Analysis\n\nDirect and indirect effects:\n\n$$\n\\begin{aligned}\n\\text{NDE} &= \\mathbb{E}[Y(t,M(t'))] - \\mathbb{E}[Y(t',M(t'))] \\\\\n\\text{NIE} &= \\mathbb{E}[Y(t,M(t))] - \\mathbb{E}[Y(t,M(t'))]\n\\end{aligned}\n$$\n\n### 3. Time-Varying Treatments\n\nG-computation formula:\n\n$$\n\\mathbb{E}[Y_{\\bar{a}}] = \\sum_{\\bar{l}} \\prod_{t=0}^K P(l_t|l_{t-1},a_{t-1})P(y|\\bar{l},\\bar{a})\n$$\n\n## Implementation Considerations\n\n### 1. Sensitivity Analysis\n\nRosenbaum bounds:\n\n$$\n\\frac{1}{\\Gamma} \\leq \\frac{P(Z=1|X)P(Z=0|X')}{P(Z=0|X)P(Z=1|X')} \\leq \\Gamma\n$$\n\n### 2. Missing Data\n\nMultiple imputation:\n\n$$\n\\hat{\\theta} = \\frac{1}{M}\\sum_{m=1}^M \\hat{\\theta}_m\n$$\n\n### 3. Heterogeneous Effects\n\nConditional average treatment effect:\n\n$$\n\\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0)|X=x]\n$$\n\n## Best Practices\n\n### 1. Study Design\n\n1. Randomization:\n   - Complete randomization\n   - Stratified randomization\n   - Cluster randomization\n\n2. Sample Size:\n   - Power analysis\n   - Effect size estimation\n   - Variance components\n\n3. Measurement:\n   - Reliability\n   - Validity\n   - Missing data handling\n\n### 2. Analysis Strategy\n\n1. Identification:\n   - Check assumptions\n   - Sensitivity analysis\n   - Multiple methods\n\n2. Estimation:\n   - Robust methods\n   - Bootstrap\n   - Cross-validation\n\n3. Interpretation:\n   - Effect sizes\n   - Confidence intervals\n   - Multiple testing\n\n## Applications\n\n### 1. Economics\n\nProgram evaluation:\n- Treatment effects\n- Policy analysis\n- Market interventions\n\n### 2. Healthcare\n\nClinical trials:\n- Drug efficacy\n- Treatment comparison\n- Side effects\n\n### 3. Social Sciences\n\nPolicy research:\n- Educational interventions\n- Social programs\n- Behavioral studies\n\n## References\n\n1. Theory:\n   - \"Causality\" by Pearl\n   - \"Causal Inference in Statistics\" by Pearl et al.\n   - \"Elements of Causal Inference\" by Peters et al.\n\n2. Methods:\n   - \"Mostly Harmless Econometrics\" by Angrist and Pischke\n   - \"Counterfactuals and Causal Inference\" by Morgan and Winship\n   - \"Causal Inference for Statistics\" by Hernán and Robins\n\n3. Applications:\n   - \"Causal Machine Learning\" by Athey and Imbens\n   - \"The Book of Why\" by Pearl and Mackenzie\n   - \"Observation and Experiment\" by Rosenbaum\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}