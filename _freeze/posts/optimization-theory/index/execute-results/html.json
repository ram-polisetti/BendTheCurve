{
  "hash": "f7893f9fbff08379f78a08cce4d6cc4e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Optimization Theory in Machine Learning\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, optimization, mathematics, theory]\nimage: \"optimization_theory.jpg\"\ndescription: \"A rigorous exploration of optimization theory in machine learning, covering convex optimization, non-convex optimization, and modern algorithms.\"\njupyter: python3\n---\n\n\n\n\n# Optimization Theory in Machine Learning\n\n## Convex Analysis\n\n### 1. Convex Sets and Functions\n\nConvex set definition:\n\n$$\n\\theta x + (1-\\theta)y \\in C, \\forall x,y \\in C, \\theta \\in [0,1]\n$$\n\nConvex function:\n\n$$\nf(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y)\n$$\n\n### 2. Properties\n\nFirst-order characterization:\n\n$$\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x)\n$$\n\nSecond-order characterization:\n\n$$\n\\nabla^2 f(x) \\succeq 0\n$$\n\n### 3. Strong Convexity\n\nDefinition:\n\n$$\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu}{2}\\|y-x\\|^2\n$$\n\nQuadratic growth:\n\n$$\nf(x) - f(x^*) \\geq \\frac{\\mu}{2}\\|x-x^*\\|^2\n$$\n\n## Optimality Conditions\n\n### 1. First-Order Conditions\n\nUnconstrained:\n\n$$\n\\nabla f(x^*) = 0\n$$\n\nConstrained (KKT):\n\n$$\n\\begin{aligned}\n\\nabla_x \\mathcal{L}(x^*,\\lambda^*) &= 0 \\\\\ng_i(x^*) &\\leq 0 \\\\\n\\lambda_i^* g_i(x^*) &= 0 \\\\\n\\lambda_i^* &\\geq 0\n\\end{aligned}\n$$\n\n### 2. Second-Order Conditions\n\nUnconstrained:\n\n$$\n\\nabla^2 f(x^*) \\succeq 0\n$$\n\nConstrained:\n\n$$\ny^T\\nabla^2_{xx}\\mathcal{L}(x^*,\\lambda^*)y \\geq 0\n$$\n\n### 3. Saddle Point Conditions\n\nMinimax:\n\n$$\n\\mathcal{L}(x^*,\\lambda) \\leq \\mathcal{L}(x^*,\\lambda^*) \\leq \\mathcal{L}(x,\\lambda^*)\n$$\n\nDuality gap:\n\n$$\nf(x^*) - g(\\lambda^*) = 0\n$$\n\n## Gradient Methods\n\n### 1. Gradient Descent\n\nUpdate rule:\n\n$$\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n$$\n\nConvergence rate (convex):\n\n$$\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n$$\n\n### 2. Accelerated Methods\n\nNesterov's acceleration:\n\n$$\n\\begin{aligned}\ny_k &= x_k + \\beta_k(x_k - x_{k-1}) \\\\\nx_{k+1} &= y_k - \\eta_k\\nabla f(y_k)\n\\end{aligned}\n$$\n\nConvergence rate:\n\n$$\nf(x_k) - f(x^*) \\leq \\frac{2L\\|x_0-x^*\\|^2}{(k+1)^2}\n$$\n\n### 3. Stochastic Methods\n\nSGD update:\n\n$$\nx_{k+1} = x_k - \\eta_k\\nabla f_{i_k}(x_k)\n$$\n\nConvergence rate:\n\n$$\n\\mathbb{E}[f(x_k) - f(x^*)] \\leq \\frac{L\\|x_0-x^*\\|^2}{2k} + \\frac{L\\sigma^2}{2}\\sum_{t=1}^k \\eta_t^2\n$$\n\n## Non-Convex Optimization\n\n### 1. Local Minima\n\nFirst-order condition:\n\n$$\n\\|\\nabla f(x^*)\\| \\leq \\epsilon\n$$\n\nSecond-order condition:\n\n$$\n\\lambda_{\\min}(\\nabla^2 f(x^*)) \\geq -\\sqrt{\\epsilon}\n$$\n\n### 2. Escape from Saddle Points\n\nPerturbed gradient descent:\n\n$$\nx_{k+1} = x_k - \\eta\\nabla f(x_k) + \\xi_k\n$$\n\nWhere:\n- $\\xi_k \\sim \\mathcal{N}(0,\\sigma^2I)$\n\n### 3. Global Optimization\n\nBranch and bound:\n\n$$\n\\text{LB}(R) \\leq \\min_{x \\in R} f(x) \\leq \\text{UB}(R)\n$$\n\nSimulated annealing:\n\n$$\nP(\\text{accept}) = \\exp(-\\frac{\\Delta E}{T_k})\n$$\n\n## Modern Optimization Methods\n\n### 1. Adaptive Methods\n\nAdaGrad:\n\n$$\nx_{t+1,i} = x_{t,i} - \\frac{\\eta}{\\sqrt{\\sum_{s=1}^t g_{s,i}^2}}g_{t,i}\n$$\n\nAdam:\n\n$$\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\\nx_{t+1} &= x_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t\n\\end{aligned}\n$$\n\n### 2. Natural Gradient\n\nFisher information:\n\n$$\nF(x) = \\mathbb{E}_{p(y|x)}[\\nabla \\log p(y|x)\\nabla \\log p(y|x)^T]\n$$\n\nUpdate rule:\n\n$$\nx_{k+1} = x_k - \\eta F(x_k)^{-1}\\nabla f(x_k)\n$$\n\n### 3. Second-Order Methods\n\nNewton's method:\n\n$$\nx_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\n$$\n\nBFGS update:\n\n$$\nB_{k+1} = B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}\n$$\n\n## Constrained Optimization\n\n### 1. Projected Gradient\n\nUpdate rule:\n\n$$\nx_{k+1} = \\Pi_C(x_k - \\eta_k\\nabla f(x_k))\n$$\n\nConvergence rate:\n\n$$\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n$$\n\n### 2. Proximal Methods\n\nProximal operator:\n\n$$\n\\text{prox}_{\\eta g}(x) = \\arg\\min_y \\{g(y) + \\frac{1}{2\\eta}\\|y-x\\|^2\\}\n$$\n\nISTA update:\n\n$$\nx_{k+1} = \\text{prox}_{\\eta g}(x_k - \\eta\\nabla f(x_k))\n$$\n\n### 3. Augmented Lagrangian\n\nFunction:\n\n$$\n\\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\lambda^Tg(x) + \\frac{\\rho}{2}\\|g(x)\\|^2\n$$\n\nUpdate rules:\n\n$$\n\\begin{aligned}\nx_{k+1} &= \\arg\\min_x \\mathcal{L}_\\rho(x,\\lambda_k) \\\\\n\\lambda_{k+1} &= \\lambda_k + \\rho g(x_{k+1})\n\\end{aligned}\n$$\n\n## Advanced Topics\n\n### 1. Distributed Optimization\n\nADMM algorithm:\n\n$$\n\\begin{aligned}\nx_{k+1} &= \\arg\\min_x \\mathcal{L}_\\rho(x,z_k,y_k) \\\\\nz_{k+1} &= \\arg\\min_z \\mathcal{L}_\\rho(x_{k+1},z,y_k) \\\\\ny_{k+1} &= y_k + \\rho(Ax_{k+1} + Bz_{k+1} - c)\n\\end{aligned}\n$$\n\n### 2. Online Optimization\n\nRegret bound:\n\n$$\nR_T \\leq O(\\sqrt{T})\n$$\n\nFollow-the-regularized-leader:\n\n$$\nx_{t+1} = \\arg\\min_x \\{\\eta\\sum_{s=1}^t \\ell_s(x) + R(x)\\}\n$$\n\n### 3. Zeroth-Order Optimization\n\nGradient estimation:\n\n$$\n\\hat{\\nabla} f(x) = \\frac{d}{r}f(x+r\\xi)\\xi\n$$\n\nWhere:\n- $\\xi \\sim \\text{Unif}(\\mathbb{S}^{d-1})$\n\n## Best Practices\n\n### 1. Algorithm Selection\n\n1. Problem Structure:\n   - Convexity\n   - Smoothness\n   - Constraints\n\n2. Data Properties:\n   - Size\n   - Dimensionality\n   - Sparsity\n\n3. Computational Resources:\n   - Memory\n   - Processing power\n   - Time constraints\n\n### 2. Implementation\n\n1. Initialization:\n   - Parameter scaling\n   - Random seeding\n   - Warm start\n\n2. Monitoring:\n   - Convergence\n   - Stability\n   - Resource usage\n\n3. Tuning:\n   - Learning rates\n   - Momentum\n   - Regularization\n\n## References\n\n1. Theory:\n   - \"Convex Optimization\" by Boyd and Vandenberghe\n   - \"Introductory Lectures on Convex Optimization\" by Nesterov\n   - \"Optimization Methods for Large-Scale Machine Learning\" by Bottou et al.\n\n2. Methods:\n   - \"Numerical Optimization\" by Nocedal and Wright\n   - \"First-Order Methods in Optimization\" by Beck\n   - \"Proximal Algorithms\" by Parikh and Boyd\n\n3. Applications:\n   - \"Deep Learning\" by Goodfellow et al.\n   - \"Optimization for Machine Learning\" by Sra et al.\n   - \"Large Scale Optimization in Machine Learning\" by Shalev-Shwartz and Zhang\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}