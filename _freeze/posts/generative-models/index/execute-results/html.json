{
  "hash": "a92c408b1c9958128f7fd1b722cd4f53",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Generative Models: Mathematical Foundations and Architectures\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, generative-models, deep-learning, mathematics]\nimage: \"generative_models.jpg\"\ndescription: \"A rigorous mathematical exploration of generative models, including GANs, VAEs, and diffusion models.\"\njupyter: python3\n---\n\n\n\n\n# Generative Models: Mathematical Foundations\n\n## Variational Autoencoders (VAEs)\n\n### 1. Evidence Lower Bound (ELBO)\n\nThe VAE objective maximizes the ELBO:\n\n$$\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))\n$$\n\nWhere:\n- $q_\\phi(z|x)$ is the encoder (inference model)\n- $p_\\theta(x|z)$ is the decoder (generative model)\n- $p(z)$ is the prior distribution\n- $D_{KL}$ is the Kullback-Leibler divergence\n\n### 2. Reparameterization Trick\n\nEnables backpropagation through sampling:\n\n$$\nz = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n$$\n\nWhere:\n- $\\mu_\\phi(x)$ is the mean encoder network\n- $\\sigma_\\phi(x)$ is the standard deviation encoder network\n- $\\odot$ denotes element-wise multiplication\n\n## Generative Adversarial Networks (GANs)\n\n### 1. Minimax Objective\n\nThe original GAN formulation:\n\n$$\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\n$$\n\nWhere:\n- $G$ is the generator\n- $D$ is the discriminator\n- $p_{data}$ is the real data distribution\n- $p_z$ is the latent distribution\n\n### 2. Wasserstein Distance\n\nWGAN objective using Kantorovich-Rubinstein duality:\n\n$$\n\\min_G \\max_{D \\in \\mathcal{F}_L} \\mathbb{E}_{x\\sim p_{data}}[D(x)] - \\mathbb{E}_{z\\sim p_z}[D(G(z))]\n$$\n\nWhere:\n- $\\mathcal{F}_L$ is the set of 1-Lipschitz functions\n\n### 3. Gradient Penalty\n\nWGAN-GP regularization term:\n\n$$\n\\lambda \\mathbb{E}_{\\hat{x}\\sim p_{\\hat{x}}}[(\\|\\nabla_{\\hat{x}}D(\\hat{x})\\|_2 - 1)^2]\n$$\n\nWhere:\n- $\\hat{x}$ is sampled along straight lines between real and generated samples\n- $\\lambda$ is the penalty coefficient\n\n## Diffusion Models\n\n### 1. Forward Process\n\nThe forward diffusion process:\n\n$$\nq(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_tI)\n$$\n\nWith closed form for arbitrary timestep:\n\n$$\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)\n$$\n\nWhere:\n- $\\beta_t$ is the noise schedule\n- $\\alpha_t = 1-\\beta_t$\n- $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$\n\n### 2. Reverse Process\n\nThe reverse diffusion process:\n\n$$\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n$$\n\nTraining objective:\n\n$$\n\\mathcal{L} = \\mathbb{E}_{x_0,\\epsilon,t}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2\\right]\n$$\n\nWhere:\n- $\\epsilon_\\theta$ predicts the noise component\n- $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$\n\n## Advanced Architectures\n\n### 1. Normalizing Flows\n\nChange of variables formula:\n\n$$\n\\log p_X(x) = \\log p_Z(f^{-1}(x)) + \\log\\left|\\det\\frac{\\partial f^{-1}}{\\partial x}\\right|\n$$\n\nWhere:\n- $f$ is an invertible transformation\n- $p_Z$ is a simple base distribution\n\n### 2. Autoregressive Models\n\nFactorized probability:\n\n$$\np(x) = \\prod_{i=1}^n p(x_i|x_{<i})\n$$\n\nWith masked convolutions:\n\n$$\ny_i = \\sum_{j \\leq i} m_{ij}(w_{ij} \\cdot x_j)\n$$\n\n### 3. Energy-Based Models\n\nProbability density:\n\n$$\np(x) = \\frac{1}{Z}e^{-E(x)}\n$$\n\nWhere:\n- $E(x)$ is the energy function\n- $Z = \\int e^{-E(x)}dx$ is the partition function\n\n## Training Dynamics\n\n### 1. Mode Collapse in GANs\n\nJensen-Shannon divergence:\n\n$$\nJSD(P\\|Q) = \\frac{1}{2}D_{KL}(P\\|\\frac{P+Q}{2}) + \\frac{1}{2}D_{KL}(Q\\|\\frac{P+Q}{2})\n$$\n\n### 2. VAE Posterior Collapse\n\nKL-divergence analysis:\n\n$$\nD_{KL}(q_\\phi(z|x)\\|p(z)) = \\frac{1}{2}\\sum_{j=1}^d(\\sigma_j^2 + \\mu_j^2 - \\log\\sigma_j^2 - 1)\n$$\n\n### 3. Diffusion Model Training\n\nDenoising score matching:\n\n$$\n\\nabla_x \\log p(x) = \\mathbb{E}_{p(t|x)}[\\nabla_x \\log p(x|x_t)]\n$$\n\n## Advanced Training Techniques\n\n### 1. Progressive Growing\n\nResolution-dependent loss:\n\n$$\n\\mathcal{L}_\\text{total} = \\sum_{r} \\alpha_r \\mathcal{L}_r\n$$\n\nWhere:\n- $r$ is the resolution level\n- $\\alpha_r$ is the weighting factor\n\n### 2. Style Mixing\n\nStyle transfer in latent space:\n\n$$\nw = \\mathcal{M}(z) = f(z + \\Delta z)\n$$\n\nWhere:\n- $\\mathcal{M}$ is the mapping network\n- $f$ is a non-linear transformation\n\n### 3. Adaptive Instance Normalization (AdaIN)\n\nStyle transfer operation:\n\n$$\n\\text{AdaIN}(x,y) = \\sigma(y)\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right) + \\mu(y)\n$$\n\n## Evaluation Metrics\n\n### 1. Inception Score\n\nMeasures quality and diversity:\n\n$$\nIS = \\exp(\\mathbb{E}_{x\\sim p_g}[D_{KL}(p(y|x)\\|p(y))])\n$$\n\n### 2. FrÃ©chet Inception Distance\n\nDistribution similarity metric:\n\n$$\nFID = \\|\\mu_r - \\mu_g\\|^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r\\Sigma_g)^{1/2})\n$$\n\nWhere:\n- $\\mu_r, \\Sigma_r$ are real data statistics\n- $\\mu_g, \\Sigma_g$ are generated data statistics\n\n### 3. Precision and Recall\n\nTwo-way evaluation:\n\n$$\n\\begin{aligned}\n\\text{Precision} &= \\mathbb{E}_{x\\sim p_g}[\\max_{y\\sim p_r} s(x,y)] \\\\\n\\text{Recall} &= \\mathbb{E}_{y\\sim p_r}[\\max_{x\\sim p_g} s(x,y)]\n\\end{aligned}\n$$\n\n## Implementation Considerations\n\n### 1. Architectural Choices\n\n1. Generator Design:\n   - Transposed convolutions vs upsampling\n   - Skip connections\n   - Attention mechanisms\n\n2. Discriminator Design:\n   - Spectral normalization\n   - Residual blocks\n   - Multi-scale discrimination\n\n3. Loss Functions:\n   - Adversarial loss\n   - Reconstruction loss\n   - Perceptual loss\n\n### 2. Training Stability\n\n1. Gradient Penalties:\n   - R1 regularization\n   - Path length regularization\n   - Consistency regularization\n\n2. Learning Rate:\n   - Two time-scale update rule\n   - Adaptive learning rates\n   - Warmup scheduling\n\n3. Batch Size:\n   - Gradient accumulation\n   - Mixed precision training\n   - Memory-efficient backprop\n\n## Best Practices\n\n### 1. Model Selection\n\n1. VAEs for:\n   - Structured latent spaces\n   - Reconstruction tasks\n   - Interpretable representations\n\n2. GANs for:\n   - High-quality generation\n   - Style transfer\n   - Domain translation\n\n3. Diffusion Models for:\n   - High-fidelity generation\n   - Controlled generation\n   - Robust training\n\n### 2. Hyperparameter Tuning\n\n1. Learning Rates:\n   - Generator: 1e-4 to 1e-3\n   - Discriminator: 2e-4 to 2e-3\n   - VAE: 1e-3 to 1e-2\n\n2. Batch Sizes:\n   - GANs: 32 to 128\n   - VAEs: 64 to 256\n   - Diffusion: 32 to 64\n\n3. Architecture:\n   - Layer depth\n   - Channel width\n   - Attention layers\n\n## References\n\n1. Theory:\n   - \"Auto-Encoding Variational Bayes\" by Kingma and Welling\n   - \"Generative Adversarial Networks\" by Goodfellow et al.\n   - \"Denoising Diffusion Probabilistic Models\" by Ho et al.\n\n2. Architecture:\n   - \"Progressive Growing of GANs\" by Karras et al.\n   - \"StyleGAN\" by Karras et al.\n   - \"Normalizing Flows\" by Rezende and Mohamed\n\n3. Training:\n   - \"Improved Training of Wasserstein GANs\" by Gulrajani et al.\n   - \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\" by Brock et al.\n   - \"Improved VQGAN for Image Generation\" by Esser et al.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}