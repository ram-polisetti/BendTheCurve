{
  "hash": "4160b236c45fdcfd665ca4250edf9ebc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Getting Started with PyTorch\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [deep-learning, python, tutorial, machine-learning, data-science]\nimage: \"pytorch_logo.png\"\ndescription: \"An introduction to deep learning with PyTorch, covering basic concepts and building your first neural network.\"\n---\n\n\n# Getting Started with Deep Learning using PyTorch\n\nIn this post, we'll dive into the basics of deep learning using PyTorch. We'll build a simple neural network and understand the fundamental concepts of deep learning.\n\n## Setup\n\n::: {#2a1e5f31 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set plot style\nplt.style.use('default')\nsns.set_theme()\n\n# Generate synthetic data\nn_samples = 1000\nX = np.random.uniform(-5, 5, (n_samples, 1))\ny = 2 * np.sin(X) + np.random.normal(0, 0.2, (n_samples, 1))\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n```\n:::\n\n\n## Building the Neural Network\n\n::: {#c67f4f68 .cell execution_count=2}\n``` {.python .cell-code}\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 64)\n        self.layer3 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.layer3(x)\n        return x\n\n# Create the model\nmodel = SimpleNN()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n```\n:::\n\n\n## Training the Model\n\n::: {#f0e6c1e8 .cell execution_count=3}\n``` {.python .cell-code}\n# Training loop\nepochs = 100\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.title('Training Loss Over Time')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [10/100], Loss: 0.1010\nEpoch [20/100], Loss: 0.0901\nEpoch [30/100], Loss: 0.0706\nEpoch [40/100], Loss: 0.0683\nEpoch [50/100], Loss: 0.0631\nEpoch [60/100], Loss: 0.0613\nEpoch [70/100], Loss: 0.0600\nEpoch [80/100], Loss: 0.0587\nEpoch [90/100], Loss: 0.0576\nEpoch [100/100], Loss: 0.0564\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=857 height=473}\n:::\n:::\n\n\n## Visualizing the Results\n\n::: {#90699536 .cell execution_count=4}\n``` {.python .cell-code}\n# Generate predictions\nmodel.eval()\nwith torch.no_grad():\n    X_test = torch.FloatTensor(np.linspace(-6, 6, 200).reshape(-1, 1))\n    y_pred = model(X_test)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, alpha=0.5, label='Data')\nplt.plot(X_test, y_pred, 'r', label='Model Prediction')\nplt.plot(X_test, 2 * np.sin(X_test), 'g--', label='True Function')\nplt.legend()\nplt.title('Neural Network Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=846 height=550}\n:::\n:::\n\n\n## Understanding the Model\n\nLet's analyze what our neural network has learned:\n\n1. The network architecture consists of:\n   - Input layer (1 neuron)\n   - Two hidden layers (64 neurons each)\n   - Output layer (1 neuron)\n   - ReLU activation functions\n\n2. Training process:\n   - Used Mean Squared Error loss\n   - Adam optimizer\n   - 100 epochs of training\n\n3. Results:\n   - The model successfully learned the underlying sinusoidal pattern\n   - Handles noise in the data well\n   - Generalizes to unseen data points\n\n## Key Takeaways\n\nIn this tutorial, we learned:\n\n1. How to create a neural network using PyTorch\n2. Basic concepts of:\n   - Network architecture\n   - Forward and backward propagation\n   - Loss functions and optimization\n3. Visualization of training progress and results\n\nNext Steps:\n- Exploring more complex architectures\n- Understanding different activation functions\n- Working with real-world datasets\n- Implementing different types of neural networks (CNNs, RNNs)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}