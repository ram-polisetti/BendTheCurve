{
  "hash": "aa460fbbe9d0e09be27a31447d9479bb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Preprocessing in Python\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [data-science, python, tutorial, machine-learning]\nimage: \"preprocessing.png\"\ndescription: \"A comprehensive guide to data preprocessing techniques in Python, covering handling missing values, scaling, and feature engineering.\"\n---\n\n\n# Data Preprocessing in Python\n\nData preprocessing is a crucial step in any data science project. In this post, we'll explore common preprocessing techniques and how to implement them using Python's popular data science libraries.\n\n## Setup and Sample Data\n\n::: {#f0489d63 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed and plot style\nnp.random.seed(42)\nplt.style.use('default')\nsns.set_theme()\n\n# Create sample dataset with common preprocessing challenges\nn_samples = 1000\ndata = {\n    'age': np.random.normal(35, 10, n_samples),\n    'income': np.random.lognormal(10, 1, n_samples),\n    'education_years': np.random.randint(8, 22, n_samples),\n    'job_category': np.random.choice(['Tech', 'Finance', 'Healthcare', 'Other'], n_samples),\n    'satisfaction_score': np.random.randint(1, 11, n_samples)\n}\n\n# Add some missing values\ndf = pd.DataFrame(data)\ndf.loc[np.random.choice(n_samples, 100), 'income'] = np.nan\ndf.loc[np.random.choice(n_samples, 50), 'education_years'] = np.nan\ndf.loc[np.random.choice(n_samples, 30), 'job_category'] = np.nan\n\n# Display initial data info\nprint(\"Initial Dataset Info:\")\nprint(df.info())\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial Dataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   age                 1000 non-null   float64\n 1   income              908 non-null    float64\n 2   education_years     950 non-null    float64\n 3   job_category        971 non-null    object \n 4   satisfaction_score  1000 non-null   int64  \ndtypes: float64(3), int64(1), object(1)\nmemory usage: 39.2+ KB\nNone\n\nMissing Values:\nage                    0\nincome                92\neducation_years       50\njob_category          29\nsatisfaction_score     0\ndtype: int64\n```\n:::\n:::\n\n\n## Handling Missing Values\n\n::: {#0829104e .cell execution_count=2}\n``` {.python .cell-code}\n# Function to handle missing values\ndef handle_missing_values(df):\n    # Numeric columns: fill with median\n    numeric_imputer = SimpleImputer(strategy='median')\n    df[['income', 'education_years']] = numeric_imputer.fit_transform(df[['income', 'education_years']])\n    \n    # Categorical columns: fill with mode\n    categorical_imputer = SimpleImputer(strategy='most_frequent')\n    df[['job_category']] = categorical_imputer.fit_transform(df[['job_category']])\n    \n    return df\n\n# Handle missing values\ndf_clean = df.copy()\ndf_clean = handle_missing_values(df_clean)\n\nprint(\"\\nMissing Values After Imputation:\")\nprint(df_clean.isnull().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMissing Values After Imputation:\nage                   0\nincome                0\neducation_years       0\njob_category          0\nsatisfaction_score    0\ndtype: int64\n```\n:::\n:::\n\n\n## Feature Scaling\n\n::: {#78211175 .cell execution_count=3}\n``` {.python .cell-code}\n# Scale numeric features\nscaler = StandardScaler()\nnumeric_cols = ['age', 'income', 'education_years']\ndf_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n\n# Visualize scaled features\nplt.figure(figsize=(15, 5))\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df_clean[col], bins=30, kde=True)\n    plt.title(f'Distribution of Scaled {col}')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=1484 height=484}\n:::\n:::\n\n\n## Categorical Encoding\n\n::: {#7138bc43 .cell execution_count=4}\n``` {.python .cell-code}\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndf_clean['job_category_encoded'] = label_encoder.fit_transform(df_clean['job_category'])\n\n# Visualize categorical distribution\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_clean, x='job_category')\nplt.title('Distribution of Job Categories')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=854 height=522}\n:::\n:::\n\n\n## Feature Engineering\n\n::: {#8b52e74e .cell execution_count=5}\n``` {.python .cell-code}\n# Create new features\ndf_clean['income_per_education'] = np.exp(df_clean['income']) / (df_clean['education_years'] + 1)  # Add 1 to avoid division by zero\ndf_clean['is_high_satisfaction'] = (df_clean['satisfaction_score'] >= 8).astype(int)\n\n# Visualize engineered features\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(data=df_clean, x='job_category', y='income_per_education')\nplt.title('Income per Education Year by Job Category')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=df_clean, x='job_category', y='is_high_satisfaction')\nplt.title('High Satisfaction Rate by Job Category')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=1184 height=484}\n:::\n:::\n\n\n## Key Takeaways\n\nIn this tutorial, we learned essential data preprocessing techniques:\n\n1. Handling missing values using imputation\n2. Scaling numeric features\n3. Encoding categorical variables\n4. Creating new features through feature engineering\n\nThese techniques are crucial for preparing your data for machine learning models. In future posts, we'll explore:\n- Advanced feature engineering techniques\n- Handling imbalanced datasets\n- Dealing with outliers\n- Automated feature selection\n\nStay tuned!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}