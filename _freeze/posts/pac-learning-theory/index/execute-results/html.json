{
  "hash": "54bb8a16180d5e67e383bac1f292501d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PAC Learning Theory and VC Dimension\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, theory, mathematics, statistics]\nimage: \"pac_learning.jpg\"\ndescription: \"A beginner-friendly guide to PAC learning theory and VC dimension with interactive visualizations and practical examples.\"\njupyter: python3\n---\n\n\n# PAC Learning Theory and VC Dimension\n\n:::{.callout-note}\n## Learning Objectives\nBy the end of this article, you will:\n1. Understand PAC learning intuitively and mathematically\n2. Visualize VC dimension in practice\n3. Calculate sample complexity for real problems\n4. Implement PAC learning algorithms\n5. Apply VC theory to model selection\n:::\n\n## Introduction\n\nImagine you're teaching a robot to recognize apples . How can you be \"probably approximately correct\" about its ability to recognize any apple? PAC learning theory gives us the mathematical framework to answer such questions.\n\n::: {#8e32648e .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification, make_circles\nfrom sklearn.model_selection import learning_curve\nimport time\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n```\n:::\n\n\n## 1. PAC Learning Visualization\n\nLet's visualize what \"probably approximately correct\" means:\n\n::: {#c24ec661 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\ndef visualize_pac_learning(n_samples=100, noise_level=0.1):\n    # Generate synthetic dataset\n    X, y = make_circles(n_samples=n_samples, noise=noise_level, factor=0.3)\n    \n    # Train models with different sample sizes\n    sample_sizes = [10, 30, 50, n_samples]\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    axes = axes.ravel()\n    \n    for i, size in enumerate(sample_sizes):\n        # Train model on subset\n        model = SVC(kernel='rbf')\n        idx = np.random.choice(n_samples, size=size, replace=False)\n        model.fit(X[idx], y[idx])\n        \n        # Create grid for decision boundary\n        xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100),\n                            np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100))\n        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        \n        # Plot\n        axes[i].contourf(xx, yy, Z, alpha=0.4)\n        axes[i].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n        axes[i].set_title(f'Training samples: {size}')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_pac_learning()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1142 height=1141}\n:::\n:::\n\n\n:::{.callout-tip}\n## Key Insight\nNotice how the decision boundary becomes more stable and accurate as we increase the sample size. This is PAC learning in action!\n:::\n\n## 2. VC Dimension Explorer\n\nLet's create an interactive tool to explore VC dimension:\n\n::: {#b128207c .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\ndef explore_vc_dimension(n_points=100):\n    def generate_points(n):\n        return np.random.rand(n, 2)\n    \n    def plot_linear_classifier(ax, points, labels):\n        if len(points) >= 2:\n            model = SVC(kernel='linear')\n            try:\n                model.fit(points, labels)\n                \n                # Plot decision boundary\n                xx, yy = np.meshgrid(np.linspace(0, 1, 100),\n                                   np.linspace(0, 1, 100))\n                Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                ax.contourf(xx, yy, Z, alpha=0.4)\n            except:\n                pass\n        \n        # Plot points\n        colors = ['red' if l == 0 else 'blue' for l in labels]\n        ax.scatter(points[:, 0], points[:, 1], c=colors)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n    \n    # Generate different labelings\n    points = generate_points(3)  # Try with 3 points\n    all_labels = [[int(i) for i in format(j, f'0{3}b')] \n                 for j in range(2**3)]\n    \n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    axes = axes.ravel()\n    \n    for i, labels in enumerate(all_labels):\n        plot_linear_classifier(axes[i], points, labels)\n        axes[i].set_title(f'Labeling {i+1}')\n    \n    plt.tight_layout()\n    plt.show()\n\nexplore_vc_dimension()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=1526 height=757}\n:::\n:::\n\n\n:::{.callout-note}\n## Understanding VC Dimension\nThe plots above show different possible labelings of 3 points. A linear classifier (VC dimension = 3) can shatter these points in most, but not all configurations.\n:::\n\n## Theoretical Foundations\n\n### 1. PAC Learning Framework\n\nThe PAC (Probably Approximately Correct) learning framework provides theoretical guarantees for learning algorithms:\n\n$$\nP_{S \\sim \\mathcal{D}^m}(\\text{error}_\\mathcal{D}(h_S) \\leq \\epsilon) \\geq 1-\\delta\n$$\n\nWhere:\n- $\\epsilon$ is the accuracy parameter (how close to perfect)\n- $\\delta$ is the confidence parameter (how sure we are)\n- $m$ is the sample size\n- $h_S$ is the learned hypothesis\n\n### 2. Sample Complexity\n\nThe fundamental bound for sample complexity:\n\n$$\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{1}{\\delta}\\right)\n$$\n\nFor the realizable case (when perfect classification is possible):\n\n$$\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln\\frac{1}{\\delta}\\right)\n$$\n\n### 3. VC Dimension Theory\n\nThe VC dimension of a hypothesis class $\\mathcal{H}$ is the largest number of points that can be shattered (assigned any possible labeling) by $\\mathcal{H}$.\n\nGrowth function:\n\n$$\n\\Pi_\\mathcal{H}(m) = \\max_{x_1,...,x_m \\in \\mathcal{X}}|\\{(h(x_1),...,h(x_m)): h \\in \\mathcal{H}\\}|\n$$\n\nSauer's Lemma:\n\n$$\n\\text{If VC}(\\mathcal{H}) = d, \\text{ then } \\Pi_\\mathcal{H}(m) \\leq \\sum_{i=0}^d \\binom{m}{i}\n$$\n\n### 4. Generalization Bounds\n\nThe fundamental theorem of learning theory:\n\n$$\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| > \\epsilon) \\leq 4\\Pi_\\mathcal{H}(2m)\\exp(-\\frac{m\\epsilon^2}{8})\n$$\n\nSample complexity in terms of VC dimension:\n\n$$\nm = O\\left(\\frac{d}{\\epsilon^2}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon^2}\\ln\\frac{1}{\\delta}\\right)\n$$\n\n:::{.callout-note}\n## Key Insight\nThe VC dimension ($d$) appears in the sample complexity bound, showing how model complexity affects learning guarantees.\n:::\n\n## PAC Learning Framework\n\n### 1. Basic Definitions\n\nLet's make PAC learning concrete with an example:\n\n::: {#1c007355 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nclass PACLearner:\n    def __init__(self, epsilon=0.1, delta=0.05):\n        self.epsilon = epsilon  # accuracy parameter\n        self.delta = delta    # confidence parameter\n        self.model = None\n    \n    def required_samples(self, vc_dim):\n        \"\"\"Calculate required sample size using VC bound\"\"\"\n        return int(np.ceil((8/self.epsilon) * \n                         (2*vc_dim * np.log2(16/self.epsilon) + \n                          np.log2(2/self.delta))))\n    \n    def fit(self, X, y):\n        \"\"\"Train model with PAC guarantees\"\"\"\n        n_samples = len(X)\n        required = self.required_samples(vc_dim=3)  # for linear classifier\n        \n        if n_samples < required:\n            print(f\"Warning: Need at least {required} samples for PAC guarantees\")\n        \n        self.model = SVC(kernel='linear')\n        self.model.fit(X, y)\n        return self\n    \n    def predict(self, X):\n        return self.model.predict(X)\n\n# Example usage\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n                          n_informative=2, random_state=42)\nlearner = PACLearner(epsilon=0.1, delta=0.05)\nprint(f\"Required samples: {learner.required_samples(vc_dim=3)}\")\nlearner.fit(X, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequired samples: 3941\nWarning: Need at least 3941 samples for PAC guarantees\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<__main__.PACLearner at 0x16c51beb0>\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Practical PAC Learning\n1. Choose your desired accuracy (ε) and confidence (δ)\n2. Calculate required sample size using VC dimension\n3. Collect enough samples to meet PAC guarantees\n4. Train your model on the collected samples\n:::\n\n## Advanced Topics\n\n### 1. Rademacher Complexity\n\nRademacher complexity measures the richness of a hypothesis class:\n\n$$\n\\mathfrak{R}_S(\\mathcal{H}) = \\mathbb{E}_\\sigma\\left[\\sup_{h \\in \\mathcal{H}}\\frac{1}{m}\\sum_{i=1}^m \\sigma_i h(x_i)\\right]\n$$\n\n### 2. Agnostic PAC Learning\n\nFor the non-realizable case:\n\n$$\n\\text{error}_\\mathcal{D}(h) \\leq \\min_{h' \\in \\mathcal{H}}\\text{error}_\\mathcal{D}(h') + \\epsilon\n$$\n\nSample complexity:\n\n$$\nm \\geq \\frac{2}{\\epsilon^2}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{2}{\\delta}\\right)\n$$\n\n### 3. Structural Risk Minimization\n\nFor nested hypothesis classes $\\mathcal{H}_1 \\subset \\mathcal{H}_2 \\subset ... \\subset \\mathcal{H}_k$:\n\n$$\n\\text{pen}(h) = \\sqrt{\\frac{\\text{VC}(\\mathcal{H}(h))\\ln(em/\\text{VC}(\\mathcal{H}(h))) + \\ln(1/\\delta)}{m}}\n$$\n\n:::{.callout-tip}\n## Practical Application\nUse structural risk minimization to automatically select model complexity based on your dataset size.\n:::\n\n## Practical Implementation\n\nHere's a complete example of PAC learning in practice:\n\n\n#| code-fold: false\nclass MemoryEfficientLearner:\n    def __init__(self, max_memory=1000):\n        self.max_memory = max_memory\n        self.model = None\n    \n    def fit_with_memory_constraint(self, X, y):\n        n_samples = len(X)\n        batch_size = min(self.max_memory, n_samples)\n        \n        # Simulate streaming learning\n        times = []\n        memories = []\n        accuracies = []\n        \n        for batch_end in range(batch_size, n_samples + batch_size, batch_size):\n            batch_start = batch_end - batch_size\n            X_batch = X[batch_start:batch_end]\n            y_batch = y[batch_start:batch_end]\n            \n            start_time = time.time()\n            if self.model is None:\n                self.model = SVC(kernel='linear')\n            self.model.fit(X_batch, y_batch)\n            \n            times.append(time.time() - start_time)\n            memories.append(batch_size * X.shape[1] * 8)  # Approximate memory in bytes\n            accuracies.append(self.model.score(X_batch, y_batch))\n        \n        return times, memories, accuracies\n\n# Generate synthetic dataset with proper parameters\nX, y = make_classification(\n    n_samples=2000,\n    n_features=2,\n    n_informative=2,    # All features are informative\n    n_redundant=0,      # No redundant features\n    n_repeated=0,       # No repeated features\n    n_classes=2,\n    random_state=42\n)\n\n# Scale the features for better SVM performance\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Create and train the memory-efficient learner\nlearner = MemoryEfficientLearner(max_memory=500)\ntimes, memories, accuracies = learner.fit_with_memory_constraint(X, y)\n\n# Create subplots with proper spacing\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Space-Time Tradeoff\nax1.plot(memories, times, 'go-', linewidth=2, markersize=8)\nax1.set_xlabel('Memory usage (bytes)')\nax1.set_ylabel('Training time (seconds)')\nax1.set_title('Space-Time Tradeoff')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Memory vs Accuracy\nax2.plot(memories, accuracies, 'bo-', linewidth=2, markersize=8)\nax2.set_xlabel('Memory usage (bytes)')\nax2.set_ylabel('Accuracy')\nax2.set_title('Memory-Accuracy Tradeoff')\nax2.grid(True, alpha=0.3)\n\n# Improve plot aesthetics\nplt.tight_layout()\nfor ax in [ax1, ax2]:\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    \nplt.show()\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}