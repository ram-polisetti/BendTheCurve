{
  "hash": "37ef68c583d50e377ab9de663af5b26e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Getting Started with Scikit-learn\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, python, tutorial, data-science]\nimage: \"sklearn_logo.png\"\ndescription: \"A beginner's guide to machine learning with scikit-learn, covering basic concepts and implementing your first ML model.\"\n---\n\n\n# Introduction to Machine Learning with Scikit-learn\n\nIn this post, we'll explore the basics of machine learning using Python's scikit-learn library. We'll build a simple classification model and understand the fundamental ML workflow.\n\n## Setup\n\n::: {#415e6678 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plot style\nplt.style.use('default')\nsns.set_theme()\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 1000\n\n# Create features\nX = np.random.normal(size=(n_samples, 2))\n# Create two clusters\ny = (X[:, 0]**2 + X[:, 1]**2 > 2).astype(int)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\n## Data Preprocessing\n\n::: {#2a49cc6b .cell execution_count=2}\n``` {.python .cell-code}\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Look at the distribution of scaled features\nplt.figure(figsize=(10, 5))\nsns.histplot(X_train_scaled[:, 0], bins=30, kde=True)\nplt.title('Distribution of Scaled Feature 1')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=846 height=473}\n:::\n:::\n\n\n## Training the Model\n\n::: {#a647466f .cell execution_count=3}\n``` {.python .cell-code}\n# Create and train the model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = rf_model.predict(X_test_scaled)\n\n# Print the results\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       133\n           1       0.92      0.99      0.95        67\n\n    accuracy                           0.96       200\n   macro avg       0.95      0.97      0.96       200\nweighted avg       0.97      0.96      0.97       200\n\n```\n:::\n:::\n\n\n## Visualizing the Results\n\n::: {#58f9e6e7 .cell execution_count=4}\n``` {.python .cell-code}\n# Create a mesh grid to visualize the decision boundary\ndef plot_decision_boundary(X, y, model, scaler):\n    h = 0.02  # Step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Scale the mesh points\n    mesh_points = scaler.transform(np.c_[xx.ravel(), yy.ravel()])\n    \n    # Make predictions\n    Z = model.predict(mesh_points)\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the decision boundary\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n    plt.title('Random Forest Decision Boundary')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    return plt.gcf()\n\n# Plot the decision boundary\nplot_decision_boundary(X_test, y_test, rf_model, scaler)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=846 height=704}\n:::\n:::\n\n\n## Key Takeaways\n\nIn this tutorial, we learned:\n\n1. How to prepare data for machine learning\n2. Basic usage of scikit-learn's RandomForestClassifier\n3. Model evaluation using classification metrics\n4. Visualization of model decision boundaries\n\nThis is just the beginning of our machine learning journey. In future posts, we'll explore:\n- More advanced algorithms\n- Feature engineering techniques\n- Model tuning and optimization\n- Real-world applications\n\nStay tuned!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}