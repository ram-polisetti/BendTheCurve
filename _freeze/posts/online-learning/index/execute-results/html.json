{
  "hash": "f66fde8dfc0de9f1c138faca2ddadb2e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Online Learning and Regret Minimization\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, theory, mathematics, online-learning]\nimage: \"online_learning.jpg\"\ndescription: \"A rigorous exploration of online learning theory and regret minimization, covering fundamental algorithms, bounds, and applications.\"\njupyter: python3\n---\n\n\n\n\n# Online Learning and Regret Minimization\n\n## Fundamental Concepts\n\n### 1. Online Learning Protocol\n\nLearning process:\n1. Receive instance $x_t$\n2. Predict $\\hat{y}_t$\n3. Observe true outcome $y_t$\n4. Suffer loss $\\ell({\\hat{y}_t, y_t})$\n5. Update model\n\nRegret definition:\n\n$$\nR_T = \\sum_{t=1}^T \\ell(h_t(x_t), y_t) - \\min_{h \\in \\mathcal{H}}\\sum_{t=1}^T \\ell(h(x_t), y_t)\n$$\n\n### 2. Types of Regret\n\nExternal regret:\n\n$$\nR_T^{\\text{ext}} = \\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a_t)\\right] - \\min_{a \\in \\mathcal{A}}\\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a)\\right]\n$$\n\nInternal/swap regret:\n\n$$\nR_T^{\\text{int}} = \\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a_t)\\right] - \\min_{\\phi: \\mathcal{A} \\to \\mathcal{A}}\\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(\\phi(a_t))\\right]\n$$\n\n### 3. Performance Measures\n\nAverage regret:\n\n$$\n\\bar{R}_T = \\frac{R_T}{T}\n$$\n\nCompetitive ratio:\n\n$$\nCR_T = \\frac{\\sum_{t=1}^T \\ell_t(a_t)}{\\min_{a \\in \\mathcal{A}}\\sum_{t=1}^T \\ell_t(a)}\n$$\n\n## Online Convex Optimization\n\n### 1. Online Gradient Descent\n\nUpdate rule:\n\n$$\nw_{t+1} = \\Pi_{\\mathcal{W}}(w_t - \\eta_t \\nabla \\ell_t(w_t))\n$$\n\nRegret bound:\n\n$$\nR_T \\leq \\frac{D^2}{2\\eta} + \\frac{\\eta G^2T}{2}\n$$\n\nWhere:\n- $D$ is diameter of feasible set\n- $G$ is gradient bound\n- $\\eta$ is learning rate\n\n### 2. Follow The Regularized Leader\n\nFTRL update:\n\n$$\nw_{t+1} = \\arg\\min_{w \\in \\mathcal{W}}\\{\\eta\\sum_{s=1}^t \\ell_s(w) + R(w)\\}\n$$\n\nRegret bound:\n\n$$\nR_T \\leq \\frac{R(w^*)}{\\eta} + \\frac{\\eta G^2T}{2}\n$$\n\n### 3. Mirror Descent\n\nUpdate rule:\n\n$$\n\\nabla \\psi(w_{t+1}) = \\nabla \\psi(w_t) - \\eta_t \\nabla \\ell_t(w_t)\n$$\n\nRegret bound:\n\n$$\nR_T \\leq \\frac{D_\\psi(w^*\\|w_1)}{\\eta} + \\frac{\\eta G^2T}{2}\n$$\n\n## Multi-Armed Bandits\n\n### 1. UCB Algorithm\n\nUCB index:\n\n$$\nUCB_i(t) = \\hat{\\mu}_i(t) + \\sqrt{\\frac{2\\ln t}{N_i(t)}}\n$$\n\nRegret bound:\n\n$$\nR_T \\leq \\sum_{i:\\Delta_i>0}\\left(\\frac{8\\ln T}{\\Delta_i} + (1+\\pi^2/3)\\Delta_i\\right)\n$$\n\n### 2. Thompson Sampling\n\nPosterior sampling:\n\n$$\n\\theta_i(t) \\sim Beta(\\alpha_i(t), \\beta_i(t))\n$$\n\nRegret bound:\n\n$$\nR_T \\leq O(\\sqrt{KT\\ln T})\n$$\n\n### 3. Exp3 Algorithm\n\nProbability update:\n\n$$\np_i(t) = \\frac{(1-\\gamma)\\exp(\\eta G_i(t))}{\\sum_{j=1}^K \\exp(\\eta G_j(t))} + \\frac{\\gamma}{K}\n$$\n\nRegret bound:\n\n$$\nR_T \\leq 2\\sqrt{KT\\ln K}\n$$\n\n## Expert Algorithms\n\n### 1. Weighted Majority\n\nWeight update:\n\n$$\nw_i(t+1) = w_i(t)(1-\\eta)^{\\ell_i(t)}\n$$\n\nRegret bound:\n\n$$\nR_T \\leq \\sqrt{T\\ln N}\n$$\n\n### 2. Hedge Algorithm\n\nProbability update:\n\n$$\np_i(t+1) = \\frac{\\exp(-\\eta L_i(t))}{\\sum_{j=1}^N \\exp(-\\eta L_j(t))}\n$$\n\nRegret bound:\n\n$$\nR_T \\leq \\sqrt{2T\\ln N}\n$$\n\n### 3. AdaHedge\n\nAdaptive learning rate:\n\n$$\n\\eta_t = \\frac{\\ln N}{V_t}\n$$\n\nWhere:\n- $V_t$ is cumulative variance\n- $N$ is number of experts\n\n## Advanced Topics\n\n### 1. Adaptive Algorithms\n\nAdaGrad update:\n\n$$\nw_{t+1,i} = w_{t,i} - \\frac{\\eta}{\\sqrt{\\sum_{s=1}^t g_{s,i}^2}}g_{t,i}\n$$\n\nRegret bound:\n\n$$\nR_T \\leq O(\\sqrt{T}\\|w^*\\|_2\\sqrt{\\sum_{i=1}^d\\sum_{t=1}^T g_{t,i}^2})\n$$\n\n### 2. Second-Order Methods\n\nONS update:\n\n$$\nw_{t+1} = w_t - \\eta A_t^{-1}\\nabla \\ell_t(w_t)\n$$\n\nWhere:\n- $A_t = \\sum_{s=1}^t \\nabla \\ell_s(w_s)\\nabla \\ell_s(w_s)^T$\n\n### 3. Parameter-Free Methods\n\nMetaGrad update:\n\n$$\nw_{t+1} = w_t - \\eta_t H_t^{-1/2}\\nabla \\ell_t(w_t)\n$$\n\nWhere:\n- $H_t$ is preconditioner matrix\n\n## Applications\n\n### 1. Portfolio Selection\n\nObjective:\n\n$$\n\\max_{w \\in \\Delta_n} \\sum_{t=1}^T \\log(w^T r_t)\n$$\n\nUniversal portfolio:\n\n$$\nw_{t+1} = \\int_{\\Delta_n} w P_t(w)dw\n$$\n\n### 2. Online Routing\n\nFlow update:\n\n$$\nf_{t+1}(e) = f_t(e)\\exp(-\\eta \\ell_t(e))\n$$\n\nPath selection:\n\n$$\nP(p) = \\frac{\\prod_{e \\in p}f_t(e)}{\\sum_{p' \\in \\mathcal{P}}\\prod_{e \\in p'}f_t(e)}\n$$\n\n### 3. Online Classification\n\nPerceptron update:\n\n$$\nw_{t+1} = w_t + y_tx_t\\mathbb{1}[y_tw_t^Tx_t \\leq 0]\n$$\n\nMistake bound:\n\n$$\nM \\leq \\left(\\frac{R}{\\gamma}\\right)^2\n$$\n\n## Theoretical Results\n\n### 1. Lower Bounds\n\nMinimax regret:\n\n$$\n\\min_{\\text{ALG}}\\max_{\\text{ADV}} R_T = \\Omega(\\sqrt{T})\n$$\n\nExpert setting:\n\n$$\nR_T = \\Omega(\\sqrt{T\\ln N})\n$$\n\n### 2. Information Theory\n\nRedundancy bound:\n\n$$\nR_T \\leq \\frac{KL(P\\|Q) + \\ln(1/\\delta)}{\\eta} + \\frac{\\eta T}{8}\n$$\n\n### 3. Game Theory\n\nNash equilibrium:\n\n$$\n\\max_P \\min_Q \\mathbb{E}_{p \\sim P, q \\sim Q}[M(p,q)] = \\min_Q \\max_P \\mathbb{E}_{p \\sim P, q \\sim Q}[M(p,q)]\n$$\n\n## Implementation Considerations\n\n### 1. Algorithm Selection\n\n1. Problem Structure:\n   - Convexity\n   - Smoothness\n   - Sparsity\n\n2. Computational Constraints:\n   - Memory limits\n   - Update time\n   - Parallelization\n\n3. Performance Requirements:\n   - Regret bounds\n   - Adaptation speed\n   - Robustness\n\n### 2. Parameter Tuning\n\n1. Learning Rates:\n   - Fixed vs adaptive\n   - Schedule design\n   - Initialization\n\n2. Exploration:\n   - Exploration rate\n   - Decay schedule\n   - Adaptive schemes\n\n3. Regularization:\n   - Strength\n   - Type selection\n   - Adaptation\n\n## Best Practices\n\n### 1. Algorithm Design\n\n1. Robustness:\n   - Adversarial scenarios\n   - Noise handling\n   - Distribution shifts\n\n2. Efficiency:\n   - Memory usage\n   - Update complexity\n   - Parallelization\n\n3. Adaptivity:\n   - Parameter tuning\n   - Distribution changes\n   - Model selection\n\n### 2. Implementation\n\n1. Data Handling:\n   - Streaming processing\n   - Feature extraction\n   - Preprocessing\n\n2. Monitoring:\n   - Regret tracking\n   - Performance metrics\n   - Resource usage\n\n3. Deployment:\n   - System integration\n   - Error handling\n   - Scaling strategy\n\n## References\n\n1. Theory:\n   - \"Introduction to Online Convex Optimization\" by Hazan\n   - \"Bandit Algorithms\" by Lattimore and Szepesv√°ri\n   - \"Prediction, Learning, and Games\" by Cesa-Bianchi and Lugosi\n\n2. Methods:\n   - \"Online Learning and Online Convex Optimization\" by Shalev-Shwartz\n   - \"A Modern Introduction to Online Learning\" by Orabona\n   - \"Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems\" by Bubeck and Cesa-Bianchi\n\n3. Applications:\n   - \"Online Portfolio Selection\" by Li and Hoi\n   - \"Online Learning in Routing Games\" by Roughgarden\n   - \"Online Methods in Machine Learning\" by Bottou\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}