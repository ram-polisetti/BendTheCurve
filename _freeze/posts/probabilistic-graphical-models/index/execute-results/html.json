{
  "hash": "e0b711acc28a703106584aca8ddf5f92",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Probabilistic Graphical Models: Mathematical Foundations\"\nauthor: \"Ram Polisetti\"\ndate: \"2024-03-19\"\ncategories: [machine-learning, probabilistic-models, mathematics, bayesian]\nimage: \"pgm.jpg\"\ndescription: \"A rigorous exploration of probabilistic graphical models, covering mathematical foundations, inference algorithms, and learning methods.\"\njupyter: python3\n---\n\n\n\n\n# Probabilistic Graphical Models\n\n## Bayesian Networks\n\n### 1. Factorization\n\nJoint probability factorization:\n\n$$\nP(X_1, ..., X_n) = \\prod_{i=1}^n P(X_i | \\text{Pa}(X_i))\n$$\n\nWhere:\n- $X_i$ are random variables\n- $\\text{Pa}(X_i)$ are parents of $X_i$ in the graph\n\n### 2. Conditional Independence\n\nD-separation criterion:\n- Two nodes are d-separated if all paths between them are blocked\n- A path is blocked if:\n  * Contains a collider not in evidence\n  * Contains a non-collider in evidence\n\nFormal definition:\n\n$$\nX \\perp\\!\\!\\!\\perp Y | Z \\iff P(X|Y,Z) = P(X|Z)\n$$\n\n## Markov Random Fields\n\n### 1. Gibbs Distribution\n\nJoint distribution representation:\n\n$$\nP(X = x) = \\frac{1}{Z}\\exp\\left(-\\sum_{c \\in \\mathcal{C}} \\psi_c(x_c)\\right)\n$$\n\nWhere:\n- $\\mathcal{C}$ is the set of cliques\n- $\\psi_c$ are potential functions\n- $Z$ is the partition function:\n\n$$\nZ = \\sum_x \\exp\\left(-\\sum_{c \\in \\mathcal{C}} \\psi_c(x_c)\\right)\n$$\n\n### 2. Hammersley-Clifford Theorem\n\nEquivalence between positive distributions and Gibbs distributions:\n\n$$\nP(X = x) > 0 \\iff P(X = x) = \\frac{1}{Z}\\prod_{c \\in \\mathcal{C}} \\phi_c(x_c)\n$$\n\nWhere:\n- $\\phi_c$ are non-negative factors\n\n## Inference Algorithms\n\n### 1. Variable Elimination\n\nComplexity for tree-structured graphs:\n\n$$\nO(n \\cdot d^{w})\n$$\n\nWhere:\n- $n$ is number of variables\n- $d$ is domain size\n- $w$ is tree width\n\nAlgorithm steps:\n1. Choose elimination ordering\n2. For each variable:\n   - Multiply relevant factors\n   - Sum out variable\n\n### 2. Belief Propagation\n\nMessage passing equations:\n\n$$\n\\begin{aligned}\n\\mu_{i \\to j}(x_j) &= \\sum_{x_i} \\phi_i(x_i)\\phi_{ij}(x_i,x_j)\\prod_{k \\in N(i)\\backslash j} \\mu_{k \\to i}(x_i) \\\\\nb_i(x_i) &\\propto \\phi_i(x_i)\\prod_{j \\in N(i)} \\mu_{j \\to i}(x_i)\n\\end{aligned}\n$$\n\nWhere:\n- $\\mu_{i \\to j}$ is message from i to j\n- $b_i$ is belief at node i\n- $N(i)$ is neighbors of i\n\n### 3. Junction Tree Algorithm\n\nClique tree construction:\n1. Moralize graph\n2. Triangulate\n3. Find maximal cliques\n4. Build junction tree\n\nRunning intersection property:\n\n$$\nS_{ij} = C_i \\cap C_j \\subseteq C_k\n$$\n\nFor any cliques $C_i$, $C_j$, and clique $C_k$ on path between them.\n\n## Learning Methods\n\n### 1. Maximum Likelihood Estimation\n\nObjective function:\n\n$$\n\\hat{\\theta}_{MLE} = \\arg\\max_\\theta \\sum_{i=1}^N \\log P(x^{(i)}|\\theta)\n$$\n\nFor complete data in Bayesian networks:\n\n$$\n\\hat{\\theta}_{ijk} = \\frac{N_{ijk}}{\\sum_k N_{ijk}}\n$$\n\nWhere:\n- $N_{ijk}$ is count of $X_i=k$ with parent configuration j\n\n### 2. Bayesian Parameter Learning\n\nPosterior distribution:\n\n$$\nP(\\theta|D) \\propto P(D|\\theta)P(\\theta)\n$$\n\nWith Dirichlet prior:\n\n$$\n\\theta_{ijk} \\sim \\text{Dir}(\\alpha_{ijk})\n$$\n\nPosterior parameters:\n\n$$\n\\alpha_{ijk}^{\\text{post}} = \\alpha_{ijk} + N_{ijk}\n$$\n\n### 3. Structure Learning\n\nScore-based learning objective:\n\n$$\nG^* = \\arg\\max_G \\text{score}(G:D)\n$$\n\nCommon scores:\n1. BIC score:\n\n$$\n\\text{BIC}(G:D) = \\ell(D|\\hat{\\theta}, G) - \\frac{\\log N}{2}|G|\n$$\n\n2. Bayesian score:\n\n$$\nP(G|D) \\propto P(D|G)P(G)\n$$\n\n## Advanced Topics\n\n### 1. Variational Inference\n\nEvidence lower bound (ELBO):\n\n$$\n\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x,z)] - \\mathbb{E}_q[\\log q(z)]\n$$\n\nMean field approximation:\n\n$$\nq(z) = \\prod_i q_i(z_i)\n$$\n\nUpdate equations:\n\n$$\n\\log q_j^*(z_j) = \\mathbb{E}_{q_{-j}}[\\log p(x,z)] + \\text{const}\n$$\n\n### 2. MCMC Methods\n\nMetropolis-Hastings acceptance ratio:\n\n$$\n\\alpha = \\min\\left(1, \\frac{p(x')q(x|x')}{p(x)q(x'|x)}\\right)\n$$\n\nGibbs sampling update:\n\n$$\nx_i^{(t+1)} \\sim p(x_i|x_{-i}^{(t)})\n$$\n\n### 3. Conditional Random Fields\n\nLinear-chain CRF probability:\n\n$$\nP(y|x) = \\frac{1}{Z(x)}\\exp\\left(\\sum_{t=1}^T\\sum_k \\lambda_k f_k(y_t,y_{t-1},x_t)\\right)\n$$\n\nWhere:\n- $f_k$ are feature functions\n- $\\lambda_k$ are weights\n- $Z(x)$ is normalization factor\n\n## Implementation Considerations\n\n### 1. Numerical Stability\n\nLog-space computations:\n\n$$\n\\log\\sum_i \\exp(x_i) = \\max_i x_i + \\log\\sum_i \\exp(x_i - \\max_i x_i)\n$$\n\n### 2. Sparse Representations\n\nEfficient factor operations:\n- Sparse matrices for CPTs\n- Vectorized operations\n- Caching intermediate results\n\n### 3. Parallelization\n\nParallel message passing:\n- Tree-structured graphs\n- Junction tree clusters\n- Mini-batch learning\n\n## Best Practices\n\n### 1. Model Selection\n\n1. Network Structure:\n   - Expert knowledge\n   - Causal relationships\n   - Data-driven learning\n\n2. Inference Method:\n   - Exact vs approximate\n   - Graph structure\n   - Domain size\n\n3. Learning Approach:\n   - Data completeness\n   - Prior knowledge\n   - Computational resources\n\n### 2. Performance Optimization\n\n1. Variable Ordering:\n   - Min-fill heuristic\n   - Min-degree ordering\n   - Weighted variants\n\n2. Message Scheduling:\n   - Residual belief propagation\n   - Priority-based updates\n   - Asynchronous methods\n\n3. Memory Management:\n   - Factor caching\n   - Message memoization\n   - Sparse representations\n\n## Applications\n\n### 1. Medical Diagnosis\n\nNetwork structure:\n- Diseases as root nodes\n- Symptoms as leaf nodes\n- Test results as intermediate nodes\n\nInference tasks:\n- Diagnostic reasoning\n- Predictive reasoning\n- Intercausal reasoning\n\n### 2. Computer Vision\n\nMRF applications:\n- Image segmentation\n- Stereo matching\n- Image restoration\n\nEnergy function:\n\n$$\nE(x) = \\sum_i \\phi_i(x_i) + \\sum_{i,j} \\phi_{ij}(x_i,x_j)\n$$\n\n### 3. Natural Language Processing\n\nLinear-chain CRFs:\n- Part-of-speech tagging\n- Named entity recognition\n- Sequence labeling\n\nFeature templates:\n- Word features\n- Context windows\n- Transition features\n\n## References\n\n1. Theory:\n   - \"Probabilistic Graphical Models\" by Koller and Friedman\n   - \"Pattern Recognition and Machine Learning\" by Bishop\n   - \"Information Theory, Inference, and Learning Algorithms\" by MacKay\n\n2. Algorithms:\n   - \"Understanding Belief Propagation and its Generalizations\" by Yedidia et al.\n   - \"An Introduction to MCMC for Machine Learning\" by Andrieu et al.\n   - \"Structured Prediction for Natural Language Processing\" by Smith\n\n3. Applications:\n   - \"Medical Applications of Artificial Intelligence\" by Dua and Acharya\n   - \"Computer Vision: A Modern Approach\" by Forsyth and Ponce\n   - \"Speech and Language Processing\" by Jurafsky and Martin\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}