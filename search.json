[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "ML Journey Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWelcome to BendTheCurve\n\n\n1 min\n\n\n\nwelcome\n\n\nintroduction\n\n\n\n\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n1 min\n\n\n\nnews\n\n\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n1 min\n\n\n\nnews\n\n\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n1 min\n\n\n\nnews\n\n\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis with Python\n\n\n2 min\n\n\n\nanalysis\n\n\npython\n\n\nvisualization\n\n\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Scikit-learn\n\n\n2 min\n\n\n\nmachine-learning\n\n\npython\n\n\ntutorial\n\n\ndata-science\n\n\n\nA beginner‚Äôs guide to machine learning with scikit-learn, covering basic concepts and implementing your first ML model.\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with PyTorch\n\n\n3 min\n\n\n\ndeep-learning\n\n\npython\n\n\ntutorial\n\n\nmachine-learning\n\n\ndata-science\n\n\n\nAn introduction to deep learning with PyTorch, covering basic concepts and building your first neural network.\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing in Python\n\n\n2 min\n\n\n\ndata-science\n\n\npython\n\n\ntutorial\n\n\nmachine-learning\n\n\n\nA comprehensive guide to data preprocessing techniques in Python, covering handling missing values, scaling, and feature engineering.\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Data Visualization with Plotly\n\n\n4 min\n\n\n\ndata-science\n\n\npython\n\n\ntutorial\n\n\ndata-visualization\n\n\n\nLearn how to create interactive and engaging data visualizations using Plotly in Python.\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BendThe-Curve Together",
    "section": "",
    "text": "BendThe-Curve is a collaborative space where we learn, share, and grow together in the exciting world of Data Science and AI. Our community is built on:\n\nü§ù Shared Knowledge & Experiences\nüöÄ Collective Learning & Growth\nüí° Community-Driven Projects\nüåç Real-World Impact Through Data\nüîó Network of Data Enthusiasts\n\n\n\nExplore our latest community contributions below, or visit the Posts page for more shared knowledge and experiences."
  },
  {
    "objectID": "index.html#welcome-to-our-learning-community",
    "href": "index.html#welcome-to-our-learning-community",
    "title": "BendThe-Curve Together",
    "section": "",
    "text": "BendThe-Curve is a collaborative space where we learn, share, and grow together in the exciting world of Data Science and AI. Our community is built on:\n\nü§ù Shared Knowledge & Experiences\nüöÄ Collective Learning & Growth\nüí° Community-Driven Projects\nüåç Real-World Impact Through Data\nüîó Network of Data Enthusiasts\n\n\n\nExplore our latest community contributions below, or visit the Posts page for more shared knowledge and experiences."
  },
  {
    "objectID": "posts/bend-the-curve-intro/index.html",
    "href": "posts/bend-the-curve-intro/index.html",
    "title": "Welcome to BendTheCurve",
    "section": "",
    "text": "I‚Äôm excited to introduce BendTheCurve, a platform dedicated to exploring and sharing insights about data science, machine learning, and their real-world applications. Here, we‚Äôll dive deep into:\n\n\n\nData Science Insights: Regular posts about data analysis, visualization, and interpretation\nMachine Learning Deep Dives: Exploring both basic and advanced ML concepts\nPractical Tutorials: Hands-on guides and code examples\nIndustry Applications: Real-world case studies and applications\n\n\n\n\nWhether you‚Äôre a beginner in data science or an experienced practitioner, BendTheCurve aims to provide valuable content that helps you grow in your data science journey.\nStay tuned for upcoming posts!"
  },
  {
    "objectID": "posts/bend-the-curve-intro/index.html#what-to-expect",
    "href": "posts/bend-the-curve-intro/index.html#what-to-expect",
    "title": "Welcome to BendTheCurve",
    "section": "",
    "text": "Data Science Insights: Regular posts about data analysis, visualization, and interpretation\nMachine Learning Deep Dives: Exploring both basic and advanced ML concepts\nPractical Tutorials: Hands-on guides and code examples\nIndustry Applications: Real-world case studies and applications"
  },
  {
    "objectID": "posts/bend-the-curve-intro/index.html#join-the-journey",
    "href": "posts/bend-the-curve-intro/index.html#join-the-journey",
    "title": "Welcome to BendTheCurve",
    "section": "",
    "text": "Whether you‚Äôre a beginner in data science or an experienced practitioner, BendTheCurve aims to provide valuable content that helps you grow in your data science journey.\nStay tuned for upcoming posts!"
  },
  {
    "objectID": "posts/data-preprocessing/index.html",
    "href": "posts/data-preprocessing/index.html",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "Data preprocessing is a crucial step in any data science project. In this post, we‚Äôll explore common preprocessing techniques and how to implement them using Python‚Äôs popular data science libraries.\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed and plot style\nnp.random.seed(42)\nplt.style.use('default')\nsns.set_theme()\n\n# Create sample dataset with common preprocessing challenges\nn_samples = 1000\ndata = {\n    'age': np.random.normal(35, 10, n_samples),\n    'income': np.random.lognormal(10, 1, n_samples),\n    'education_years': np.random.randint(8, 22, n_samples),\n    'job_category': np.random.choice(['Tech', 'Finance', 'Healthcare', 'Other'], n_samples),\n    'satisfaction_score': np.random.randint(1, 11, n_samples)\n}\n\n# Add some missing values\ndf = pd.DataFrame(data)\ndf.loc[np.random.choice(n_samples, 100), 'income'] = np.nan\ndf.loc[np.random.choice(n_samples, 50), 'education_years'] = np.nan\ndf.loc[np.random.choice(n_samples, 30), 'job_category'] = np.nan\n\n# Display initial data info\nprint(\"Initial Dataset Info:\")\nprint(df.info())\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n\nInitial Dataset Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   age                 1000 non-null   float64\n 1   income              908 non-null    float64\n 2   education_years     950 non-null    float64\n 3   job_category        971 non-null    object \n 4   satisfaction_score  1000 non-null   int64  \ndtypes: float64(3), int64(1), object(1)\nmemory usage: 39.2+ KB\nNone\n\nMissing Values:\nage                    0\nincome                92\neducation_years       50\njob_category          29\nsatisfaction_score     0\ndtype: int64\n\n\n\n\n\n\n\nCode\n# Function to handle missing values\ndef handle_missing_values(df):\n    # Numeric columns: fill with median\n    numeric_imputer = SimpleImputer(strategy='median')\n    df[['income', 'education_years']] = numeric_imputer.fit_transform(df[['income', 'education_years']])\n    \n    # Categorical columns: fill with mode\n    categorical_imputer = SimpleImputer(strategy='most_frequent')\n    df[['job_category']] = categorical_imputer.fit_transform(df[['job_category']])\n    \n    return df\n\n# Handle missing values\ndf_clean = df.copy()\ndf_clean = handle_missing_values(df_clean)\n\nprint(\"\\nMissing Values After Imputation:\")\nprint(df_clean.isnull().sum())\n\n\n\nMissing Values After Imputation:\nage                   0\nincome                0\neducation_years       0\njob_category          0\nsatisfaction_score    0\ndtype: int64\n\n\n\n\n\n\n\nCode\n# Scale numeric features\nscaler = StandardScaler()\nnumeric_cols = ['age', 'income', 'education_years']\ndf_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n\n# Visualize scaled features\nplt.figure(figsize=(15, 5))\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df_clean[col], bins=30, kde=True)\n    plt.title(f'Distribution of Scaled {col}')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndf_clean['job_category_encoded'] = label_encoder.fit_transform(df_clean['job_category'])\n\n# Visualize categorical distribution\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_clean, x='job_category')\nplt.title('Distribution of Job Categories')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create new features\ndf_clean['income_per_education'] = np.exp(df_clean['income']) / (df_clean['education_years'] + 1)  # Add 1 to avoid division by zero\ndf_clean['is_high_satisfaction'] = (df_clean['satisfaction_score'] &gt;= 8).astype(int)\n\n# Visualize engineered features\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(data=df_clean, x='job_category', y='income_per_education')\nplt.title('Income per Education Year by Job Category')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=df_clean, x='job_category', y='is_high_satisfaction')\nplt.title('High Satisfaction Rate by Job Category')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIn this tutorial, we learned essential data preprocessing techniques:\n\nHandling missing values using imputation\nScaling numeric features\nEncoding categorical variables\nCreating new features through feature engineering\n\nThese techniques are crucial for preparing your data for machine learning models. In future posts, we‚Äôll explore: - Advanced feature engineering techniques - Handling imbalanced datasets - Dealing with outliers - Automated feature selection\nStay tuned!"
  },
  {
    "objectID": "posts/data-preprocessing/index.html#setup-and-sample-data",
    "href": "posts/data-preprocessing/index.html#setup-and-sample-data",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed and plot style\nnp.random.seed(42)\nplt.style.use('default')\nsns.set_theme()\n\n# Create sample dataset with common preprocessing challenges\nn_samples = 1000\ndata = {\n    'age': np.random.normal(35, 10, n_samples),\n    'income': np.random.lognormal(10, 1, n_samples),\n    'education_years': np.random.randint(8, 22, n_samples),\n    'job_category': np.random.choice(['Tech', 'Finance', 'Healthcare', 'Other'], n_samples),\n    'satisfaction_score': np.random.randint(1, 11, n_samples)\n}\n\n# Add some missing values\ndf = pd.DataFrame(data)\ndf.loc[np.random.choice(n_samples, 100), 'income'] = np.nan\ndf.loc[np.random.choice(n_samples, 50), 'education_years'] = np.nan\ndf.loc[np.random.choice(n_samples, 30), 'job_category'] = np.nan\n\n# Display initial data info\nprint(\"Initial Dataset Info:\")\nprint(df.info())\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n\nInitial Dataset Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   age                 1000 non-null   float64\n 1   income              908 non-null    float64\n 2   education_years     950 non-null    float64\n 3   job_category        971 non-null    object \n 4   satisfaction_score  1000 non-null   int64  \ndtypes: float64(3), int64(1), object(1)\nmemory usage: 39.2+ KB\nNone\n\nMissing Values:\nage                    0\nincome                92\neducation_years       50\njob_category          29\nsatisfaction_score     0\ndtype: int64"
  },
  {
    "objectID": "posts/data-preprocessing/index.html#handling-missing-values",
    "href": "posts/data-preprocessing/index.html#handling-missing-values",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "Code\n# Function to handle missing values\ndef handle_missing_values(df):\n    # Numeric columns: fill with median\n    numeric_imputer = SimpleImputer(strategy='median')\n    df[['income', 'education_years']] = numeric_imputer.fit_transform(df[['income', 'education_years']])\n    \n    # Categorical columns: fill with mode\n    categorical_imputer = SimpleImputer(strategy='most_frequent')\n    df[['job_category']] = categorical_imputer.fit_transform(df[['job_category']])\n    \n    return df\n\n# Handle missing values\ndf_clean = df.copy()\ndf_clean = handle_missing_values(df_clean)\n\nprint(\"\\nMissing Values After Imputation:\")\nprint(df_clean.isnull().sum())\n\n\n\nMissing Values After Imputation:\nage                   0\nincome                0\neducation_years       0\njob_category          0\nsatisfaction_score    0\ndtype: int64"
  },
  {
    "objectID": "posts/data-preprocessing/index.html#feature-scaling",
    "href": "posts/data-preprocessing/index.html#feature-scaling",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "Code\n# Scale numeric features\nscaler = StandardScaler()\nnumeric_cols = ['age', 'income', 'education_years']\ndf_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n\n# Visualize scaled features\nplt.figure(figsize=(15, 5))\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df_clean[col], bins=30, kde=True)\n    plt.title(f'Distribution of Scaled {col}')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/data-preprocessing/index.html#categorical-encoding",
    "href": "posts/data-preprocessing/index.html#categorical-encoding",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "Code\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndf_clean['job_category_encoded'] = label_encoder.fit_transform(df_clean['job_category'])\n\n# Visualize categorical distribution\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_clean, x='job_category')\nplt.title('Distribution of Job Categories')\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "posts/data-preprocessing/index.html#feature-engineering",
    "href": "posts/data-preprocessing/index.html#feature-engineering",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "Code\n# Create new features\ndf_clean['income_per_education'] = np.exp(df_clean['income']) / (df_clean['education_years'] + 1)  # Add 1 to avoid division by zero\ndf_clean['is_high_satisfaction'] = (df_clean['satisfaction_score'] &gt;= 8).astype(int)\n\n# Visualize engineered features\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(data=df_clean, x='job_category', y='income_per_education')\nplt.title('Income per Education Year by Job Category')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=df_clean, x='job_category', y='is_high_satisfaction')\nplt.title('High Satisfaction Rate by Job Category')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/data-preprocessing/index.html#key-takeaways",
    "href": "posts/data-preprocessing/index.html#key-takeaways",
    "title": "Data Preprocessing in Python",
    "section": "",
    "text": "In this tutorial, we learned essential data preprocessing techniques:\n\nHandling missing values using imputation\nScaling numeric features\nEncoding categorical variables\nCreating new features through feature engineering\n\nThese techniques are crucial for preparing your data for machine learning models. In future posts, we‚Äôll explore: - Advanced feature engineering techniques - Handling imbalanced datasets - Dealing with outliers - Automated feature selection\nStay tuned!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/intro-to-sklearn/index.html",
    "href": "posts/intro-to-sklearn/index.html",
    "title": "Getting Started with Scikit-learn",
    "section": "",
    "text": "In this post, we‚Äôll explore the basics of machine learning using Python‚Äôs scikit-learn library. We‚Äôll build a simple classification model and understand the fundamental ML workflow.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plot style\nplt.style.use('default')\nsns.set_theme()\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 1000\n\n# Create features\nX = np.random.normal(size=(n_samples, 2))\n# Create two clusters\ny = (X[:, 0]**2 + X[:, 1]**2 &gt; 2).astype(int)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\n\n\nCode\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Look at the distribution of scaled features\nplt.figure(figsize=(10, 5))\nsns.histplot(X_train_scaled[:, 0], bins=30, kde=True)\nplt.title('Distribution of Scaled Feature 1')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create and train the model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = rf_model.predict(X_test_scaled)\n\n# Print the results\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       133\n           1       0.92      0.99      0.95        67\n\n    accuracy                           0.96       200\n   macro avg       0.95      0.97      0.96       200\nweighted avg       0.97      0.96      0.97       200\n\n\n\n\n\n\n\n\nCode\n# Create a mesh grid to visualize the decision boundary\ndef plot_decision_boundary(X, y, model, scaler):\n    h = 0.02  # Step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Scale the mesh points\n    mesh_points = scaler.transform(np.c_[xx.ravel(), yy.ravel()])\n    \n    # Make predictions\n    Z = model.predict(mesh_points)\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the decision boundary\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n    plt.title('Random Forest Decision Boundary')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    return plt.gcf()\n\n# Plot the decision boundary\nplot_decision_boundary(X_test, y_test, rf_model, scaler)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIn this tutorial, we learned:\n\nHow to prepare data for machine learning\nBasic usage of scikit-learn‚Äôs RandomForestClassifier\nModel evaluation using classification metrics\nVisualization of model decision boundaries\n\nThis is just the beginning of our machine learning journey. In future posts, we‚Äôll explore: - More advanced algorithms - Feature engineering techniques - Model tuning and optimization - Real-world applications\nStay tuned!"
  },
  {
    "objectID": "posts/intro-to-sklearn/index.html#setup",
    "href": "posts/intro-to-sklearn/index.html#setup",
    "title": "Getting Started with Scikit-learn",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plot style\nplt.style.use('default')\nsns.set_theme()\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 1000\n\n# Create features\nX = np.random.normal(size=(n_samples, 2))\n# Create two clusters\ny = (X[:, 0]**2 + X[:, 1]**2 &gt; 2).astype(int)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/intro-to-sklearn/index.html#data-preprocessing",
    "href": "posts/intro-to-sklearn/index.html#data-preprocessing",
    "title": "Getting Started with Scikit-learn",
    "section": "",
    "text": "Code\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Look at the distribution of scaled features\nplt.figure(figsize=(10, 5))\nsns.histplot(X_train_scaled[:, 0], bins=30, kde=True)\nplt.title('Distribution of Scaled Feature 1')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()"
  },
  {
    "objectID": "posts/intro-to-sklearn/index.html#training-the-model",
    "href": "posts/intro-to-sklearn/index.html#training-the-model",
    "title": "Getting Started with Scikit-learn",
    "section": "",
    "text": "Code\n# Create and train the model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = rf_model.predict(X_test_scaled)\n\n# Print the results\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97       133\n           1       0.92      0.99      0.95        67\n\n    accuracy                           0.96       200\n   macro avg       0.95      0.97      0.96       200\nweighted avg       0.97      0.96      0.97       200"
  },
  {
    "objectID": "posts/intro-to-sklearn/index.html#visualizing-the-results",
    "href": "posts/intro-to-sklearn/index.html#visualizing-the-results",
    "title": "Getting Started with Scikit-learn",
    "section": "",
    "text": "Code\n# Create a mesh grid to visualize the decision boundary\ndef plot_decision_boundary(X, y, model, scaler):\n    h = 0.02  # Step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Scale the mesh points\n    mesh_points = scaler.transform(np.c_[xx.ravel(), yy.ravel()])\n    \n    # Make predictions\n    Z = model.predict(mesh_points)\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the decision boundary\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n    plt.title('Random Forest Decision Boundary')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    return plt.gcf()\n\n# Plot the decision boundary\nplot_decision_boundary(X_test, y_test, rf_model, scaler)\nplt.show()"
  },
  {
    "objectID": "posts/intro-to-sklearn/index.html#key-takeaways",
    "href": "posts/intro-to-sklearn/index.html#key-takeaways",
    "title": "Getting Started with Scikit-learn",
    "section": "",
    "text": "In this tutorial, we learned:\n\nHow to prepare data for machine learning\nBasic usage of scikit-learn‚Äôs RandomForestClassifier\nModel evaluation using classification metrics\nVisualization of model decision boundaries\n\nThis is just the beginning of our machine learning journey. In future posts, we‚Äôll explore: - More advanced algorithms - Feature engineering techniques - Model tuning and optimization - Real-world applications\nStay tuned!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Data Analysis with Python",
    "section": "",
    "text": "In this post, we‚Äôll demonstrate how to use our custom utility functions for data analysis and visualization. We‚Äôll analyze a sample dataset of daily temperature readings.\n\n\nFirst, let‚Äôs import our utility functions and create some sample data:\n\n\nCode\n# Add project root to Python path\nimport sys\nimport os\nsys.path.append('/Users/rampolisetti/Github/bendthecurve')\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom utils.plotting import plot_distribution, time_series_plot\nfrom utils.data_prep import create_features\n\n# Create sample temperature data\nnp.random.seed(42)\ndates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\ntemperatures = np.random.normal(loc=20, scale=5, size=len(dates))\ntemperatures = temperatures + 10 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365)  # Add seasonal pattern\n\n# Create DataFrame\ndf = pd.DataFrame({\n    'date': dates,\n    'temperature': temperatures\n})\n\n\n\n\n\nLet‚Äôs analyze the distribution of temperatures and create some time-based features:\n\n\nCode\n# Plot temperature distribution\nfig1 = plot_distribution(\n    df['temperature'],\n    title='Distribution of Daily Temperatures',\n    xlabel='Temperature (¬∞C)',\n    bins=30\n)\n\n\n\n\n\n\n\n\n\nNow, let‚Äôs look at the time series with a 7-day moving average:\n\n\nCode\n# Create time series plot\nfig2 = time_series_plot(\n    df.set_index('date')['temperature'],\n    title='Daily Temperatures Over Time',\n    ylabel='Temperature (¬∞C)',\n    rolling_window=7\n)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use our utility functions to create time-based features:\n\n\nCode\n# Create time-based features\ndf_features = create_features(\n    df,\n    date_column='date',\n    cyclical_features=True\n)\n\n# Display the first few rows with new features\nprint(\"\\nDataFrame with engineered features:\")\nprint(df_features.head())\n\n\n\nDataFrame with engineered features:\n        date  temperature  year  month  day  day_of_week  month_sin  \\\n0 2023-01-01    22.483571  2023      1    1            6        0.5   \n1 2023-01-02    19.480812  2023      1    2            0        0.5   \n2 2023-01-03    23.582659  2023      1    3            1        0.5   \n3 2023-01-04    28.131346  2023      1    4            2        0.5   \n4 2023-01-05    19.517257  2023      1    5            3        0.5   \n\n   month_cos   dow_sin   dow_cos  \n0   0.866025 -0.781831  0.623490  \n1   0.866025  0.000000  1.000000  \n2   0.866025  0.781831  0.623490  \n3   0.866025  0.974928 -0.222521  \n4   0.866025  0.433884 -0.900969  \n\n\n\n\n\nLet‚Äôs examine how temperature varies by month using a box plot:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df_features, x='month', y='temperature')\nplt.title('Temperature Distribution by Month')\nplt.xlabel('Month')\nplt.ylabel('Temperature (¬∞C)')\n\n\nText(0, 0.5, 'Temperature (¬∞C)')\n\n\n\n\n\n\n\n\n\n\n\n\nThis example demonstrates how our utility functions make it easy to: 1. Create informative visualizations 2. Add time-based features 3. Analyze seasonal patterns\nIn future posts, we‚Äôll explore more advanced analyses using these utilities."
  },
  {
    "objectID": "posts/post-with-code/index.html#setup",
    "href": "posts/post-with-code/index.html#setup",
    "title": "Data Analysis with Python",
    "section": "",
    "text": "First, let‚Äôs import our utility functions and create some sample data:\n\n\nCode\n# Add project root to Python path\nimport sys\nimport os\nsys.path.append('/Users/rampolisetti/Github/bendthecurve')\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom utils.plotting import plot_distribution, time_series_plot\nfrom utils.data_prep import create_features\n\n# Create sample temperature data\nnp.random.seed(42)\ndates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\ntemperatures = np.random.normal(loc=20, scale=5, size=len(dates))\ntemperatures = temperatures + 10 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365)  # Add seasonal pattern\n\n# Create DataFrame\ndf = pd.DataFrame({\n    'date': dates,\n    'temperature': temperatures\n})"
  },
  {
    "objectID": "posts/post-with-code/index.html#data-analysis",
    "href": "posts/post-with-code/index.html#data-analysis",
    "title": "Data Analysis with Python",
    "section": "",
    "text": "Let‚Äôs analyze the distribution of temperatures and create some time-based features:\n\n\nCode\n# Plot temperature distribution\nfig1 = plot_distribution(\n    df['temperature'],\n    title='Distribution of Daily Temperatures',\n    xlabel='Temperature (¬∞C)',\n    bins=30\n)\n\n\n\n\n\n\n\n\n\nNow, let‚Äôs look at the time series with a 7-day moving average:\n\n\nCode\n# Create time series plot\nfig2 = time_series_plot(\n    df.set_index('date')['temperature'],\n    title='Daily Temperatures Over Time',\n    ylabel='Temperature (¬∞C)',\n    rolling_window=7\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#feature-engineering",
    "href": "posts/post-with-code/index.html#feature-engineering",
    "title": "Data Analysis with Python",
    "section": "",
    "text": "We can use our utility functions to create time-based features:\n\n\nCode\n# Create time-based features\ndf_features = create_features(\n    df,\n    date_column='date',\n    cyclical_features=True\n)\n\n# Display the first few rows with new features\nprint(\"\\nDataFrame with engineered features:\")\nprint(df_features.head())\n\n\n\nDataFrame with engineered features:\n        date  temperature  year  month  day  day_of_week  month_sin  \\\n0 2023-01-01    22.483571  2023      1    1            6        0.5   \n1 2023-01-02    19.480812  2023      1    2            0        0.5   \n2 2023-01-03    23.582659  2023      1    3            1        0.5   \n3 2023-01-04    28.131346  2023      1    4            2        0.5   \n4 2023-01-05    19.517257  2023      1    5            3        0.5   \n\n   month_cos   dow_sin   dow_cos  \n0   0.866025 -0.781831  0.623490  \n1   0.866025  0.000000  1.000000  \n2   0.866025  0.781831  0.623490  \n3   0.866025  0.974928 -0.222521  \n4   0.866025  0.433884 -0.900969"
  },
  {
    "objectID": "posts/post-with-code/index.html#analysis-of-seasonal-patterns",
    "href": "posts/post-with-code/index.html#analysis-of-seasonal-patterns",
    "title": "Data Analysis with Python",
    "section": "",
    "text": "Let‚Äôs examine how temperature varies by month using a box plot:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df_features, x='month', y='temperature')\nplt.title('Temperature Distribution by Month')\nplt.xlabel('Month')\nplt.ylabel('Temperature (¬∞C)')\n\n\nText(0, 0.5, 'Temperature (¬∞C)')"
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Data Analysis with Python",
    "section": "",
    "text": "This example demonstrates how our utility functions make it easy to: 1. Create informative visualizations 2. Add time-based features 3. Analyze seasonal patterns\nIn future posts, we‚Äôll explore more advanced analyses using these utilities."
  },
  {
    "objectID": "posts/Hahaha copy/index.html",
    "href": "posts/Hahaha copy/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html",
    "href": "posts/intro-to-pytorch/index.html",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "In this post, we‚Äôll dive into the basics of deep learning using PyTorch. We‚Äôll build a simple neural network and understand the fundamental concepts of deep learning.\n\n\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set plot style\nplt.style.use('default')\nsns.set_theme()\n\n# Generate synthetic data\nn_samples = 1000\nX = np.random.uniform(-5, 5, (n_samples, 1))\ny = 2 * np.sin(X) + np.random.normal(0, 0.2, (n_samples, 1))\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n\n\n\n\n\n\nCode\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 64)\n        self.layer3 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.layer3(x)\n        return x\n\n# Create the model\nmodel = SimpleNN()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n\n\n\n\n\nCode\n# Training loop\nepochs = 100\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.title('Training Loss Over Time')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.show()\n\n\nEpoch [10/100], Loss: 0.1010\nEpoch [20/100], Loss: 0.0901\nEpoch [30/100], Loss: 0.0706\nEpoch [40/100], Loss: 0.0683\nEpoch [50/100], Loss: 0.0631\nEpoch [60/100], Loss: 0.0613\nEpoch [70/100], Loss: 0.0600\nEpoch [80/100], Loss: 0.0587\nEpoch [90/100], Loss: 0.0576\nEpoch [100/100], Loss: 0.0564\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate predictions\nmodel.eval()\nwith torch.no_grad():\n    X_test = torch.FloatTensor(np.linspace(-6, 6, 200).reshape(-1, 1))\n    y_pred = model(X_test)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, alpha=0.5, label='Data')\nplt.plot(X_test, y_pred, 'r', label='Model Prediction')\nplt.plot(X_test, 2 * np.sin(X_test), 'g--', label='True Function')\nplt.legend()\nplt.title('Neural Network Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs analyze what our neural network has learned:\n\nThe network architecture consists of:\n\nInput layer (1 neuron)\nTwo hidden layers (64 neurons each)\nOutput layer (1 neuron)\nReLU activation functions\n\nTraining process:\n\nUsed Mean Squared Error loss\nAdam optimizer\n100 epochs of training\n\nResults:\n\nThe model successfully learned the underlying sinusoidal pattern\nHandles noise in the data well\nGeneralizes to unseen data points\n\n\n\n\n\nIn this tutorial, we learned:\n\nHow to create a neural network using PyTorch\nBasic concepts of:\n\nNetwork architecture\nForward and backward propagation\nLoss functions and optimization\n\nVisualization of training progress and results\n\nNext Steps: - Exploring more complex architectures - Understanding different activation functions - Working with real-world datasets - Implementing different types of neural networks (CNNs, RNNs)"
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html#setup",
    "href": "posts/intro-to-pytorch/index.html#setup",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "Code\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set plot style\nplt.style.use('default')\nsns.set_theme()\n\n# Generate synthetic data\nn_samples = 1000\nX = np.random.uniform(-5, 5, (n_samples, 1))\ny = 2 * np.sin(X) + np.random.normal(0, 0.2, (n_samples, 1))\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)"
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html#building-the-neural-network",
    "href": "posts/intro-to-pytorch/index.html#building-the-neural-network",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "Code\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 64)\n        self.layer3 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.layer3(x)\n        return x\n\n# Create the model\nmodel = SimpleNN()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)"
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html#training-the-model",
    "href": "posts/intro-to-pytorch/index.html#training-the-model",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "Code\n# Training loop\nepochs = 100\nlosses = []\n\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.title('Training Loss Over Time')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.show()\n\n\nEpoch [10/100], Loss: 0.1010\nEpoch [20/100], Loss: 0.0901\nEpoch [30/100], Loss: 0.0706\nEpoch [40/100], Loss: 0.0683\nEpoch [50/100], Loss: 0.0631\nEpoch [60/100], Loss: 0.0613\nEpoch [70/100], Loss: 0.0600\nEpoch [80/100], Loss: 0.0587\nEpoch [90/100], Loss: 0.0576\nEpoch [100/100], Loss: 0.0564"
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html#visualizing-the-results",
    "href": "posts/intro-to-pytorch/index.html#visualizing-the-results",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "Code\n# Generate predictions\nmodel.eval()\nwith torch.no_grad():\n    X_test = torch.FloatTensor(np.linspace(-6, 6, 200).reshape(-1, 1))\n    y_pred = model(X_test)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, alpha=0.5, label='Data')\nplt.plot(X_test, y_pred, 'r', label='Model Prediction')\nplt.plot(X_test, 2 * np.sin(X_test), 'g--', label='True Function')\nplt.legend()\nplt.title('Neural Network Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.show()"
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html#understanding-the-model",
    "href": "posts/intro-to-pytorch/index.html#understanding-the-model",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "Let‚Äôs analyze what our neural network has learned:\n\nThe network architecture consists of:\n\nInput layer (1 neuron)\nTwo hidden layers (64 neurons each)\nOutput layer (1 neuron)\nReLU activation functions\n\nTraining process:\n\nUsed Mean Squared Error loss\nAdam optimizer\n100 epochs of training\n\nResults:\n\nThe model successfully learned the underlying sinusoidal pattern\nHandles noise in the data well\nGeneralizes to unseen data points"
  },
  {
    "objectID": "posts/intro-to-pytorch/index.html#key-takeaways",
    "href": "posts/intro-to-pytorch/index.html#key-takeaways",
    "title": "Getting Started with PyTorch",
    "section": "",
    "text": "In this tutorial, we learned:\n\nHow to create a neural network using PyTorch\nBasic concepts of:\n\nNetwork architecture\nForward and backward propagation\nLoss functions and optimization\n\nVisualization of training progress and results\n\nNext Steps: - Exploring more complex architectures - Understanding different activation functions - Working with real-world datasets - Implementing different types of neural networks (CNNs, RNNs)"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html",
    "href": "posts/data-viz-plotly/index.html",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "In this tutorial, we‚Äôll explore how to create interactive visualizations using Plotly, a powerful library that allows us to build web-based plots and dashboards.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create sample dataset\nn_samples = 1000\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=n_samples, freq='D'),\n    'sales': np.random.normal(1000, 200, n_samples) + np.sin(np.linspace(0, 10, n_samples)) * 100,\n    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n    'product': np.random.choice(['A', 'B', 'C'], n_samples),\n    'customer_satisfaction': np.random.normal(4, 0.5, n_samples).clip(1, 5),\n    'units_sold': np.random.poisson(100, n_samples)\n}\n\ndf = pd.DataFrame(data)\n\n\n\n\n\n\n\nCode\n# Create daily sales by region\ndaily_sales = df.groupby(['date', 'region'])['sales'].mean().reset_index()\ndaily_sales_wide = daily_sales.pivot(index='date', columns='region', values='sales')\n\n# Create interactive line plot\nfig = go.Figure()\n\nfor region in daily_sales_wide.columns:\n    fig.add_trace(\n        go.Scatter(\n            x=daily_sales_wide.index,\n            y=daily_sales_wide[region],\n            name=region,\n            mode='lines'\n        )\n    )\n\nfig.update_layout(\n    title='Daily Sales by Region',\n    xaxis_title='Date',\n    yaxis_title='Sales',\n    hovermode='x unified'\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\nCode\n# Create scatter plot of sales vs. customer satisfaction\nfig = px.scatter(\n    df,\n    x='customer_satisfaction',\n    y='sales',\n    color='region',\n    size='units_sold',\n    hover_data=['product'],\n    title='Sales vs. Customer Satisfaction'\n)\n\nfig.update_layout(\n    xaxis_title='Customer Satisfaction',\n    yaxis_title='Sales'\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\nCode\n# Create monthly aggregates\ndf['month'] = df['date'].dt.to_period('M')\nmonthly_stats = df.groupby(['month', 'region', 'product']).agg({\n    'sales': 'sum',\n    'units_sold': 'sum',\n    'customer_satisfaction': 'mean'\n}).reset_index()\nmonthly_stats['month'] = monthly_stats['month'].astype(str)\n\n# Create animated bubble chart\nfig = px.scatter(\n    monthly_stats,\n    x='sales',\n    y='customer_satisfaction',\n    size='units_sold',\n    color='region',\n    animation_frame='month',\n    hover_name='product',\n    title='Monthly Sales Performance'\n)\n\nfig.update_layout(\n    xaxis_title='Total Sales',\n    yaxis_title='Average Customer Satisfaction'\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\nCode\n# Create correlation matrix\nnumeric_cols = ['sales', 'customer_satisfaction', 'units_sold']\ncorr_matrix = df[numeric_cols].corr()\n\n# Create heatmap\nfig = go.Figure(data=go.Heatmap(\n    z=corr_matrix,\n    x=corr_matrix.columns,\n    y=corr_matrix.columns,\n    text=np.round(corr_matrix, 2),\n    texttemplate='%{text}',\n    textfont={'size': 12},\n    hoverongaps=False,\n    colorscale='RdBu'\n))\n\nfig.update_layout(\n    title='Correlation Heatmap',\n    width=600,\n    height=600\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\nCode\n# Create box plots with violin overlay\nfig = go.Figure()\n\nfor region in df['region'].unique():\n    # Add violin plot\n    fig.add_trace(go.Violin(\n        x=df['region'][df['region'] == region],\n        y=df['sales'][df['region'] == region],\n        name=region,\n        box_visible=True,\n        meanline_visible=True\n    ))\n\nfig.update_layout(\n    title='Sales Distribution by Region',\n    xaxis_title='Region',\n    yaxis_title='Sales',\n    showlegend=False\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\nCode\n# Create 3D scatter plot\nfig = px.scatter_3d(\n    df,\n    x='sales',\n    y='customer_satisfaction',\n    z='units_sold',\n    color='region',\n    symbol='product',\n    title='Sales Performance in 3D'\n)\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='Sales',\n        yaxis_title='Customer Satisfaction',\n        zaxis_title='Units Sold'\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\nIn this tutorial, we learned how to create:\n\nInteractive line plots with multiple series\nScatter plots with hover information\nAnimated bubble charts\nInteractive heatmaps\nBox plots with violin overlays\n3D scatter plots\n\nThese interactive visualizations can help you: - Explore relationships in your data - Identify patterns and trends - Communicate insights effectively - Create engaging presentations\nIn future posts, we‚Äôll explore: - Custom themes and styling - Advanced Plotly features - Building dashboards with Dash - Real-time data visualization\nStay tuned!"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#setup",
    "href": "posts/data-viz-plotly/index.html#setup",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create sample dataset\nn_samples = 1000\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=n_samples, freq='D'),\n    'sales': np.random.normal(1000, 200, n_samples) + np.sin(np.linspace(0, 10, n_samples)) * 100,\n    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n    'product': np.random.choice(['A', 'B', 'C'], n_samples),\n    'customer_satisfaction': np.random.normal(4, 0.5, n_samples).clip(1, 5),\n    'units_sold': np.random.poisson(100, n_samples)\n}\n\ndf = pd.DataFrame(data)"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#line-plot-with-multiple-series",
    "href": "posts/data-viz-plotly/index.html#line-plot-with-multiple-series",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\n# Create daily sales by region\ndaily_sales = df.groupby(['date', 'region'])['sales'].mean().reset_index()\ndaily_sales_wide = daily_sales.pivot(index='date', columns='region', values='sales')\n\n# Create interactive line plot\nfig = go.Figure()\n\nfor region in daily_sales_wide.columns:\n    fig.add_trace(\n        go.Scatter(\n            x=daily_sales_wide.index,\n            y=daily_sales_wide[region],\n            name=region,\n            mode='lines'\n        )\n    )\n\nfig.update_layout(\n    title='Daily Sales by Region',\n    xaxis_title='Date',\n    yaxis_title='Sales',\n    hovermode='x unified'\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#interactive-scatter-plot",
    "href": "posts/data-viz-plotly/index.html#interactive-scatter-plot",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\n# Create scatter plot of sales vs. customer satisfaction\nfig = px.scatter(\n    df,\n    x='customer_satisfaction',\n    y='sales',\n    color='region',\n    size='units_sold',\n    hover_data=['product'],\n    title='Sales vs. Customer Satisfaction'\n)\n\nfig.update_layout(\n    xaxis_title='Customer Satisfaction',\n    yaxis_title='Sales'\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#animated-bubble-chart",
    "href": "posts/data-viz-plotly/index.html#animated-bubble-chart",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\n# Create monthly aggregates\ndf['month'] = df['date'].dt.to_period('M')\nmonthly_stats = df.groupby(['month', 'region', 'product']).agg({\n    'sales': 'sum',\n    'units_sold': 'sum',\n    'customer_satisfaction': 'mean'\n}).reset_index()\nmonthly_stats['month'] = monthly_stats['month'].astype(str)\n\n# Create animated bubble chart\nfig = px.scatter(\n    monthly_stats,\n    x='sales',\n    y='customer_satisfaction',\n    size='units_sold',\n    color='region',\n    animation_frame='month',\n    hover_name='product',\n    title='Monthly Sales Performance'\n)\n\nfig.update_layout(\n    xaxis_title='Total Sales',\n    yaxis_title='Average Customer Satisfaction'\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#interactive-heatmap",
    "href": "posts/data-viz-plotly/index.html#interactive-heatmap",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\n# Create correlation matrix\nnumeric_cols = ['sales', 'customer_satisfaction', 'units_sold']\ncorr_matrix = df[numeric_cols].corr()\n\n# Create heatmap\nfig = go.Figure(data=go.Heatmap(\n    z=corr_matrix,\n    x=corr_matrix.columns,\n    y=corr_matrix.columns,\n    text=np.round(corr_matrix, 2),\n    texttemplate='%{text}',\n    textfont={'size': 12},\n    hoverongaps=False,\n    colorscale='RdBu'\n))\n\nfig.update_layout(\n    title='Correlation Heatmap',\n    width=600,\n    height=600\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#box-plots-with-violin-overlay",
    "href": "posts/data-viz-plotly/index.html#box-plots-with-violin-overlay",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\n# Create box plots with violin overlay\nfig = go.Figure()\n\nfor region in df['region'].unique():\n    # Add violin plot\n    fig.add_trace(go.Violin(\n        x=df['region'][df['region'] == region],\n        y=df['sales'][df['region'] == region],\n        name=region,\n        box_visible=True,\n        meanline_visible=True\n    ))\n\nfig.update_layout(\n    title='Sales Distribution by Region',\n    xaxis_title='Region',\n    yaxis_title='Sales',\n    showlegend=False\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#d-scatter-plot",
    "href": "posts/data-viz-plotly/index.html#d-scatter-plot",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "Code\n# Create 3D scatter plot\nfig = px.scatter_3d(\n    df,\n    x='sales',\n    y='customer_satisfaction',\n    z='units_sold',\n    color='region',\n    symbol='product',\n    title='Sales Performance in 3D'\n)\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='Sales',\n        yaxis_title='Customer Satisfaction',\n        zaxis_title='Units Sold'\n    )\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/data-viz-plotly/index.html#key-takeaways",
    "href": "posts/data-viz-plotly/index.html#key-takeaways",
    "title": "Interactive Data Visualization with Plotly",
    "section": "",
    "text": "In this tutorial, we learned how to create:\n\nInteractive line plots with multiple series\nScatter plots with hover information\nAnimated bubble charts\nInteractive heatmaps\nBox plots with violin overlays\n3D scatter plots\n\nThese interactive visualizations can help you: - Explore relationships in your data - Identify patterns and trends - Communicate insights effectively - Create engaging presentations\nIn future posts, we‚Äôll explore: - Custom themes and styling - Advanced Plotly features - Building dashboards with Dash - Real-time data visualization\nStay tuned!"
  },
  {
    "objectID": "posts/Hahaha/index.html",
    "href": "posts/Hahaha/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I‚Äôm Ram Polisetti, a passionate learner exploring the world of Machine Learning and Data Science. Through BendThe-Curve, I document my learning journey, share insights, and build a community of fellow ML enthusiasts."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hi! I‚Äôm Ram Polisetti, a passionate learner exploring the world of Machine Learning and Data Science. Through BendThe-Curve, I document my learning journey, share insights, and build a community of fellow ML enthusiasts."
  },
  {
    "objectID": "about.html#why-bendthe-curve",
    "href": "about.html#why-bendthe-curve",
    "title": "About",
    "section": "Why ‚ÄúBendThe-Curve‚Äù?",
    "text": "Why ‚ÄúBendThe-Curve‚Äù?\nThe name ‚ÄúBendThe-Curve‚Äù represents the exponential growth in learning and understanding that comes with consistent practice and exploration in the field of Machine Learning. It‚Äôs about:\n\nüìà Accelerating the learning process\nüéØ Finding optimal solutions\nüîÑ Continuous improvement\nüå± Growing together as a community"
  },
  {
    "objectID": "about.html#blog-focus",
    "href": "about.html#blog-focus",
    "title": "About",
    "section": "Blog Focus",
    "text": "Blog Focus\nThis blog covers:\n\nMachine Learning Algorithms & Applications\nData Analysis & Visualization\nPython Programming\nReal-world ML Projects\nLearning Resources & Tips\n\nFeel free to reach out through any of the social links above. Let‚Äôs learn and grow together!"
  }
]