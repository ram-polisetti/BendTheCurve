{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Model Selection & Evaluation: Making the Right Choices\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [machine-learning, model-selection, evaluation, tutorial]\n",
        "image: \"model_selection.jpg\"\n",
        "description: \"A comprehensive guide to selecting and evaluating machine learning models, with practical examples and best practices.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model Selection & Evaluation in Machine Learning\n",
        "\n",
        "Choosing the right model and properly evaluating its performance are crucial skills in machine learning. This post provides a comprehensive guide to these essential topics.\n",
        "\n",
        "## Setup and Data Loading\n"
      ],
      "id": "64ead59d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_diabetes, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn')\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Load datasets\n",
        "X_cancer, y_cancer = load_breast_cancer(return_X_y=True)\n",
        "X_diabetes, y_diabetes = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split datasets\n",
        "X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_cancer_train_scaled = scaler.fit_transform(X_cancer_train)\n",
        "X_cancer_test_scaled = scaler.transform(X_cancer_test)\n",
        "X_diabetes_train_scaled = scaler.fit_transform(X_diabetes_train)\n",
        "X_diabetes_test_scaled = scaler.transform(X_diabetes_test)"
      ],
      "id": "16db1116",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Different Models\n",
        "\n",
        "### 1. Classification Models Comparison\n"
      ],
      "id": "4a4f198d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "results = []\n",
        "for name, clf in classifiers.items():\n",
        "    # Train the model\n",
        "    clf.fit(X_cancer_train_scaled, y_cancer_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_cancer_test_scaled)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_cancer_test, y_pred),\n",
        "        'Precision': precision_score(y_cancer_test, y_pred),\n",
        "        'Recall': recall_score(y_cancer_test, y_pred),\n",
        "        'F1 Score': f1_score(y_cancer_test, y_pred)\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nClassification Models Comparison:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(12, 6))\n",
        "results_melted = pd.melt(results_df, id_vars=['Model'], \n",
        "                        value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "sns.barplot(data=results_melted, x='Model', y='value', hue='variable')\n",
        "plt.title('Classification Models Performance Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "b0d192c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Cross-Validation Strategies\n"
      ],
      "id": "a90ffee1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
        "\n",
        "def compare_cv_strategies(X, y, model, cv_strategies):\n",
        "    results = []\n",
        "    \n",
        "    for name, cv in cv_strategies.items():\n",
        "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "        results.append({\n",
        "            'Strategy': name,\n",
        "            'Mean Score': scores.mean(),\n",
        "            'Std Score': scores.std()\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Define cross-validation strategies\n",
        "cv_strategies = {\n",
        "    'KFold (k=5)': KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    'StratifiedKFold': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    'TimeSeriesSplit': TimeSeriesSplit(n_splits=5)\n",
        "}\n",
        "\n",
        "# Compare strategies using Random Forest\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "cv_results = compare_cv_strategies(X_cancer_scaled, y_cancer, rf_classifier, cv_strategies)\n",
        "\n",
        "print(\"\\nCross-Validation Strategies Comparison:\")\n",
        "print(cv_results.round(3))\n",
        "\n",
        "# Visualize CV results\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=cv_results, x='Strategy', y='Mean Score', \n",
        "            yerr=cv_results['Std Score'], capsize=0.1)\n",
        "plt.title('Cross-Validation Strategies Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "967e6b0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Hyperparameter Tuning\n"
      ],
      "id": "0b504cda"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_cancer_train_scaled, y_cancer_train)\n",
        "\n",
        "# Print best parameters and score\n",
        "print(\"\\nGrid Search Results:\")\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", round(grid_search.best_score_, 3))\n",
        "\n",
        "# Create DataFrame of CV results\n",
        "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
        "cv_results = cv_results[['params', 'mean_test_score', 'std_test_score']]\n",
        "cv_results = cv_results.sort_values('mean_test_score', ascending=False).head()\n",
        "\n",
        "print(\"\\nTop 5 Parameter Combinations:\")\n",
        "print(cv_results.round(3))"
      ],
      "id": "2b0755c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Learning Curves Analysis\n"
      ],
      "id": "757efead"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "def plot_learning_curves(estimator, X, y, title):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=5, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    \n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, label='Training score')\n",
        "    plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
        "    \n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel('Training Examples')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Plot learning curves for different models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    plot_learning_curves(model, X_cancer_scaled, y_cancer, f'Learning Curves - {name}')\n",
        "    plt.show()"
      ],
      "id": "7c7eded4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Model Calibration\n"
      ],
      "id": "63a570ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "\n",
        "def plot_calibration_curves(clf, X_train, X_test, y_train, y_test, name):\n",
        "    # Train and calibrate classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "    calibrated_clf = CalibratedClassifierCV(clf, cv=5)\n",
        "    calibrated_clf.fit(X_train, y_train)\n",
        "    \n",
        "    # Get predictions\n",
        "    prob_pos = clf.predict_proba(X_test)[:, 1]\n",
        "    prob_pos_calibrated = calibrated_clf.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate calibration curves\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_test, prob_pos, n_bins=10)\n",
        "    fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(\n",
        "        y_test, prob_pos_calibrated, n_bins=10)\n",
        "    \n",
        "    # Plot calibration curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
        "    plt.plot(mean_predicted_value, fraction_of_positives, \n",
        "             label=f'Original {name}')\n",
        "    plt.plot(mean_predicted_value_cal, fraction_of_positives_cal, \n",
        "             label=f'Calibrated {name}')\n",
        "    \n",
        "    plt.xlabel('Mean predicted probability')\n",
        "    plt.ylabel('Fraction of positives')\n",
        "    plt.title(f'Calibration Curves - {name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Plot calibration curves for different models\n",
        "for name, model in models.items():\n",
        "    plot_calibration_curves(model, X_cancer_train_scaled, X_cancer_test_scaled,\n",
        "                          y_cancer_train, y_cancer_test, name)\n",
        "    plt.show()"
      ],
      "id": "a0745e66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Model Selection\n",
        "\n",
        "1. **Start Simple**\n",
        "   - Begin with simple models (e.g., logistic regression)\n",
        "   - Establish baseline performance\n",
        "   - Gradually increase complexity\n",
        "\n",
        "2. **Consider Model Characteristics**\n",
        "   - Training time and computational resources\n",
        "   - Model interpretability\n",
        "   - Prediction speed\n",
        "   - Memory usage\n",
        "\n",
        "3. **Validation Strategy**\n",
        "   - Use appropriate cross-validation\n",
        "   - Consider data distribution\n",
        "   - Account for time series if applicable\n",
        "\n",
        "4. **Hyperparameter Tuning**\n",
        "   - Start with broad search\n",
        "   - Refine promising areas\n",
        "   - Use random or grid search\n",
        "   - Consider Bayesian optimization\n",
        "\n",
        "## Common Pitfalls to Avoid\n",
        "\n",
        "1. **Overfitting**\n",
        "   - Using too complex models\n",
        "   - Not enough regularization\n",
        "   - Not using cross-validation\n",
        "\n",
        "2. **Data Leakage**\n",
        "   - Information leak in preprocessing\n",
        "   - Wrong cross-validation strategy\n",
        "   - Test set contamination\n",
        "\n",
        "3. **Wrong Metrics**\n",
        "   - Using inappropriate metrics\n",
        "   - Not considering business context\n",
        "   - Ignoring model calibration\n",
        "\n",
        "4. **Poor Validation**\n",
        "   - Not using cross-validation\n",
        "   - Wrong splitting strategy\n",
        "   - Ignoring data distribution\n",
        "\n",
        "## Model Selection Framework\n",
        "\n",
        "1. **Problem Analysis**\n",
        "   - Define objectives\n",
        "   - Understand constraints\n",
        "   - Identify success metrics\n",
        "\n",
        "2. **Data Assessment**\n",
        "   - Size and quality\n",
        "   - Feature characteristics\n",
        "   - Target distribution\n",
        "\n",
        "3. **Model Shortlisting**\n",
        "   - Consider problem type\n",
        "   - Resource constraints\n",
        "   - Interpretability needs\n",
        "\n",
        "4. **Evaluation Strategy**\n",
        "   - Choose metrics\n",
        "   - Select validation approach\n",
        "   - Define baseline\n",
        "\n",
        "5. **Implementation**\n",
        "   - Start simple\n",
        "   - Iterate and improve\n",
        "   - Document findings\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Model selection and evaluation are critical steps in the machine learning pipeline. Key takeaways:\n",
        "\n",
        "1. Start with simple models and establish baselines\n",
        "2. Use appropriate validation strategies\n",
        "3. Consider multiple evaluation metrics\n",
        "4. Be aware of common pitfalls\n",
        "5. Document your process and findings\n",
        "\n",
        "In the next post, we'll explore deep learning fundamentals and neural network architectures.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "1. Books:\n",
        "   - \"Hands-On Machine Learning\" by Aurélien Géron\n",
        "   - \"Applied Predictive Modeling\" by Max Kuhn\n",
        "\n",
        "2. Online Resources:\n",
        "   - Scikit-learn Model Selection Guide\n",
        "   - Machine Learning Mastery Model Selection Tutorials\n",
        "   - Papers with Code Benchmarks\n",
        "\n",
        "Remember: The best model is often not the most complex one, but the one that best balances performance, interpretability, and practical constraints."
      ],
      "id": "83ec3dbe"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}