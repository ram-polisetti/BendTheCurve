{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"ML Systems & Deployment: From Research to Production\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [machine-learning, deployment, mlops, system-design]\n",
        "image: \"ml_systems.jpg\"\n",
        "description: \"A comprehensive guide to designing, deploying, and maintaining machine learning systems in production environments.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Machine Learning Systems and Deployment\n",
        "\n",
        "Moving from experimental notebooks to production systems is one of the most challenging aspects of machine learning. This post covers the essential aspects of building and deploying ML systems.\n",
        "\n",
        "## 1. ML System Architecture\n",
        "\n",
        "### Basic ML Service Architecture\n"
      ],
      "id": "d5ea7ae5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from typing import Dict, List, Union, Any\n",
        "\n",
        "class MLService:\n",
        "    def __init__(self, model_path: str = None):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.feature_names = None\n",
        "        self.model_metadata = {\n",
        "            'last_training_date': None,\n",
        "            'model_version': None,\n",
        "            'performance_metrics': {}\n",
        "        }\n",
        "        \n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "    \n",
        "    def preprocess_data(self, data: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Preprocess input data.\"\"\"\n",
        "        if self.scaler is None:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.scaler.fit(data)\n",
        "        return self.scaler.transform(data)\n",
        "    \n",
        "    def train(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:\n",
        "        \"\"\"Train the model and return performance metrics.\"\"\"\n",
        "        # Store feature names\n",
        "        self.feature_names = list(X.columns)\n",
        "        \n",
        "        # Preprocess data\n",
        "        X_scaled = self.preprocess_data(X)\n",
        "        \n",
        "        # Initialize and train model\n",
        "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        self.model.fit(X_scaled, y)\n",
        "        \n",
        "        # Update metadata\n",
        "        self.model_metadata.update({\n",
        "            'last_training_date': datetime.now().isoformat(),\n",
        "            'model_version': '1.0.0',\n",
        "            'feature_names': self.feature_names\n",
        "        })\n",
        "        \n",
        "        return {'status': 'success', 'message': 'Model trained successfully'}\n",
        "    \n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions on new data.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained. Please train the model first.\")\n",
        "        \n",
        "        # Validate features\n",
        "        if not all(feat in X.columns for feat in self.feature_names):\n",
        "            raise ValueError(\"Missing required features in input data.\")\n",
        "        \n",
        "        # Preprocess and predict\n",
        "        X_scaled = self.preprocess_data(X[self.feature_names])\n",
        "        return self.model.predict(X_scaled)\n",
        "    \n",
        "    def save_model(self, path: str) -> None:\n",
        "        \"\"\"Save model and associated metadata.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save.\")\n",
        "        \n",
        "        model_artifacts = {\n",
        "            'model': self.model,\n",
        "            'scaler': self.scaler,\n",
        "            'metadata': self.model_metadata\n",
        "        }\n",
        "        joblib.dump(model_artifacts, path)\n",
        "        \n",
        "    def load_model(self, path: str) -> None:\n",
        "        \"\"\"Load model and associated metadata.\"\"\"\n",
        "        model_artifacts = joblib.load(path)\n",
        "        self.model = model_artifacts['model']\n",
        "        self.scaler = model_artifacts['scaler']\n",
        "        self.model_metadata = model_artifacts['metadata']\n",
        "        self.feature_names = self.model_metadata.get('feature_names')\n",
        "\n",
        "# Example usage\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate sample data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                         n_redundant=5, random_state=42)\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Initialize and train service\n",
        "ml_service = MLService()\n",
        "training_result = ml_service.train(X_df, y)\n",
        "print(\"Training Result:\", training_result)\n",
        "\n",
        "# Make predictions\n",
        "sample_data = X_df.head(5)\n",
        "predictions = ml_service.predict(sample_data)\n",
        "print(\"\\nSample Predictions:\", predictions)\n",
        "\n",
        "# Save and load model\n",
        "ml_service.save_model('model.joblib')\n",
        "new_service = MLService('model.joblib')"
      ],
      "id": "ea97262a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Versioning and Metadata\n"
      ],
      "id": "0c9eb76a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ModelRegistry:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.current_version = None\n",
        "    \n",
        "    def register_model(self, model_path: str, version: str,\n",
        "                      metadata: Dict[str, Any]) -> None:\n",
        "        \"\"\"Register a new model version.\"\"\"\n",
        "        self.models[version] = {\n",
        "            'path': model_path,\n",
        "            'metadata': metadata,\n",
        "            'registration_time': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "    def get_model(self, version: str = None) -> MLService:\n",
        "        \"\"\"Retrieve a specific model version.\"\"\"\n",
        "        version = version or self.current_version\n",
        "        if version not in self.models:\n",
        "            raise ValueError(f\"Model version {version} not found.\")\n",
        "        \n",
        "        return MLService(self.models[version]['path'])\n",
        "    \n",
        "    def set_current_version(self, version: str) -> None:\n",
        "        \"\"\"Set the current production model version.\"\"\"\n",
        "        if version not in self.models:\n",
        "            raise ValueError(f\"Model version {version} not found.\")\n",
        "        self.current_version = version\n",
        "    \n",
        "    def list_versions(self) -> pd.DataFrame:\n",
        "        \"\"\"List all registered model versions.\"\"\"\n",
        "        versions_data = []\n",
        "        for version, info in self.models.items():\n",
        "            data = {\n",
        "                'version': version,\n",
        "                'path': info['path'],\n",
        "                'registration_time': info['registration_time']\n",
        "            }\n",
        "            data.update(info['metadata'])\n",
        "            versions_data.append(data)\n",
        "        \n",
        "        return pd.DataFrame(versions_data)\n",
        "\n",
        "# Example usage\n",
        "registry = ModelRegistry()\n",
        "\n",
        "# Register model versions\n",
        "registry.register_model(\n",
        "    'model.joblib',\n",
        "    '1.0.0',\n",
        "    {'accuracy': 0.95, 'environment': 'production'}\n",
        ")\n",
        "\n",
        "registry.register_model(\n",
        "    'model.joblib',\n",
        "    '1.1.0',\n",
        "    {'accuracy': 0.96, 'environment': 'staging'}\n",
        ")\n",
        "\n",
        "# List versions\n",
        "print(\"Registered Models:\")\n",
        "print(registry.list_versions())"
      ],
      "id": "1fd9f51f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Monitoring and Logging\n",
        "\n",
        "### Performance Monitoring System\n"
      ],
      "id": "c88414de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ModelMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics_log = []\n",
        "        self.prediction_log = []\n",
        "        \n",
        "    def log_prediction(self, input_data: pd.DataFrame, prediction: np.ndarray,\n",
        "                      metadata: Dict[str, Any]) -> None:\n",
        "        \"\"\"Log a prediction event.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'input_shape': input_data.shape,\n",
        "            'prediction_shape': prediction.shape,\n",
        "            'metadata': metadata\n",
        "        }\n",
        "        self.prediction_log.append(log_entry)\n",
        "    \n",
        "    def log_metrics(self, metrics: Dict[str, float]) -> None:\n",
        "        \"\"\"Log performance metrics.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'metrics': metrics\n",
        "        }\n",
        "        self.metrics_log.append(log_entry)\n",
        "    \n",
        "    def get_metrics_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Get summary of logged metrics.\"\"\"\n",
        "        return pd.DataFrame(self.metrics_log)\n",
        "    \n",
        "    def get_prediction_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Get summary of prediction logs.\"\"\"\n",
        "        return pd.DataFrame(self.prediction_log)\n",
        "\n",
        "class MonitoredMLService(MLService):\n",
        "    def __init__(self, model_path: str = None):\n",
        "        super().__init__(model_path)\n",
        "        self.monitor = ModelMonitor()\n",
        "    \n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions with monitoring.\"\"\"\n",
        "        predictions = super().predict(X)\n",
        "        \n",
        "        # Log prediction event\n",
        "        self.monitor.log_prediction(\n",
        "            X,\n",
        "            predictions,\n",
        "            {'model_version': self.model_metadata.get('model_version')}\n",
        "        )\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "# Example usage\n",
        "monitored_service = MonitoredMLService()\n",
        "monitored_service.train(X_df, y)\n",
        "\n",
        "# Make predictions with monitoring\n",
        "for _ in range(5):\n",
        "    sample_data = X_df.sample(n=10)\n",
        "    predictions = monitored_service.predict(sample_data)\n",
        "\n",
        "# View monitoring logs\n",
        "print(\"\\nPrediction Logs:\")\n",
        "print(monitored_service.monitor.get_prediction_summary())"
      ],
      "id": "bd99e60a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Drift Detection\n"
      ],
      "id": "26857e0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy import stats\n",
        "\n",
        "class DriftDetector:\n",
        "    def __init__(self, reference_data: pd.DataFrame):\n",
        "        self.reference_data = reference_data\n",
        "        self.reference_statistics = self._compute_statistics(reference_data)\n",
        "    \n",
        "    def _compute_statistics(self, data: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Compute summary statistics for each feature.\"\"\"\n",
        "        stats_dict = {}\n",
        "        for column in data.columns:\n",
        "            stats_dict[column] = {\n",
        "                'mean': data[column].mean(),\n",
        "                'std': data[column].std(),\n",
        "                'median': data[column].median(),\n",
        "                'q1': data[column].quantile(0.25),\n",
        "                'q3': data[column].quantile(0.75)\n",
        "            }\n",
        "        return stats_dict\n",
        "    \n",
        "    def detect_drift(self, new_data: pd.DataFrame,\n",
        "                    threshold: float = 0.05) -> Dict[str, Any]:\n",
        "        \"\"\"Detect drift in new data compared to reference data.\"\"\"\n",
        "        drift_results = {}\n",
        "        \n",
        "        for column in self.reference_data.columns:\n",
        "            # Perform Kolmogorov-Smirnov test\n",
        "            ks_statistic, p_value = stats.ks_2samp(\n",
        "                self.reference_data[column],\n",
        "                new_data[column]\n",
        "            )\n",
        "            \n",
        "            # Compare statistics\n",
        "            ref_stats = self.reference_statistics[column]\n",
        "            new_stats = self._compute_statistics(new_data)[column]\n",
        "            \n",
        "            # Calculate relative changes\n",
        "            stat_changes = {\n",
        "                stat: abs(new_stats[stat] - ref_stats[stat]) / abs(ref_stats[stat])\n",
        "                for stat in ref_stats.keys()\n",
        "            }\n",
        "            \n",
        "            drift_results[column] = {\n",
        "                'ks_statistic': ks_statistic,\n",
        "                'p_value': p_value,\n",
        "                'has_drift': p_value < threshold,\n",
        "                'stat_changes': stat_changes\n",
        "            }\n",
        "        \n",
        "        return drift_results\n",
        "\n",
        "# Example usage\n",
        "# Split data into reference and new\n",
        "reference_data = X_df.iloc[:800]\n",
        "new_data = X_df.iloc[800:]\n",
        "\n",
        "# Initialize drift detector\n",
        "drift_detector = DriftDetector(reference_data)\n",
        "\n",
        "# Detect drift\n",
        "drift_results = drift_detector.detect_drift(new_data)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nDrift Detection Results:\")\n",
        "for feature, results in drift_results.items():\n",
        "    if results['has_drift']:\n",
        "        print(f\"\\nDrift detected in {feature}:\")\n",
        "        print(f\"p-value: {results['p_value']:.4f}\")\n",
        "        print(\"Statistical changes:\")\n",
        "        for stat, change in results['stat_changes'].items():\n",
        "            print(f\"  {stat}: {change:.2%}\")"
      ],
      "id": "2f69f836",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Serving Infrastructure\n",
        "\n",
        "### REST API Service\n"
      ],
      "id": "a6d99c7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict\n",
        "\n",
        "# Define data models\n",
        "class PredictionRequest(BaseModel):\n",
        "    features: List[List[float]]\n",
        "    feature_names: List[str]\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    predictions: List[int]\n",
        "    model_version: str\n",
        "    prediction_time: str\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(title=\"ML Model Service\")\n",
        "\n",
        "# Initialize ML service\n",
        "ml_service = MonitoredMLService()\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict(request: PredictionRequest):\n",
        "    try:\n",
        "        # Convert input to DataFrame\n",
        "        input_data = pd.DataFrame(\n",
        "            request.features,\n",
        "            columns=request.feature_names\n",
        "        )\n",
        "        \n",
        "        # Make prediction\n",
        "        predictions = ml_service.predict(input_data)\n",
        "        \n",
        "        return PredictionResponse(\n",
        "            predictions=predictions.tolist(),\n",
        "            model_version=ml_service.model_metadata['model_version'],\n",
        "            prediction_time=datetime.now().isoformat()\n",
        "        )\n",
        "    \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Note: In a real application, you would run this with uvicorn\n",
        "print(\"\\nAPI Documentation:\")\n",
        "print(\"POST /predict\")\n",
        "print(\"Request body schema:\", PredictionRequest.schema())\n",
        "print(\"Response body schema:\", PredictionResponse.schema())"
      ],
      "id": "b9a6291e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deployment Strategies\n",
        "\n",
        "### A/B Testing Implementation\n"
      ],
      "id": "319cdf87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ABTestingService:\n",
        "    def __init__(self, model_a: MLService, model_b: MLService,\n",
        "                 traffic_split: float = 0.5):\n",
        "        self.model_a = model_a\n",
        "        self.model_b = model_b\n",
        "        self.traffic_split = traffic_split\n",
        "        self.results = {\n",
        "            'model_a': {'predictions': 0, 'errors': 0},\n",
        "            'model_b': {'predictions': 0, 'errors': 0}\n",
        "        }\n",
        "    \n",
        "    def predict(self, X: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Make prediction using A/B testing.\"\"\"\n",
        "        # Randomly select model based on traffic split\n",
        "        if np.random.random() < self.traffic_split:\n",
        "            model = self.model_a\n",
        "            model_name = 'model_a'\n",
        "        else:\n",
        "            model = self.model_b\n",
        "            model_name = 'model_b'\n",
        "        \n",
        "        try:\n",
        "            prediction = model.predict(X)\n",
        "            self.results[model_name]['predictions'] += 1\n",
        "            \n",
        "            return {\n",
        "                'prediction': prediction,\n",
        "                'model_used': model_name,\n",
        "                'model_version': model.model_metadata['model_version']\n",
        "            }\n",
        "        \n",
        "        except Exception as e:\n",
        "            self.results[model_name]['errors'] += 1\n",
        "            raise e\n",
        "    \n",
        "    def get_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get A/B testing statistics.\"\"\"\n",
        "        return {\n",
        "            'model_a_stats': {\n",
        "                'usage_percentage': self.results['model_a']['predictions'] /\n",
        "                    max(1, sum(r['predictions'] for r in self.results.values())),\n",
        "                'error_rate': self.results['model_a']['errors'] /\n",
        "                    max(1, self.results['model_a']['predictions'])\n",
        "            },\n",
        "            'model_b_stats': {\n",
        "                'usage_percentage': self.results['model_b']['predictions'] /\n",
        "                    max(1, sum(r['predictions'] for r in self.results.values())),\n",
        "                'error_rate': self.results['model_b']['errors'] /\n",
        "                    max(1, self.results['model_b']['predictions'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "# Create two model versions\n",
        "model_a = MLService()\n",
        "model_b = MLService()\n",
        "\n",
        "# Train models with slightly different data\n",
        "model_a.train(X_df.iloc[:800], y[:800])\n",
        "model_b.train(X_df.iloc[200:], y[200:])\n",
        "\n",
        "# Initialize A/B testing service\n",
        "ab_service = ABTestingService(model_a, model_b, traffic_split=0.7)\n",
        "\n",
        "# Make predictions\n",
        "for _ in range(100):\n",
        "    sample_data = X_df.sample(n=1)\n",
        "    try:\n",
        "        result = ab_service.predict(sample_data)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Get statistics\n",
        "print(\"\\nA/B Testing Statistics:\")\n",
        "print(json.dumps(ab_service.get_statistics(), indent=2))"
      ],
      "id": "d6f0f0ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for ML Systems\n",
        "\n",
        "1. **System Design**\n",
        "   - Modular architecture\n",
        "   - Clear separation of concerns\n",
        "   - Proper error handling\n",
        "   - Comprehensive logging\n",
        "\n",
        "2. **Deployment**\n",
        "   - Automated testing\n",
        "   - Gradual rollout\n",
        "   - Monitoring and alerting\n",
        "   - Rollback capabilities\n",
        "\n",
        "3. **Maintenance**\n",
        "   - Regular model updates\n",
        "   - Performance monitoring\n",
        "   - Data drift detection\n",
        "   - Resource optimization\n",
        "\n",
        "4. **Documentation**\n",
        "   - System architecture\n",
        "   - API specifications\n",
        "   - Model versioning\n",
        "   - Deployment procedures\n",
        "\n",
        "## Common Challenges and Solutions\n",
        "\n",
        "1. **Scaling Issues**\n",
        "   - Load balancing\n",
        "   - Caching strategies\n",
        "   - Batch processing\n",
        "   - Resource management\n",
        "\n",
        "2. **Model Degradation**\n",
        "   - Regular retraining\n",
        "   - Performance monitoring\n",
        "   - Data quality checks\n",
        "   - Version control\n",
        "\n",
        "3. **System Reliability**\n",
        "   - Redundancy\n",
        "   - Error handling\n",
        "   - Failover strategies\n",
        "   - Backup procedures\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Building production ML systems requires:\n",
        "\n",
        "1. Robust architecture\n",
        "2. Comprehensive monitoring\n",
        "3. Effective deployment strategies\n",
        "4. Proper maintenance procedures\n",
        "\n",
        "In the next post, we'll explore specific industry applications and case studies.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "1. Books:\n",
        "   - \"Designing Machine Learning Systems\" by Chip Huyen\n",
        "   - \"Machine Learning Design Patterns\" by Lakshmanan et al.\n",
        "   - \"Building Machine Learning Powered Applications\" by Emmanuel Ameisen\n",
        "\n",
        "2. Online Resources:\n",
        "   - MLOps Community\n",
        "   - Google Cloud ML Operations\n",
        "   - AWS ML Deployment Best Practices\n",
        "\n",
        "Remember: A well-designed ML system is more than just a modelâ€”it's a complete solution that handles data processing, monitoring, and maintenance effectively."
      ],
      "id": "83574010"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}