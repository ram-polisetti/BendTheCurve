{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Deep Learning Fundamentals: From Neurons to Neural Networks\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [deep-learning, neural-networks, pytorch, tutorial]\n",
        "image: \"neural_network.jpg\"\n",
        "description: \"A comprehensive introduction to deep learning fundamentals, covering key concepts, architectures, and practical PyTorch implementations.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Deep Learning Fundamentals: Understanding Neural Networks\n",
        "\n",
        "Deep learning has revolutionized machine learning with its ability to automatically learn hierarchical representations. This post covers the fundamental concepts and practical implementations using PyTorch.\n",
        "\n",
        "## Setup and Prerequisites\n"
      ],
      "id": "6b99dbd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "id": "593eb18b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Neural Network Basics\n",
        "\n",
        "### The Building Blocks: Neurons and Activation Functions\n"
      ],
      "id": "c404ce2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Implement a simple neuron\n",
        "def neuron(x, w, b, activation_fn):\n",
        "    z = np.dot(w, x) + b\n",
        "    return activation_fn(z)\n",
        "\n",
        "# Common activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Visualize activation functions\n",
        "x = np.linspace(-5, 5, 100)\n",
        "activations = {\n",
        "    'ReLU': relu,\n",
        "    'Sigmoid': sigmoid,\n",
        "    'Tanh': tanh\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (name, fn) in enumerate(activations.items(), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.plot(x, fn(x))\n",
        "    plt.title(f'{name} Activation')\n",
        "    plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "a7f82982",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forward and Backward Propagation\n"
      ],
      "id": "2acc8893"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple neural network implementation from scratch\n",
        "class BasicNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.b1 = np.zeros((hidden_size, 1))\n",
        "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
        "        self.b2 = np.zeros((output_size, 1))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # First layer\n",
        "        self.z1 = np.dot(self.W1, X) + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "        \n",
        "        # Second layer\n",
        "        self.z2 = np.dot(self.W2, self.a1) + self.b2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def backward(self, X, y, learning_rate=0.01):\n",
        "        m = X.shape[1]\n",
        "        \n",
        "        # Backward propagation\n",
        "        dz2 = self.a2 - y\n",
        "        dW2 = (1/m) * np.dot(dz2, self.a1.T)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
        "        \n",
        "        da1 = np.dot(self.W2.T, dz2)\n",
        "        dz1 = da1 * (self.z1 > 0)  # ReLU derivative\n",
        "        dW1 = (1/m) * np.dot(dz1, X.T)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
        "        \n",
        "        # Update parameters\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "\n",
        "# Demonstrate with a simple XOR problem\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
        "y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "nn = BasicNeuralNetwork(2, 4, 1)\n",
        "\n",
        "# Training\n",
        "losses = []\n",
        "for i in range(10000):\n",
        "    # Forward pass\n",
        "    output = nn.forward(X)\n",
        "    loss = -np.mean(y * np.log(output) + (1-y) * np.log(1-output))\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # Backward pass\n",
        "    nn.backward(X, y)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "138fea1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PyTorch Implementation\n",
        "\n",
        "### Creating a Neural Network in PyTorch\n"
      ],
      "id": "5d71f9a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reset any previous PyTorch models\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# PyTorch Neural Network\n",
        "class TorchNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "# Create synthetic dataset\n",
        "class SyntheticDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000):\n",
        "        X = torch.randn(num_samples, 10)\n",
        "        y = torch.sum(X[:, :5], dim=1) > 0  # Binary classification\n",
        "        self.X = X\n",
        "        self.y = y.long()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = SyntheticDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "torch_model = TorchNeuralNetwork(10, 64, 2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(torch_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    \n",
        "    return train_losses\n",
        "\n",
        "# Train the model\n",
        "torch_losses = train_model(torch_model, dataloader, criterion, optimizer)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(torch_losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "749d1980",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding Model Architecture Components\n",
        "\n",
        "### Layers and Their Functions\n"
      ],
      "id": "60ffc585"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrate different layer types\n",
        "class LayerDemo(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LayerDemo, self).__init__()\n",
        "        \n",
        "        # Common layer types\n",
        "        self.linear = nn.Linear(input_size, 64)\n",
        "        self.conv1d = nn.Conv1d(1, 16, kernel_size=3)\n",
        "        self.conv2d = nn.Conv2d(1, 16, kernel_size=3)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batchnorm = nn.BatchNorm1d(64)\n",
        "        self.layernorm = nn.LayerNorm(64)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # This is just for demonstration\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Print model architecture\n",
        "model = LayerDemo(input_size=10)\n",
        "print(model)"
      ],
      "id": "7468ff82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialization and Regularization\n"
      ],
      "id": "25ff6f8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "# Demonstrate weight initialization\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Visualize weight distributions\n",
        "def plot_weight_distribution(model):\n",
        "    weights = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            weights.extend(param.data.cpu().numpy().flatten())\n",
        "    \n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.hist(weights, bins=50)\n",
        "    plt.title('Weight Distribution')\n",
        "    plt.xlabel('Weight Value')\n",
        "    plt.ylabel('Count')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_weight_distribution(model)"
      ],
      "id": "3ea333c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Deep Neural Networks\n",
        "\n",
        "### Loss Functions and Optimizers\n"
      ],
      "id": "085f89aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrate different loss functions\n",
        "def plot_loss_functions():\n",
        "    x = torch.linspace(-3, 3, 100)\n",
        "    y_true = torch.ones_like(x)\n",
        "    \n",
        "    # Common loss functions\n",
        "    mse = nn.MSELoss()(x.view(-1, 1), y_true.view(-1, 1))\n",
        "    bce = nn.BCEWithLogitsLoss()(x.view(-1, 1), y_true.view(-1, 1))\n",
        "    l1 = nn.L1Loss()(x.view(-1, 1), y_true.view(-1, 1))\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(131)\n",
        "    plt.plot(x.numpy(), mse.numpy())\n",
        "    plt.title('MSE Loss')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    plt.plot(x.numpy(), bce.numpy())\n",
        "    plt.title('BCE Loss')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    plt.plot(x.numpy(), l1.numpy())\n",
        "    plt.title('L1 Loss')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_functions()"
      ],
      "id": "0ec40cb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Rate Scheduling\n"
      ],
      "id": "f1a6e39d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_lr_schedules():\n",
        "    epochs = 100\n",
        "    initial_lr = 0.1\n",
        "    \n",
        "    # Different learning rate schedules\n",
        "    step_lr = []\n",
        "    exp_lr = []\n",
        "    cosine_lr = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Step LR\n",
        "        step_lr.append(initial_lr * (0.1 ** (epoch // 30)))\n",
        "        \n",
        "        # Exponential LR\n",
        "        exp_lr.append(initial_lr * (0.95 ** epoch))\n",
        "        \n",
        "        # Cosine LR\n",
        "        cosine_lr.append(initial_lr * (1 + np.cos(np.pi * epoch / epochs)) / 2)\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(step_lr, label='Step LR')\n",
        "    plt.plot(exp_lr, label='Exponential LR')\n",
        "    plt.plot(cosine_lr, label='Cosine LR')\n",
        "    plt.title('Learning Rate Schedules')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_lr_schedules()"
      ],
      "id": "2d4408c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Common Challenges and Solutions\n",
        "\n",
        "### Vanishing/Exploding Gradients\n"
      ],
      "id": "9f032c72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_gradient_problems():\n",
        "    # Create a deep network\n",
        "    class DeepNet(nn.Module):\n",
        "        def __init__(self, depth):\n",
        "            super(DeepNet, self).__init__()\n",
        "            self.layers = nn.ModuleList([\n",
        "                nn.Linear(10, 10) for _ in range(depth)\n",
        "            ])\n",
        "            self.activation = nn.Tanh()\n",
        "        \n",
        "        def forward(self, x):\n",
        "            for layer in self.layers:\n",
        "                x = self.activation(layer(x))\n",
        "            return x\n",
        "    \n",
        "    # Create networks with different depths\n",
        "    shallow_net = DeepNet(depth=3)\n",
        "    deep_net = DeepNet(depth=20)\n",
        "    \n",
        "    # Initialize with different scales\n",
        "    def init_weights_scale(m, scale=1.0):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data *= scale\n",
        "    \n",
        "    shallow_net.apply(lambda m: init_weights_scale(m, 1.0))\n",
        "    deep_net.apply(lambda m: init_weights_scale(m, 1.0))\n",
        "    \n",
        "    # Forward pass\n",
        "    x = torch.randn(1, 10)\n",
        "    shallow_output = shallow_net(x)\n",
        "    deep_output = deep_net(x)\n",
        "    \n",
        "    print(\"Shallow Network Output Magnitude:\", torch.norm(shallow_output).item())\n",
        "    print(\"Deep Network Output Magnitude:\", torch.norm(deep_output).item())\n",
        "\n",
        "demonstrate_gradient_problems()"
      ],
      "id": "18eb30e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Normalization and Residual Connections\n"
      ],
      "id": "41fb597c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Visualize feature maps\n",
        "def plot_feature_maps(x, layer_outputs, title):\n",
        "    fig, axes = plt.subplots(1, len(layer_outputs), figsize=(15, 3))\n",
        "    for i, output in enumerate(layer_outputs):\n",
        "        axes[i].imshow(output[0, 0].detach().numpy(), cmap='viridis')\n",
        "        axes[i].set_title(f'Layer {i+1}')\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create sample data and demonstrate\n",
        "x = torch.randn(1, 1, 28, 28)\n",
        "block = ResidualBlock(1)\n",
        "output = block(x)\n",
        "\n",
        "# Collect intermediate outputs\n",
        "layer_outputs = [x, output]\n",
        "plot_feature_maps(x, layer_outputs, 'Feature Maps through Residual Block')"
      ],
      "id": "401471eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices and Guidelines\n",
        "\n",
        "1. **Architecture Design**\n",
        "   - Start with proven architectures\n",
        "   - Use appropriate layer types\n",
        "   - Consider computational constraints\n",
        "\n",
        "2. **Training Strategy**\n",
        "   - Choose suitable optimizers\n",
        "   - Implement learning rate scheduling\n",
        "   - Use proper batch sizes\n",
        "\n",
        "3. **Regularization**\n",
        "   - Apply dropout\n",
        "   - Use weight decay\n",
        "   - Implement early stopping\n",
        "\n",
        "4. **Monitoring and Debugging**\n",
        "   - Track training metrics\n",
        "   - Visualize gradients\n",
        "   - Monitor resource usage\n",
        "\n",
        "## Common Pitfalls to Avoid\n",
        "\n",
        "1. **Architecture Issues**\n",
        "   - Too deep/shallow networks\n",
        "   - Inappropriate layer sizes\n",
        "   - Wrong activation functions\n",
        "\n",
        "2. **Training Problems**\n",
        "   - Poor initialization\n",
        "   - Incorrect learning rates\n",
        "   - Unstable gradients\n",
        "\n",
        "3. **Data Issues**\n",
        "   - Insufficient preprocessing\n",
        "   - Imbalanced datasets\n",
        "   - Poor data augmentation\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Understanding deep learning fundamentals is crucial for:\n",
        "\n",
        "1. Building effective models\n",
        "2. Debugging training issues\n",
        "3. Optimizing performance\n",
        "4. Choosing appropriate architectures\n",
        "\n",
        "In the next post, we'll explore advanced deep learning topics and specialized architectures.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "1. Books:\n",
        "   - \"Deep Learning\" by Goodfellow, Bengio, and Courville\n",
        "   - \"Deep Learning with PyTorch\" by Stevens, Antiga, and Viehmann\n",
        "\n",
        "2. Online Resources:\n",
        "   - PyTorch Documentation\n",
        "   - Deep Learning Course by fast.ai\n",
        "   - Stanford CS231n Course Notes\n",
        "\n",
        "Remember: Deep learning is powerful but requires careful consideration of fundamentals for successful implementation."
      ],
      "id": "ec405adc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}