{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Machine Learning in Computer Vision: From Pixels to Understanding\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [machine-learning, computer-vision, deep-learning, pytorch]\n",
        "image: \"computer_vision.jpg\"\n",
        "description: \"A comprehensive guide to computer vision with machine learning, covering fundamental concepts, modern architectures, and practical implementations.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Machine Learning in Computer Vision\n",
        "\n",
        "Computer vision is one of the most successful applications of machine learning, enabling computers to understand and process visual information. This post explores key concepts and implementations in computer vision.\n",
        "\n",
        "## Setup and Prerequisites\n"
      ],
      "id": "4eb70fc8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Plotting settings\n",
        "plt.style.use('seaborn')\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "id": "face8c17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Image Processing Fundamentals\n",
        "\n",
        "### Basic Image Operations\n"
      ],
      "id": "4b22df3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_and_process_image(image_path):\n",
        "    \"\"\"Load and process an image for visualization.\"\"\"\n",
        "    # Load image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    tensor = transform(image)\n",
        "    return tensor\n",
        "\n",
        "def show_image_transformations(tensor):\n",
        "    \"\"\"Display various image transformations.\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Original\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(tensor.permute(1, 2, 0))\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Grayscale\n",
        "    plt.subplot(132)\n",
        "    gray = tensor.mean(dim=0)\n",
        "    plt.imshow(gray, cmap='gray')\n",
        "    plt.title('Grayscale')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Edge Detection (Sobel)\n",
        "    plt.subplot(133)\n",
        "    sobel_x = torch.tensor([[-1, 0, 1],\n",
        "                           [-2, 0, 2],\n",
        "                           [-1, 0, 1]]).float()\n",
        "    sobel_y = sobel_x.t()\n",
        "    \n",
        "    gray_expanded = gray.unsqueeze(0).unsqueeze(0)\n",
        "    edges_x = F.conv2d(gray_expanded, sobel_x.view(1, 1, 3, 3), padding=1)\n",
        "    edges_y = F.conv2d(gray_expanded, sobel_y.view(1, 1, 3, 3), padding=1)\n",
        "    edges = torch.sqrt(edges_x.pow(2) + edges_y.pow(2))[0, 0]\n",
        "    \n",
        "    plt.imshow(edges, cmap='gray')\n",
        "    plt.title('Edge Detection')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage with CIFAR-10\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=transforms.ToTensor())\n",
        "sample_image = trainset[0][0]\n",
        "show_image_transformations(sample_image)"
      ],
      "id": "b2447dba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convolutional Neural Networks\n",
        "\n",
        "### Basic CNN Architecture\n"
      ],
      "id": "7c065fe7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Visualize feature maps\n",
        "def visualize_feature_maps(model, image):\n",
        "    \"\"\"Visualize feature maps from different layers.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get feature maps from first conv layer\n",
        "        conv1_output = F.relu(model.conv1(image.unsqueeze(0)))\n",
        "        \n",
        "        # Plot first 16 feature maps\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        for i in range(min(16, conv1_output.shape[1])):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            plt.imshow(conv1_output[0, i].cpu(), cmap='viridis')\n",
        "            plt.axis('off')\n",
        "        plt.suptitle('First Conv Layer Feature Maps')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Create model and visualize\n",
        "model = SimpleCNN().to(device)\n",
        "sample_image = sample_image.to(device)\n",
        "visualize_feature_maps(model, sample_image)"
      ],
      "id": "54b15cf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transfer Learning with ResNet\n"
      ],
      "id": "33406e62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_transfer_learning_model(num_classes=10):\n",
        "    \"\"\"Create a transfer learning model using ResNet-18.\"\"\"\n",
        "    # Load pre-trained ResNet\n",
        "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "    \n",
        "    # Freeze all layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Replace final layer\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, num_classes)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create transfer learning model\n",
        "transfer_model = create_transfer_learning_model().to(device)\n",
        "\n",
        "# Visualize model architecture\n",
        "def print_model_structure(model):\n",
        "    \"\"\"Print model structure with parameter counts.\"\"\"\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        param_count = param.numel()\n",
        "        total_params += param_count\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param_count\n",
        "        print(f\"{name}: {param.shape}, Parameters: {param_count:,}\")\n",
        "    \n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "\n",
        "print_model_structure(transfer_model)"
      ],
      "id": "5d8a9f8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Object Detection\n",
        "\n",
        "### Simple Object Detection Pipeline\n"
      ],
      "id": "9e6d50ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ObjectDetector:\n",
        "    def __init__(self):\n",
        "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
        "            weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "        )\n",
        "        self.model.eval()\n",
        "        \n",
        "        # COCO class labels\n",
        "        self.COCO_CLASSES = [\n",
        "            'background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "            'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
        "            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "            'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
        "            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "            'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
        "            'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "            'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
        "            'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "            'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "        ]\n",
        "    \n",
        "    def detect(self, image, confidence_threshold=0.5):\n",
        "        \"\"\"Detect objects in an image.\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model([image.to(device)])\n",
        "        \n",
        "        # Get predictions above threshold\n",
        "        boxes = predictions[0]['boxes']\n",
        "        scores = predictions[0]['scores']\n",
        "        labels = predictions[0]['labels']\n",
        "        \n",
        "        mask = scores > confidence_threshold\n",
        "        boxes = boxes[mask]\n",
        "        scores = scores[mask]\n",
        "        labels = labels[mask]\n",
        "        \n",
        "        return boxes, scores, labels\n",
        "    \n",
        "    def draw_detections(self, image, boxes, scores, labels):\n",
        "        \"\"\"Draw detection boxes on image.\"\"\"\n",
        "        image_np = image.permute(1, 2, 0).cpu().numpy().copy()\n",
        "        \n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            box = box.cpu().numpy()\n",
        "            label = label.cpu().item()\n",
        "            score = score.cpu().item()\n",
        "            \n",
        "            # Draw box\n",
        "            plt.gca().add_patch(plt.Rectangle(\n",
        "                (box[0], box[1]),\n",
        "                box[2] - box[0],\n",
        "                box[3] - box[1],\n",
        "                fill=False,\n",
        "                color='red',\n",
        "                linewidth=2\n",
        "            ))\n",
        "            \n",
        "            # Add label\n",
        "            plt.text(\n",
        "                box[0], box[1] - 5,\n",
        "                f'{self.COCO_CLASSES[label]}: {score:.2f}',\n",
        "                color='red',\n",
        "                fontsize=8,\n",
        "                bbox=dict(facecolor='white', alpha=0.8)\n",
        "            )\n",
        "        \n",
        "        plt.imshow(image_np)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "detector = ObjectDetector()\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((800, 800)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use sample image\n",
        "sample_image = transform(trainset[0][0])\n",
        "boxes, scores, labels = detector.detect(sample_image)\n",
        "plt.figure(figsize=(12, 12))\n",
        "detector.draw_detections(sample_image, boxes, scores, labels)"
      ],
      "id": "cebb003f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Image Segmentation\n",
        "\n",
        "### Semantic Segmentation\n"
      ],
      "id": "d7bb5438"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=2):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(n_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.conv_block(256, 128)\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.conv_block(128, 64)\n",
        "        \n",
        "        # Final layer\n",
        "        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "    \n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.enc1(x)\n",
        "        enc1_pool = F.max_pool2d(enc1, 2)\n",
        "        enc2 = self.enc2(enc1_pool)\n",
        "        enc2_pool = F.max_pool2d(enc2, 2)\n",
        "        enc3 = self.enc3(enc2_pool)\n",
        "        \n",
        "        # Decoder\n",
        "        dec1 = self.up1(enc3)\n",
        "        dec1 = torch.cat([dec1, enc2], dim=1)\n",
        "        dec1 = self.dec1(dec1)\n",
        "        dec2 = self.up2(dec1)\n",
        "        dec2 = torch.cat([dec2, enc1], dim=1)\n",
        "        dec2 = self.dec2(dec2)\n",
        "        \n",
        "        return self.final(dec2)\n",
        "\n",
        "# Create synthetic segmentation data\n",
        "def create_synthetic_segmentation_data(size=128):\n",
        "    \"\"\"Create synthetic image and mask for segmentation.\"\"\"\n",
        "    # Create random shapes\n",
        "    image = np.zeros((size, size, 3))\n",
        "    mask = np.zeros((size, size))\n",
        "    \n",
        "    # Add random circles\n",
        "    for _ in range(3):\n",
        "        center = np.random.randint(0, size, 2)\n",
        "        radius = np.random.randint(10, 30)\n",
        "        color = np.random.rand(3)\n",
        "        \n",
        "        y, x = np.ogrid[:size, :size]\n",
        "        dist = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
        "        circle = dist <= radius\n",
        "        \n",
        "        image[circle] = color\n",
        "        mask[circle] = 1\n",
        "    \n",
        "    return (torch.FloatTensor(image.transpose(2, 0, 1)),\n",
        "            torch.LongTensor(mask))\n",
        "\n",
        "# Create model and synthetic data\n",
        "segmentation_model = SimpleUNet().to(device)\n",
        "image, mask = create_synthetic_segmentation_data()\n",
        "image = image.to(device)\n",
        "\n",
        "# Get prediction\n",
        "segmentation_model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = segmentation_model(image.unsqueeze(0))\n",
        "    pred = torch.argmax(pred, dim=1)[0]\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(131)\n",
        "plt.imshow(image.cpu().permute(1, 2, 0))\n",
        "plt.title('Input Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(mask.cpu(), cmap='gray')\n",
        "plt.title('True Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(pred.cpu(), cmap='gray')\n",
        "plt.title('Predicted Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "19bcdeac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices in Computer Vision\n",
        "\n",
        "1. **Data Preprocessing**\n",
        "   - Proper normalization\n",
        "   - Data augmentation\n",
        "   - Balanced datasets\n",
        "   - Resolution considerations\n",
        "\n",
        "2. **Model Architecture**\n",
        "   - Start with proven architectures\n",
        "   - Use transfer learning\n",
        "   - Consider computational constraints\n",
        "   - Implement proper regularization\n",
        "\n",
        "3. **Training Strategy**\n",
        "   - Learning rate scheduling\n",
        "   - Proper batch size selection\n",
        "   - Validation strategy\n",
        "   - Early stopping\n",
        "\n",
        "4. **Deployment Considerations**\n",
        "   - Model optimization\n",
        "   - Inference speed\n",
        "   - Memory constraints\n",
        "   - Hardware acceleration\n",
        "\n",
        "## Common Challenges and Solutions\n",
        "\n",
        "1. **Data Quality**\n",
        "   - Lighting variations\n",
        "   - Occlusions\n",
        "   - Scale differences\n",
        "   - Background clutter\n",
        "\n",
        "2. **Model Performance**\n",
        "   - Overfitting\n",
        "   - Class imbalance\n",
        "   - Domain adaptation\n",
        "   - Real-time constraints\n",
        "\n",
        "3. **Deployment Issues**\n",
        "   - Model size\n",
        "   - Inference speed\n",
        "   - Hardware limitations\n",
        "   - Edge deployment\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Computer vision with ML requires:\n",
        "\n",
        "1. Strong understanding of fundamentals\n",
        "2. Proper model selection and training\n",
        "3. Effective preprocessing and augmentation\n",
        "4. Consideration of deployment constraints\n",
        "\n",
        "In the next post, we'll explore natural language processing applications.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "1. Books:\n",
        "   - \"Deep Learning for Computer Vision\" by Adrian Rosebrock\n",
        "   - \"Computer Vision: Algorithms and Applications\" by Richard Szeliski\n",
        "   - \"Deep Learning\" by Goodfellow, Bengio, and Courville\n",
        "\n",
        "2. Online Resources:\n",
        "   - PyTorch Vision Tutorials\n",
        "   - Stanford CS231n Course\n",
        "   - Fast.ai Computer Vision Course\n",
        "\n",
        "Remember: Computer vision is a rapidly evolving field. Stay updated with the latest research and techniques while maintaining a strong grasp of the fundamentals."
      ],
      "id": "345fc194"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}