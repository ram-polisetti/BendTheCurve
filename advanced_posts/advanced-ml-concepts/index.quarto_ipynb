{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Advanced ML Concepts: Beyond the Basics\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [machine-learning, advanced-concepts, bayesian-methods, causal-inference]\n",
        "image: \"advanced_ml.jpg\"\n",
        "description: \"Exploring advanced machine learning concepts including probabilistic ML, Bayesian methods, uncertainty estimation, and causal inference.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Advanced Machine Learning Concepts\n",
        "\n",
        "While basic machine learning concepts form the foundation, advanced concepts are crucial for handling real-world complexity and uncertainty. This post explores sophisticated approaches that are often overlooked but essential for advanced practitioners.\n",
        "\n",
        "## Setup and Prerequisites\n"
      ],
      "id": "34b6fdb8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "pyro.set_rng_seed(42)\n",
        "\n",
        "# Plotting settings\n",
        "plt.style.use('seaborn')\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "id": "309be081",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Probabilistic Machine Learning\n",
        "\n",
        "### Bayesian Linear Regression\n"
      ],
      "id": "3490be15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Bayesian linear regression model\n",
        "def model(X, y=None):\n",
        "    # Priors\n",
        "    weight = pyro.sample('weight', dist.Normal(0., 10.))\n",
        "    bias = pyro.sample('bias', dist.Normal(0., 10.))\n",
        "    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n",
        "    \n",
        "    # Linear regression\n",
        "    mean = weight * X + bias\n",
        "    \n",
        "    # Likelihood\n",
        "    with pyro.plate('data', X.shape[0]):\n",
        "        return pyro.sample('obs', dist.Normal(mean, sigma), obs=y)\n",
        "\n",
        "# Perform inference\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# Define guide (variational distribution)\n",
        "def guide(X, y=None):\n",
        "    # Register parameters\n",
        "    weight_loc = pyro.param('weight_loc', torch.tensor(0.))\n",
        "    weight_scale = pyro.param('weight_scale', torch.tensor(1.),\n",
        "                             constraint=dist.constraints.positive)\n",
        "    bias_loc = pyro.param('bias_loc', torch.tensor(0.))\n",
        "    bias_scale = pyro.param('bias_scale', torch.tensor(1.),\n",
        "                           constraint=dist.constraints.positive)\n",
        "    sigma_loc = pyro.param('sigma_loc', torch.tensor(1.),\n",
        "                          constraint=dist.constraints.positive)\n",
        "    \n",
        "    # Sample from variational distributions\n",
        "    pyro.sample('weight', dist.Normal(weight_loc, weight_scale))\n",
        "    pyro.sample('bias', dist.Normal(bias_loc, bias_scale))\n",
        "    pyro.sample('sigma', dist.HalfNormal(sigma_loc))\n",
        "\n",
        "# Setup inference\n",
        "optimizer = Adam({\"lr\": 0.03})\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
        "\n",
        "# Training loop\n",
        "pyro.clear_param_store()\n",
        "num_iterations = 1000\n",
        "losses = []\n",
        "\n",
        "X_tensor = torch.tensor(X_train.astype(np.float32))\n",
        "y_tensor = torch.tensor(y_train.astype(np.float32))\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    loss = svi.step(X_tensor, y_tensor)\n",
        "    losses.append(loss)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('ELBO Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Visualize posterior predictions\n",
        "def get_posterior_predictions(X):\n",
        "    X_tensor = torch.tensor(X.astype(np.float32))\n",
        "    predictions = []\n",
        "    \n",
        "    for _ in range(100):\n",
        "        weight = pyro.param('weight_loc').item()\n",
        "        bias = pyro.param('bias_loc').item()\n",
        "        pred = weight * X + bias\n",
        "        predictions.append(pred)\n",
        "    \n",
        "    return np.array(predictions)\n",
        "\n",
        "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "predictions = get_posterior_predictions(X_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
        "plt.plot(X_plot, predictions.mean(axis=0), 'r-', label='Mean prediction')\n",
        "plt.fill_between(X_plot.flatten(),\n",
        "                predictions.mean(axis=0) - 2*predictions.std(axis=0),\n",
        "                predictions.mean(axis=0) + 2*predictions.std(axis=0),\n",
        "                alpha=0.3, label='95% Credible interval')\n",
        "plt.legend()\n",
        "plt.title('Bayesian Linear Regression')\n",
        "plt.show()"
      ],
      "id": "b4aa4b10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Uncertainty Estimation\n",
        "\n",
        "### Dropout as Bayesian Approximation\n"
      ],
      "id": "3ab34723"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BayesianNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.1):\n",
        "        super(BayesianNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.relu(self.linear1(x)))\n",
        "        x = self.dropout(self.relu(self.linear2(x)))\n",
        "        return self.linear3(x)\n",
        "\n",
        "# Generate synthetic data\n",
        "X = torch.linspace(-4, 4, 100).reshape(-1, 1)\n",
        "y = X.pow(3) + torch.randn_like(X)\n",
        "\n",
        "# Train model\n",
        "model = BayesianNN(1, 64, 1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Monte Carlo Dropout predictions\n",
        "def mc_dropout_predict(model, x, num_samples=100):\n",
        "    model.train()  # Enable dropout\n",
        "    predictions = []\n",
        "    for _ in range(num_samples):\n",
        "        predictions.append(model(x).detach())\n",
        "    return torch.stack(predictions)\n",
        "\n",
        "# Get predictions\n",
        "X_test = torch.linspace(-6, 6, 100).reshape(-1, 1)\n",
        "predictions = mc_dropout_predict(model, X_test)\n",
        "\n",
        "mean_pred = predictions.mean(dim=0)\n",
        "std_pred = predictions.std(dim=0)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.5, label='Training data')\n",
        "plt.plot(X_test, mean_pred, 'r-', label='Mean prediction')\n",
        "plt.fill_between(X_test.flatten(),\n",
        "                mean_pred.flatten() - 2*std_pred.flatten(),\n",
        "                mean_pred.flatten() + 2*std_pred.flatten(),\n",
        "                alpha=0.3, label='95% Confidence interval')\n",
        "plt.legend()\n",
        "plt.title('Bayesian Neural Network with MC Dropout')\n",
        "plt.show()"
      ],
      "id": "102ac9ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Causal Inference\n",
        "\n",
        "### Demonstrating Simpson's Paradox\n"
      ],
      "id": "b162c0b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate synthetic data demonstrating Simpson's Paradox\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate confounding variable\n",
        "confounder = np.random.binomial(1, 0.5, n_samples)\n",
        "\n",
        "# Generate treatment with confounding\n",
        "treatment_prob = 0.3 + 0.4 * confounder\n",
        "treatment = np.random.binomial(1, treatment_prob)\n",
        "\n",
        "# Generate outcome with confounding\n",
        "outcome_base = 0.2 + 0.3 * confounder\n",
        "outcome_effect = 0.1 - 0.4 * confounder  # Treatment effect varies by confounder\n",
        "outcome_prob = outcome_base + outcome_effect * treatment\n",
        "outcome = np.random.binomial(1, outcome_prob)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Treatment': treatment,\n",
        "    'Outcome': outcome,\n",
        "    'Confounder': confounder\n",
        "})\n",
        "\n",
        "# Overall correlation\n",
        "overall_corr = np.corrcoef(treatment, outcome)[0, 1]\n",
        "\n",
        "# Stratified correlations\n",
        "corr_0 = np.corrcoef(treatment[confounder == 0], \n",
        "                     outcome[confounder == 0])[0, 1]\n",
        "corr_1 = np.corrcoef(treatment[confounder == 1], \n",
        "                     outcome[confounder == 1])[0, 1]\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Overall relationship\n",
        "sns.regplot(data=df, x='Treatment', y='Outcome', ax=axes[0])\n",
        "axes[0].set_title(f'Overall Correlation: {overall_corr:.3f}')\n",
        "\n",
        "# Stratified by confounder\n",
        "for conf_val, ax in zip([0, 1], axes[1:]):\n",
        "    mask = df['Confounder'] == conf_val\n",
        "    sns.regplot(data=df[mask], x='Treatment', y='Outcome', ax=ax)\n",
        "    corr = np.corrcoef(df[mask]['Treatment'], \n",
        "                      df[mask]['Outcome'])[0, 1]\n",
        "    ax.set_title(f'Confounder = {conf_val}\\nCorrelation: {corr:.3f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "1523b244",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Meta-Learning\n",
        "\n",
        "### Simple Meta-Learning Example\n"
      ],
      "id": "0919772f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MetaModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MetaModel, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.adaptation_network = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x, task_embedding=None):\n",
        "        features = self.feature_extractor(x)\n",
        "        if task_embedding is not None:\n",
        "            features = features * task_embedding\n",
        "        return self.adaptation_network(features)\n",
        "\n",
        "# Generate multiple tasks\n",
        "def generate_task():\n",
        "    # Each task is a different quadratic function\n",
        "    a = np.random.uniform(-1, 1)\n",
        "    b = np.random.uniform(-1, 1)\n",
        "    c = np.random.uniform(-1, 1)\n",
        "    \n",
        "    X = torch.linspace(-2, 2, 50).reshape(-1, 1)\n",
        "    y = a * X.pow(2) + b * X + c + torch.randn_like(X) * 0.1\n",
        "    return X, y\n",
        "\n",
        "# Generate tasks\n",
        "n_tasks = 5\n",
        "tasks = [generate_task() for _ in range(n_tasks)]\n",
        "\n",
        "# Train meta-model\n",
        "meta_model = MetaModel(1, 64, 1)\n",
        "meta_optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.01)\n",
        "\n",
        "# Meta-training loop\n",
        "n_epochs = 1000\n",
        "for epoch in range(n_epochs):\n",
        "    meta_loss = 0\n",
        "    for X, y in tasks:\n",
        "        # Generate random task embedding\n",
        "        task_embedding = torch.randn(64)\n",
        "        \n",
        "        # Forward pass\n",
        "        pred = meta_model(X, task_embedding)\n",
        "        loss = nn.MSELoss()(pred, y)\n",
        "        \n",
        "        # Backward pass\n",
        "        meta_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        meta_optimizer.step()\n",
        "        \n",
        "        meta_loss += loss.item()\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, (X, y) in enumerate(tasks):\n",
        "    plt.subplot(1, n_tasks, i+1)\n",
        "    \n",
        "    # Original data\n",
        "    plt.scatter(X.numpy(), y.numpy(), alpha=0.5, label='True')\n",
        "    \n",
        "    # Model prediction\n",
        "    task_embedding = torch.randn(64)\n",
        "    with torch.no_grad():\n",
        "        pred = meta_model(X, task_embedding)\n",
        "    plt.plot(X.numpy(), pred.numpy(), 'r-', label='Predicted')\n",
        "    \n",
        "    plt.title(f'Task {i+1}')\n",
        "    if i == 0:\n",
        "        plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d1cb0436",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Optimization Techniques\n",
        "\n",
        "### Natural Gradient Descent\n"
      ],
      "id": "88e38753"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NaturalGradientOptimizer:\n",
        "    def __init__(self, parameters, lr=0.01, damping=1e-4):\n",
        "        self.parameters = list(parameters)\n",
        "        self.lr = lr\n",
        "        self.damping = damping\n",
        "        \n",
        "    def compute_fisher_matrix(self, loss):\n",
        "        # Compute gradients\n",
        "        grads = torch.autograd.grad(loss, self.parameters, create_graph=True)\n",
        "        \n",
        "        # Compute Fisher Information Matrix\n",
        "        fisher = []\n",
        "        for g in grads:\n",
        "            fisher.append(g.view(-1) @ g.view(-1))\n",
        "        \n",
        "        return sum(fisher)\n",
        "    \n",
        "    def step(self, loss):\n",
        "        # Compute gradients\n",
        "        grads = torch.autograd.grad(loss, self.parameters)\n",
        "        \n",
        "        # Compute Fisher Information Matrix\n",
        "        fisher = self.compute_fisher_matrix(loss)\n",
        "        \n",
        "        # Update parameters using natural gradient\n",
        "        with torch.no_grad():\n",
        "            for param, grad in zip(self.parameters, grads):\n",
        "                natural_grad = grad / (fisher + self.damping)\n",
        "                param.sub_(self.lr * natural_grad)\n",
        "\n",
        "# Example usage\n",
        "X = torch.linspace(-2, 2, 100).reshape(-1, 1)\n",
        "y = X.pow(2) + torch.randn_like(X) * 0.1\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1)\n",
        ")\n",
        "\n",
        "optimizer = NaturalGradientOptimizer(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "    output = model(X)\n",
        "    loss = criterion(output, y)\n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    optimizer.step(loss)\n",
        "\n",
        "# Plot training curve\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss with Natural Gradient Descent')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "id": "6754bd6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Advanced ML\n",
        "\n",
        "1. **Uncertainty Handling**\n",
        "   - Always consider model uncertainty\n",
        "   - Use probabilistic approaches when possible\n",
        "   - Implement proper validation strategies\n",
        "\n",
        "2. **Causal Considerations**\n",
        "   - Identify potential confounders\n",
        "   - Use causal diagrams\n",
        "   - Consider intervention effects\n",
        "\n",
        "3. **Meta-Learning Applications**\n",
        "   - Start with simple meta-learning approaches\n",
        "   - Consider task similarity\n",
        "   - Implement proper validation\n",
        "\n",
        "4. **Optimization Choices**\n",
        "   - Consider problem structure\n",
        "   - Use appropriate optimization techniques\n",
        "   - Monitor convergence carefully\n",
        "\n",
        "## Common Pitfalls in Advanced ML\n",
        "\n",
        "1. **Uncertainty Estimation**\n",
        "   - Overconfident predictions\n",
        "   - Ignoring epistemic uncertainty\n",
        "   - Poor calibration\n",
        "\n",
        "2. **Causal Analysis**\n",
        "   - Confounding bias\n",
        "   - Selection bias\n",
        "   - Temporal dependencies\n",
        "\n",
        "3. **Meta-Learning**\n",
        "   - Task overfitting\n",
        "   - Poor generalization\n",
        "   - Insufficient task diversity\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Advanced ML concepts provide powerful tools for:\n",
        "\n",
        "1. Better uncertainty estimation\n",
        "2. Causal understanding\n",
        "3. Improved generalization\n",
        "4. Robust optimization\n",
        "\n",
        "In the next post, we'll explore specific applications of these concepts in real-world scenarios.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "1. Books:\n",
        "   - \"Probabilistic Machine Learning\" by Kevin Murphy\n",
        "   - \"Causal Inference in Statistics\" by Pearl et al.\n",
        "   - \"Meta-Learning\" by Chelsea Finn\n",
        "\n",
        "2. Papers:\n",
        "   - \"Weight Uncertainty in Neural Networks\" by Blundell et al.\n",
        "   - \"Model-Agnostic Meta-Learning\" by Finn et al.\n",
        "   - \"Natural Gradient Works Efficiently in Learning\" by Amari\n",
        "\n",
        "Remember: These advanced concepts are essential for handling real-world complexity and uncertainty in machine learning applications."
      ],
      "id": "339403de"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}