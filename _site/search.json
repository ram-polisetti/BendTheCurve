[
  {
    "objectID": "posts/statistical-learning-theory/index.html",
    "href": "posts/statistical-learning-theory/index.html",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Basic form:\n\\[\nP(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}\n\\]\nFor non-negative random variables.\nMoment version:\n\\[\nP(|X| \\geq a) \\leq \\frac{\\mathbb{E}[|X|^r]}{a^r}\n\\]\n\n\n\nBasic form:\n\\[\nP(|X - \\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2}\n\\]\nMoment version:\n\\[\nP(|X - \\mathbb{E}[X]| \\geq t) \\leq \\frac{\\text{Var}(X)}{t^2}\n\\]\n\n\n\nSum of bounded variables:\n\\[\nP(\\frac{1}{n}\\sum_{i=1}^n X_i - \\mathbb{E}[X] \\geq t) \\leq \\exp(-\\frac{2nt^2}{(b-a)^2})\n\\]\nMartingale version:\n\\[\nP(S_n - S_0 \\geq t) \\leq \\exp(-\\frac{2t^2}{\\sum_{i=1}^n (b_i-a_i)^2})\n\\]\n\n\n\n\n\n\nVariance-based bound:\n\\[\nP(\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\geq t) \\leq \\exp(-\\frac{nt^2}{2\\sigma^2 + 2Mt/3})\n\\]\nWhere: - \\(|X_i| \\leq M\\) - \\(\\text{Var}(X_i) \\leq \\sigma^2\\)\n\n\n\nBounded differences:\n\\[\nP(f(X_1,...,X_n) - \\mathbb{E}[f] \\geq t) \\leq \\exp(-\\frac{2t^2}{\\sum_{i=1}^n c_i^2})\n\\]\nWhere: - \\(|f(x) - f(x')| \\leq c_i\\) when \\(x,x'\\) differ in i-th coordinate\n\n\n\nConvex distance:\n\\[\nP(|Z - M(Z)| \\geq t) \\leq 4\\exp(-\\frac{t^2}{4\\sigma^2})\n\\]\nWhere: - \\(Z\\) is supremum of empirical process - \\(M(Z)\\) is median\n\n\n\n\n\n\nFundamental theorem:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\hat{R}_n(h) - R(h)| &gt; \\epsilon) \\leq 8\\mathcal{N}(\\epsilon/8)\\exp(-n\\epsilon^2/128)\n\\]\nWhere: - \\(\\mathcal{N}(\\epsilon)\\) is covering number - \\(R(h)\\) is true risk - \\(\\hat{R}_n(h)\\) is empirical risk\n\n\n\nBasic inequality:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)| &gt; 2\\epsilon) \\leq 2P(\\sup_{h \\in \\mathcal{H}}|\\hat{R}_n(h) - \\hat{R}'_n(h)| &gt; \\epsilon)\n\\]\nGhost sample technique:\n\\[\n\\mathbb{E}[\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)|] \\leq 2\\mathbb{E}[\\sup_{h \\in \\mathcal{H}}|\\frac{1}{n}\\sum_{i=1}^n \\sigma_i h(X_i)|]\n\\]\n\n\n\nDefinition:\n\\[\n\\mathfrak{R}_n(\\mathcal{H}) = \\mathbb{E}_{\\sigma,S}[\\sup_{h \\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^n \\sigma_i h(x_i)]\n\\]\nGeneralization bound:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)| \\leq 2\\mathfrak{R}_n(\\mathcal{H}) + \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}) \\geq 1-\\delta\n\\]\n\n\n\n\n\n\nDefinition:\n\\[\n\\mathfrak{R}_n(\\mathcal{H}, r) = \\mathbb{E}_{\\sigma}[\\sup_{h \\in \\mathcal{H}: P(h-h^*)^2 \\leq r}\\frac{1}{n}\\sum_{i=1}^n \\sigma_i h(X_i)]\n\\]\nFixed point:\n\\[\nr^* = \\inf\\{r &gt; 0: \\mathfrak{R}_n(\\mathcal{H}, r) \\leq r/4\\}\n\\]\n\n\n\nBound:\n\\[\nP(\\sup_{h \\in B(h^*,r)}|R(h) - \\hat{R}_n(h)| &gt; \\epsilon) \\leq \\mathcal{N}(r,\\epsilon/4)\\exp(-n\\epsilon^2/8)\n\\]\nWhere: - \\(B(h^*,r)\\) is ball around optimal hypothesis\n\n\n\nGeometric slicing:\n\\[\nP(\\exists h: |R(h) - \\hat{R}_n(h)| &gt; \\epsilon\\sqrt{R(h)}) \\leq \\sum_{j=0}^\\infty P(\\sup_{h: R(h) \\in [2^j\\alpha,2^{j+1}\\alpha]}|R(h) - \\hat{R}_n(h)| &gt; \\epsilon\\sqrt{2^j\\alpha})\n\\]\n\n\n\n\n\n\nAlgorithmic stability:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\nGeneralization bound:\n\\[\nP(|R(A_S) - \\hat{R}_n(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{n\\epsilon^2}{2\\beta^2})\n\\]\n\n\n\nSample compression:\n\\[\nm \\geq k\\log\\frac{em}{k} + \\log\\frac{1}{\\delta}\n\\]\nWhere: - \\(k\\) is size of compression set - \\(m\\) is sample size\n\n\n\nKL-divergence bound:\n\\[\nR(Q) \\leq \\hat{R}_n(Q) + \\sqrt{\\frac{KL(Q\\|P) + \\ln\\frac{2\\sqrt{n}}{\\delta}}{2n}}\n\\]\n\n\n\n\n\n\nSparse recovery:\n\\[\n\\|\\hat{\\beta} - \\beta^*\\|_2 \\leq \\sqrt{\\frac{s\\log p}{n}}\n\\]\nWhere: - \\(s\\) is sparsity - \\(p\\) is dimension\n\n\n\nMatrix Bernstein:\n\\[\nP(\\|\\sum_{i=1}^n X_i\\| \\geq t) \\leq 2d\\exp(-\\frac{t^2/2}{\\sigma^2 + Mt/3})\n\\]\nWhere: - \\(\\|X_i\\| \\leq M\\) - \\(\\|\\mathbb{E}[X_iX_i^T]\\| \\leq \\sigma^2\\)\n\n\n\nMaximal inequality:\n\\[\n\\mathbb{E}[\\sup_{f \\in \\mathcal{F}}|\\sum_{i=1}^n \\epsilon_i f(X_i)|] \\leq K\\sqrt{n}\\int_0^\\infty \\sqrt{\\log N(\\epsilon,\\mathcal{F},L_2)}d\\epsilon\n\\]\n\n\n\n\n\n\n\nFixed Confidence:\n\nError tolerance\nConfidence level\nComplexity measure\n\nFixed Width:\n\nPrecision requirement\nCoverage probability\nDimension impact\n\nSequential:\n\nStopping rules\nError control\nEfficiency\n\n\n\n\n\n\nProblem Structure:\n\nIndependence\nBoundedness\nMoment conditions\n\nSample Properties:\n\nSize\nDistribution\nDependence\n\nComputational:\n\nTightness\nSimplicity\nTractability\n\n\n\n\n\n\n\n\n\nProblem Formulation:\n\nIdentify assumptions\nChoose metrics\nSet objectives\n\nBound Selection:\n\nMatch assumptions\nConsider tightness\nBalance complexity\n\nImplementation:\n\nNumerical stability\nComputational efficiency\nError handling\n\n\n\n\n\n\nSample Size:\n\nConservative estimates\nSafety margins\nPower analysis\n\nValidation:\n\nCross-validation\nBootstrap\nPermutation tests\n\nMonitoring:\n\nConvergence checks\nStability measures\nError tracking\n\n\n\n\n\n\n\nTheory:\n\n“Concentration Inequalities” by Boucheron et al.\n“Statistical Learning Theory” by Vapnik\n“High-Dimensional Probability” by Vershynin\n\nMethods:\n\n“Empirical Processes in M-Estimation” by van der Vaart and Wellner\n“Random Matrices: High Dimensional Phenomena” by Davidson and Szarek\n“Theory of Classification” by Devroye et al.\n\nApplications:\n\n“Statistical Learning Theory and Applications” by Bousquet et al.\n“High-Dimensional Statistics” by Wainwright\n“Machine Learning Theory” by Mohri et al."
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#concentration-inequalities",
    "href": "posts/statistical-learning-theory/index.html#concentration-inequalities",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Basic form:\n\\[\nP(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}\n\\]\nFor non-negative random variables.\nMoment version:\n\\[\nP(|X| \\geq a) \\leq \\frac{\\mathbb{E}[|X|^r]}{a^r}\n\\]\n\n\n\nBasic form:\n\\[\nP(|X - \\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2}\n\\]\nMoment version:\n\\[\nP(|X - \\mathbb{E}[X]| \\geq t) \\leq \\frac{\\text{Var}(X)}{t^2}\n\\]\n\n\n\nSum of bounded variables:\n\\[\nP(\\frac{1}{n}\\sum_{i=1}^n X_i - \\mathbb{E}[X] \\geq t) \\leq \\exp(-\\frac{2nt^2}{(b-a)^2})\n\\]\nMartingale version:\n\\[\nP(S_n - S_0 \\geq t) \\leq \\exp(-\\frac{2t^2}{\\sum_{i=1}^n (b_i-a_i)^2})\n\\]"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#advanced-concentration-results",
    "href": "posts/statistical-learning-theory/index.html#advanced-concentration-results",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Variance-based bound:\n\\[\nP(\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\geq t) \\leq \\exp(-\\frac{nt^2}{2\\sigma^2 + 2Mt/3})\n\\]\nWhere: - \\(|X_i| \\leq M\\) - \\(\\text{Var}(X_i) \\leq \\sigma^2\\)\n\n\n\nBounded differences:\n\\[\nP(f(X_1,...,X_n) - \\mathbb{E}[f] \\geq t) \\leq \\exp(-\\frac{2t^2}{\\sum_{i=1}^n c_i^2})\n\\]\nWhere: - \\(|f(x) - f(x')| \\leq c_i\\) when \\(x,x'\\) differ in i-th coordinate\n\n\n\nConvex distance:\n\\[\nP(|Z - M(Z)| \\geq t) \\leq 4\\exp(-\\frac{t^2}{4\\sigma^2})\n\\]\nWhere: - \\(Z\\) is supremum of empirical process - \\(M(Z)\\) is median"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#statistical-learning-bounds",
    "href": "posts/statistical-learning-theory/index.html#statistical-learning-bounds",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Fundamental theorem:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\hat{R}_n(h) - R(h)| &gt; \\epsilon) \\leq 8\\mathcal{N}(\\epsilon/8)\\exp(-n\\epsilon^2/128)\n\\]\nWhere: - \\(\\mathcal{N}(\\epsilon)\\) is covering number - \\(R(h)\\) is true risk - \\(\\hat{R}_n(h)\\) is empirical risk\n\n\n\nBasic inequality:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)| &gt; 2\\epsilon) \\leq 2P(\\sup_{h \\in \\mathcal{H}}|\\hat{R}_n(h) - \\hat{R}'_n(h)| &gt; \\epsilon)\n\\]\nGhost sample technique:\n\\[\n\\mathbb{E}[\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)|] \\leq 2\\mathbb{E}[\\sup_{h \\in \\mathcal{H}}|\\frac{1}{n}\\sum_{i=1}^n \\sigma_i h(X_i)|]\n\\]\n\n\n\nDefinition:\n\\[\n\\mathfrak{R}_n(\\mathcal{H}) = \\mathbb{E}_{\\sigma,S}[\\sup_{h \\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^n \\sigma_i h(x_i)]\n\\]\nGeneralization bound:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)| \\leq 2\\mathfrak{R}_n(\\mathcal{H}) + \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}) \\geq 1-\\delta\n\\]"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#local-analysis",
    "href": "posts/statistical-learning-theory/index.html#local-analysis",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Definition:\n\\[\n\\mathfrak{R}_n(\\mathcal{H}, r) = \\mathbb{E}_{\\sigma}[\\sup_{h \\in \\mathcal{H}: P(h-h^*)^2 \\leq r}\\frac{1}{n}\\sum_{i=1}^n \\sigma_i h(X_i)]\n\\]\nFixed point:\n\\[\nr^* = \\inf\\{r &gt; 0: \\mathfrak{R}_n(\\mathcal{H}, r) \\leq r/4\\}\n\\]\n\n\n\nBound:\n\\[\nP(\\sup_{h \\in B(h^*,r)}|R(h) - \\hat{R}_n(h)| &gt; \\epsilon) \\leq \\mathcal{N}(r,\\epsilon/4)\\exp(-n\\epsilon^2/8)\n\\]\nWhere: - \\(B(h^*,r)\\) is ball around optimal hypothesis\n\n\n\nGeometric slicing:\n\\[\nP(\\exists h: |R(h) - \\hat{R}_n(h)| &gt; \\epsilon\\sqrt{R(h)}) \\leq \\sum_{j=0}^\\infty P(\\sup_{h: R(h) \\in [2^j\\alpha,2^{j+1}\\alpha]}|R(h) - \\hat{R}_n(h)| &gt; \\epsilon\\sqrt{2^j\\alpha})\n\\]"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#advanced-theory",
    "href": "posts/statistical-learning-theory/index.html#advanced-theory",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Algorithmic stability:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\nGeneralization bound:\n\\[\nP(|R(A_S) - \\hat{R}_n(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{n\\epsilon^2}{2\\beta^2})\n\\]\n\n\n\nSample compression:\n\\[\nm \\geq k\\log\\frac{em}{k} + \\log\\frac{1}{\\delta}\n\\]\nWhere: - \\(k\\) is size of compression set - \\(m\\) is sample size\n\n\n\nKL-divergence bound:\n\\[\nR(Q) \\leq \\hat{R}_n(Q) + \\sqrt{\\frac{KL(Q\\|P) + \\ln\\frac{2\\sqrt{n}}{\\delta}}{2n}}\n\\]"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#applications",
    "href": "posts/statistical-learning-theory/index.html#applications",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Sparse recovery:\n\\[\n\\|\\hat{\\beta} - \\beta^*\\|_2 \\leq \\sqrt{\\frac{s\\log p}{n}}\n\\]\nWhere: - \\(s\\) is sparsity - \\(p\\) is dimension\n\n\n\nMatrix Bernstein:\n\\[\nP(\\|\\sum_{i=1}^n X_i\\| \\geq t) \\leq 2d\\exp(-\\frac{t^2/2}{\\sigma^2 + Mt/3})\n\\]\nWhere: - \\(\\|X_i\\| \\leq M\\) - \\(\\|\\mathbb{E}[X_iX_i^T]\\| \\leq \\sigma^2\\)\n\n\n\nMaximal inequality:\n\\[\n\\mathbb{E}[\\sup_{f \\in \\mathcal{F}}|\\sum_{i=1}^n \\epsilon_i f(X_i)|] \\leq K\\sqrt{n}\\int_0^\\infty \\sqrt{\\log N(\\epsilon,\\mathcal{F},L_2)}d\\epsilon\n\\]"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#implementation-considerations",
    "href": "posts/statistical-learning-theory/index.html#implementation-considerations",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Fixed Confidence:\n\nError tolerance\nConfidence level\nComplexity measure\n\nFixed Width:\n\nPrecision requirement\nCoverage probability\nDimension impact\n\nSequential:\n\nStopping rules\nError control\nEfficiency\n\n\n\n\n\n\nProblem Structure:\n\nIndependence\nBoundedness\nMoment conditions\n\nSample Properties:\n\nSize\nDistribution\nDependence\n\nComputational:\n\nTightness\nSimplicity\nTractability"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#best-practices",
    "href": "posts/statistical-learning-theory/index.html#best-practices",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Problem Formulation:\n\nIdentify assumptions\nChoose metrics\nSet objectives\n\nBound Selection:\n\nMatch assumptions\nConsider tightness\nBalance complexity\n\nImplementation:\n\nNumerical stability\nComputational efficiency\nError handling\n\n\n\n\n\n\nSample Size:\n\nConservative estimates\nSafety margins\nPower analysis\n\nValidation:\n\nCross-validation\nBootstrap\nPermutation tests\n\nMonitoring:\n\nConvergence checks\nStability measures\nError tracking"
  },
  {
    "objectID": "posts/statistical-learning-theory/index.html#references",
    "href": "posts/statistical-learning-theory/index.html#references",
    "title": "Statistical Learning Theory and Concentration Inequalities",
    "section": "",
    "text": "Theory:\n\n“Concentration Inequalities” by Boucheron et al.\n“Statistical Learning Theory” by Vapnik\n“High-Dimensional Probability” by Vershynin\n\nMethods:\n\n“Empirical Processes in M-Estimation” by van der Vaart and Wellner\n“Random Matrices: High Dimensional Phenomena” by Davidson and Szarek\n“Theory of Classification” by Devroye et al.\n\nApplications:\n\n“Statistical Learning Theory and Applications” by Bousquet et al.\n“High-Dimensional Statistics” by Wainwright\n“Machine Learning Theory” by Mohri et al."
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html",
    "href": "posts/probabilistic-graphical-models/index.html",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Joint probability factorization:\n\\[\nP(X_1, ..., X_n) = \\prod_{i=1}^n P(X_i | \\text{Pa}(X_i))\n\\]\nWhere: - \\(X_i\\) are random variables - \\(\\text{Pa}(X_i)\\) are parents of \\(X_i\\) in the graph\n\n\n\nD-separation criterion: - Two nodes are d-separated if all paths between them are blocked - A path is blocked if: * Contains a collider not in evidence * Contains a non-collider in evidence\nFormal definition:\n\\[\nX \\perp\\!\\!\\!\\perp Y | Z \\iff P(X|Y,Z) = P(X|Z)\n\\]\n\n\n\n\n\n\nJoint distribution representation:\n\\[\nP(X = x) = \\frac{1}{Z}\\exp\\left(-\\sum_{c \\in \\mathcal{C}} \\psi_c(x_c)\\right)\n\\]\nWhere: - \\(\\mathcal{C}\\) is the set of cliques - \\(\\psi_c\\) are potential functions - \\(Z\\) is the partition function:\n\\[\nZ = \\sum_x \\exp\\left(-\\sum_{c \\in \\mathcal{C}} \\psi_c(x_c)\\right)\n\\]\n\n\n\nEquivalence between positive distributions and Gibbs distributions:\n\\[\nP(X = x) &gt; 0 \\iff P(X = x) = \\frac{1}{Z}\\prod_{c \\in \\mathcal{C}} \\phi_c(x_c)\n\\]\nWhere: - \\(\\phi_c\\) are non-negative factors\n\n\n\n\n\n\nComplexity for tree-structured graphs:\n\\[\nO(n \\cdot d^{w})\n\\]\nWhere: - \\(n\\) is number of variables - \\(d\\) is domain size - \\(w\\) is tree width\nAlgorithm steps: 1. Choose elimination ordering 2. For each variable: - Multiply relevant factors - Sum out variable\n\n\n\nMessage passing equations:\n\\[\n\\begin{aligned}\n\\mu_{i \\to j}(x_j) &= \\sum_{x_i} \\phi_i(x_i)\\phi_{ij}(x_i,x_j)\\prod_{k \\in N(i)\\backslash j} \\mu_{k \\to i}(x_i) \\\\\nb_i(x_i) &\\propto \\phi_i(x_i)\\prod_{j \\in N(i)} \\mu_{j \\to i}(x_i)\n\\end{aligned}\n\\]\nWhere: - \\(\\mu_{i \\to j}\\) is message from i to j - \\(b_i\\) is belief at node i - \\(N(i)\\) is neighbors of i\n\n\n\nClique tree construction: 1. Moralize graph 2. Triangulate 3. Find maximal cliques 4. Build junction tree\nRunning intersection property:\n\\[\nS_{ij} = C_i \\cap C_j \\subseteq C_k\n\\]\nFor any cliques \\(C_i\\), \\(C_j\\), and clique \\(C_k\\) on path between them.\n\n\n\n\n\n\nObjective function:\n\\[\n\\hat{\\theta}_{MLE} = \\arg\\max_\\theta \\sum_{i=1}^N \\log P(x^{(i)}|\\theta)\n\\]\nFor complete data in Bayesian networks:\n\\[\n\\hat{\\theta}_{ijk} = \\frac{N_{ijk}}{\\sum_k N_{ijk}}\n\\]\nWhere: - \\(N_{ijk}\\) is count of \\(X_i=k\\) with parent configuration j\n\n\n\nPosterior distribution:\n\\[\nP(\\theta|D) \\propto P(D|\\theta)P(\\theta)\n\\]\nWith Dirichlet prior:\n\\[\n\\theta_{ijk} \\sim \\text{Dir}(\\alpha_{ijk})\n\\]\nPosterior parameters:\n\\[\n\\alpha_{ijk}^{\\text{post}} = \\alpha_{ijk} + N_{ijk}\n\\]\n\n\n\nScore-based learning objective:\n\\[\nG^* = \\arg\\max_G \\text{score}(G:D)\n\\]\nCommon scores: 1. BIC score:\n\\[\n\\text{BIC}(G:D) = \\ell(D|\\hat{\\theta}, G) - \\frac{\\log N}{2}|G|\n\\]\n\nBayesian score:\n\n\\[\nP(G|D) \\propto P(D|G)P(G)\n\\]\n\n\n\n\n\n\nEvidence lower bound (ELBO):\n\\[\n\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x,z)] - \\mathbb{E}_q[\\log q(z)]\n\\]\nMean field approximation:\n\\[\nq(z) = \\prod_i q_i(z_i)\n\\]\nUpdate equations:\n\\[\n\\log q_j^*(z_j) = \\mathbb{E}_{q_{-j}}[\\log p(x,z)] + \\text{const}\n\\]\n\n\n\nMetropolis-Hastings acceptance ratio:\n\\[\n\\alpha = \\min\\left(1, \\frac{p(x')q(x|x')}{p(x)q(x'|x)}\\right)\n\\]\nGibbs sampling update:\n\\[\nx_i^{(t+1)} \\sim p(x_i|x_{-i}^{(t)})\n\\]\n\n\n\nLinear-chain CRF probability:\n\\[\nP(y|x) = \\frac{1}{Z(x)}\\exp\\left(\\sum_{t=1}^T\\sum_k \\lambda_k f_k(y_t,y_{t-1},x_t)\\right)\n\\]\nWhere: - \\(f_k\\) are feature functions - \\(\\lambda_k\\) are weights - \\(Z(x)\\) is normalization factor\n\n\n\n\n\n\nLog-space computations:\n\\[\n\\log\\sum_i \\exp(x_i) = \\max_i x_i + \\log\\sum_i \\exp(x_i - \\max_i x_i)\n\\]\n\n\n\nEfficient factor operations: - Sparse matrices for CPTs - Vectorized operations - Caching intermediate results\n\n\n\nParallel message passing: - Tree-structured graphs - Junction tree clusters - Mini-batch learning\n\n\n\n\n\n\n\nNetwork Structure:\n\nExpert knowledge\nCausal relationships\nData-driven learning\n\nInference Method:\n\nExact vs approximate\nGraph structure\nDomain size\n\nLearning Approach:\n\nData completeness\nPrior knowledge\nComputational resources\n\n\n\n\n\n\nVariable Ordering:\n\nMin-fill heuristic\nMin-degree ordering\nWeighted variants\n\nMessage Scheduling:\n\nResidual belief propagation\nPriority-based updates\nAsynchronous methods\n\nMemory Management:\n\nFactor caching\nMessage memoization\nSparse representations\n\n\n\n\n\n\n\n\nNetwork structure: - Diseases as root nodes - Symptoms as leaf nodes - Test results as intermediate nodes\nInference tasks: - Diagnostic reasoning - Predictive reasoning - Intercausal reasoning\n\n\n\nMRF applications: - Image segmentation - Stereo matching - Image restoration\nEnergy function:\n\\[\nE(x) = \\sum_i \\phi_i(x_i) + \\sum_{i,j} \\phi_{ij}(x_i,x_j)\n\\]\n\n\n\nLinear-chain CRFs: - Part-of-speech tagging - Named entity recognition - Sequence labeling\nFeature templates: - Word features - Context windows - Transition features\n\n\n\n\n\nTheory:\n\n“Probabilistic Graphical Models” by Koller and Friedman\n“Pattern Recognition and Machine Learning” by Bishop\n“Information Theory, Inference, and Learning Algorithms” by MacKay\n\nAlgorithms:\n\n“Understanding Belief Propagation and its Generalizations” by Yedidia et al.\n“An Introduction to MCMC for Machine Learning” by Andrieu et al.\n“Structured Prediction for Natural Language Processing” by Smith\n\nApplications:\n\n“Medical Applications of Artificial Intelligence” by Dua and Acharya\n“Computer Vision: A Modern Approach” by Forsyth and Ponce\n“Speech and Language Processing” by Jurafsky and Martin"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#bayesian-networks",
    "href": "posts/probabilistic-graphical-models/index.html#bayesian-networks",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Joint probability factorization:\n\\[\nP(X_1, ..., X_n) = \\prod_{i=1}^n P(X_i | \\text{Pa}(X_i))\n\\]\nWhere: - \\(X_i\\) are random variables - \\(\\text{Pa}(X_i)\\) are parents of \\(X_i\\) in the graph\n\n\n\nD-separation criterion: - Two nodes are d-separated if all paths between them are blocked - A path is blocked if: * Contains a collider not in evidence * Contains a non-collider in evidence\nFormal definition:\n\\[\nX \\perp\\!\\!\\!\\perp Y | Z \\iff P(X|Y,Z) = P(X|Z)\n\\]"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#markov-random-fields",
    "href": "posts/probabilistic-graphical-models/index.html#markov-random-fields",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Joint distribution representation:\n\\[\nP(X = x) = \\frac{1}{Z}\\exp\\left(-\\sum_{c \\in \\mathcal{C}} \\psi_c(x_c)\\right)\n\\]\nWhere: - \\(\\mathcal{C}\\) is the set of cliques - \\(\\psi_c\\) are potential functions - \\(Z\\) is the partition function:\n\\[\nZ = \\sum_x \\exp\\left(-\\sum_{c \\in \\mathcal{C}} \\psi_c(x_c)\\right)\n\\]\n\n\n\nEquivalence between positive distributions and Gibbs distributions:\n\\[\nP(X = x) &gt; 0 \\iff P(X = x) = \\frac{1}{Z}\\prod_{c \\in \\mathcal{C}} \\phi_c(x_c)\n\\]\nWhere: - \\(\\phi_c\\) are non-negative factors"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#inference-algorithms",
    "href": "posts/probabilistic-graphical-models/index.html#inference-algorithms",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Complexity for tree-structured graphs:\n\\[\nO(n \\cdot d^{w})\n\\]\nWhere: - \\(n\\) is number of variables - \\(d\\) is domain size - \\(w\\) is tree width\nAlgorithm steps: 1. Choose elimination ordering 2. For each variable: - Multiply relevant factors - Sum out variable\n\n\n\nMessage passing equations:\n\\[\n\\begin{aligned}\n\\mu_{i \\to j}(x_j) &= \\sum_{x_i} \\phi_i(x_i)\\phi_{ij}(x_i,x_j)\\prod_{k \\in N(i)\\backslash j} \\mu_{k \\to i}(x_i) \\\\\nb_i(x_i) &\\propto \\phi_i(x_i)\\prod_{j \\in N(i)} \\mu_{j \\to i}(x_i)\n\\end{aligned}\n\\]\nWhere: - \\(\\mu_{i \\to j}\\) is message from i to j - \\(b_i\\) is belief at node i - \\(N(i)\\) is neighbors of i\n\n\n\nClique tree construction: 1. Moralize graph 2. Triangulate 3. Find maximal cliques 4. Build junction tree\nRunning intersection property:\n\\[\nS_{ij} = C_i \\cap C_j \\subseteq C_k\n\\]\nFor any cliques \\(C_i\\), \\(C_j\\), and clique \\(C_k\\) on path between them."
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#learning-methods",
    "href": "posts/probabilistic-graphical-models/index.html#learning-methods",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Objective function:\n\\[\n\\hat{\\theta}_{MLE} = \\arg\\max_\\theta \\sum_{i=1}^N \\log P(x^{(i)}|\\theta)\n\\]\nFor complete data in Bayesian networks:\n\\[\n\\hat{\\theta}_{ijk} = \\frac{N_{ijk}}{\\sum_k N_{ijk}}\n\\]\nWhere: - \\(N_{ijk}\\) is count of \\(X_i=k\\) with parent configuration j\n\n\n\nPosterior distribution:\n\\[\nP(\\theta|D) \\propto P(D|\\theta)P(\\theta)\n\\]\nWith Dirichlet prior:\n\\[\n\\theta_{ijk} \\sim \\text{Dir}(\\alpha_{ijk})\n\\]\nPosterior parameters:\n\\[\n\\alpha_{ijk}^{\\text{post}} = \\alpha_{ijk} + N_{ijk}\n\\]\n\n\n\nScore-based learning objective:\n\\[\nG^* = \\arg\\max_G \\text{score}(G:D)\n\\]\nCommon scores: 1. BIC score:\n\\[\n\\text{BIC}(G:D) = \\ell(D|\\hat{\\theta}, G) - \\frac{\\log N}{2}|G|\n\\]\n\nBayesian score:\n\n\\[\nP(G|D) \\propto P(D|G)P(G)\n\\]"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#advanced-topics",
    "href": "posts/probabilistic-graphical-models/index.html#advanced-topics",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Evidence lower bound (ELBO):\n\\[\n\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x,z)] - \\mathbb{E}_q[\\log q(z)]\n\\]\nMean field approximation:\n\\[\nq(z) = \\prod_i q_i(z_i)\n\\]\nUpdate equations:\n\\[\n\\log q_j^*(z_j) = \\mathbb{E}_{q_{-j}}[\\log p(x,z)] + \\text{const}\n\\]\n\n\n\nMetropolis-Hastings acceptance ratio:\n\\[\n\\alpha = \\min\\left(1, \\frac{p(x')q(x|x')}{p(x)q(x'|x)}\\right)\n\\]\nGibbs sampling update:\n\\[\nx_i^{(t+1)} \\sim p(x_i|x_{-i}^{(t)})\n\\]\n\n\n\nLinear-chain CRF probability:\n\\[\nP(y|x) = \\frac{1}{Z(x)}\\exp\\left(\\sum_{t=1}^T\\sum_k \\lambda_k f_k(y_t,y_{t-1},x_t)\\right)\n\\]\nWhere: - \\(f_k\\) are feature functions - \\(\\lambda_k\\) are weights - \\(Z(x)\\) is normalization factor"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#implementation-considerations",
    "href": "posts/probabilistic-graphical-models/index.html#implementation-considerations",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Log-space computations:\n\\[\n\\log\\sum_i \\exp(x_i) = \\max_i x_i + \\log\\sum_i \\exp(x_i - \\max_i x_i)\n\\]\n\n\n\nEfficient factor operations: - Sparse matrices for CPTs - Vectorized operations - Caching intermediate results\n\n\n\nParallel message passing: - Tree-structured graphs - Junction tree clusters - Mini-batch learning"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#best-practices",
    "href": "posts/probabilistic-graphical-models/index.html#best-practices",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Network Structure:\n\nExpert knowledge\nCausal relationships\nData-driven learning\n\nInference Method:\n\nExact vs approximate\nGraph structure\nDomain size\n\nLearning Approach:\n\nData completeness\nPrior knowledge\nComputational resources\n\n\n\n\n\n\nVariable Ordering:\n\nMin-fill heuristic\nMin-degree ordering\nWeighted variants\n\nMessage Scheduling:\n\nResidual belief propagation\nPriority-based updates\nAsynchronous methods\n\nMemory Management:\n\nFactor caching\nMessage memoization\nSparse representations"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#applications",
    "href": "posts/probabilistic-graphical-models/index.html#applications",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Network structure: - Diseases as root nodes - Symptoms as leaf nodes - Test results as intermediate nodes\nInference tasks: - Diagnostic reasoning - Predictive reasoning - Intercausal reasoning\n\n\n\nMRF applications: - Image segmentation - Stereo matching - Image restoration\nEnergy function:\n\\[\nE(x) = \\sum_i \\phi_i(x_i) + \\sum_{i,j} \\phi_{ij}(x_i,x_j)\n\\]\n\n\n\nLinear-chain CRFs: - Part-of-speech tagging - Named entity recognition - Sequence labeling\nFeature templates: - Word features - Context windows - Transition features"
  },
  {
    "objectID": "posts/probabilistic-graphical-models/index.html#references",
    "href": "posts/probabilistic-graphical-models/index.html#references",
    "title": "Probabilistic Graphical Models: Mathematical Foundations",
    "section": "",
    "text": "Theory:\n\n“Probabilistic Graphical Models” by Koller and Friedman\n“Pattern Recognition and Machine Learning” by Bishop\n“Information Theory, Inference, and Learning Algorithms” by MacKay\n\nAlgorithms:\n\n“Understanding Belief Propagation and its Generalizations” by Yedidia et al.\n“An Introduction to MCMC for Machine Learning” by Andrieu et al.\n“Structured Prediction for Natural Language Processing” by Smith\n\nApplications:\n\n“Medical Applications of Artificial Intelligence” by Dua and Acharya\n“Computer Vision: A Modern Approach” by Forsyth and Ponce\n“Speech and Language Processing” by Jurafsky and Martin"
  },
  {
    "objectID": "posts/optimization-theory/index.html",
    "href": "posts/optimization-theory/index.html",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Convex set definition:\n\\[\n\\theta x + (1-\\theta)y \\in C, \\forall x,y \\in C, \\theta \\in [0,1]\n\\]\nConvex function:\n\\[\nf(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y)\n\\]\n\n\n\nFirst-order characterization:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x)\n\\]\nSecond-order characterization:\n\\[\n\\nabla^2 f(x) \\succeq 0\n\\]\n\n\n\nDefinition:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu}{2}\\|y-x\\|^2\n\\]\nQuadratic growth:\n\\[\nf(x) - f(x^*) \\geq \\frac{\\mu}{2}\\|x-x^*\\|^2\n\\]\n\n\n\n\n\n\nUnconstrained:\n\\[\n\\nabla f(x^*) = 0\n\\]\nConstrained (KKT):\n\\[\n\\begin{aligned}\n\\nabla_x \\mathcal{L}(x^*,\\lambda^*) &= 0 \\\\\ng_i(x^*) &\\leq 0 \\\\\n\\lambda_i^* g_i(x^*) &= 0 \\\\\n\\lambda_i^* &\\geq 0\n\\end{aligned}\n\\]\n\n\n\nUnconstrained:\n\\[\n\\nabla^2 f(x^*) \\succeq 0\n\\]\nConstrained:\n\\[\ny^T\\nabla^2_{xx}\\mathcal{L}(x^*,\\lambda^*)y \\geq 0\n\\]\n\n\n\nMinimax:\n\\[\n\\mathcal{L}(x^*,\\lambda) \\leq \\mathcal{L}(x^*,\\lambda^*) \\leq \\mathcal{L}(x,\\lambda^*)\n\\]\nDuality gap:\n\\[\nf(x^*) - g(\\lambda^*) = 0\n\\]\n\n\n\n\n\n\nUpdate rule:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n\\]\nConvergence rate (convex):\n\\[\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n\\]\n\n\n\nNesterov’s acceleration:\n\\[\n\\begin{aligned}\ny_k &= x_k + \\beta_k(x_k - x_{k-1}) \\\\\nx_{k+1} &= y_k - \\eta_k\\nabla f(y_k)\n\\end{aligned}\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{2L\\|x_0-x^*\\|^2}{(k+1)^2}\n\\]\n\n\n\nSGD update:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f_{i_k}(x_k)\n\\]\nConvergence rate:\n\\[\n\\mathbb{E}[f(x_k) - f(x^*)] \\leq \\frac{L\\|x_0-x^*\\|^2}{2k} + \\frac{L\\sigma^2}{2}\\sum_{t=1}^k \\eta_t^2\n\\]\n\n\n\n\n\n\nFirst-order condition:\n\\[\n\\|\\nabla f(x^*)\\| \\leq \\epsilon\n\\]\nSecond-order condition:\n\\[\n\\lambda_{\\min}(\\nabla^2 f(x^*)) \\geq -\\sqrt{\\epsilon}\n\\]\n\n\n\nPerturbed gradient descent:\n\\[\nx_{k+1} = x_k - \\eta\\nabla f(x_k) + \\xi_k\n\\]\nWhere: - \\(\\xi_k \\sim \\mathcal{N}(0,\\sigma^2I)\\)\n\n\n\nBranch and bound:\n\\[\n\\text{LB}(R) \\leq \\min_{x \\in R} f(x) \\leq \\text{UB}(R)\n\\]\nSimulated annealing:\n\\[\nP(\\text{accept}) = \\exp(-\\frac{\\Delta E}{T_k})\n\\]\n\n\n\n\n\n\nAdaGrad:\n\\[\nx_{t+1,i} = x_{t,i} - \\frac{\\eta}{\\sqrt{\\sum_{s=1}^t g_{s,i}^2}}g_{t,i}\n\\]\nAdam:\n\\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\\nx_{t+1} &= x_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t\n\\end{aligned}\n\\]\n\n\n\nFisher information:\n\\[\nF(x) = \\mathbb{E}_{p(y|x)}[\\nabla \\log p(y|x)\\nabla \\log p(y|x)^T]\n\\]\nUpdate rule:\n\\[\nx_{k+1} = x_k - \\eta F(x_k)^{-1}\\nabla f(x_k)\n\\]\n\n\n\nNewton’s method:\n\\[\nx_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\n\\]\nBFGS update:\n\\[\nB_{k+1} = B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}\n\\]\n\n\n\n\n\n\nUpdate rule:\n\\[\nx_{k+1} = \\Pi_C(x_k - \\eta_k\\nabla f(x_k))\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n\\]\n\n\n\nProximal operator:\n\\[\n\\text{prox}_{\\eta g}(x) = \\arg\\min_y \\{g(y) + \\frac{1}{2\\eta}\\|y-x\\|^2\\}\n\\]\nISTA update:\n\\[\nx_{k+1} = \\text{prox}_{\\eta g}(x_k - \\eta\\nabla f(x_k))\n\\]\n\n\n\nFunction:\n\\[\n\\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\lambda^Tg(x) + \\frac{\\rho}{2}\\|g(x)\\|^2\n\\]\nUpdate rules:\n\\[\n\\begin{aligned}\nx_{k+1} &= \\arg\\min_x \\mathcal{L}_\\rho(x,\\lambda_k) \\\\\n\\lambda_{k+1} &= \\lambda_k + \\rho g(x_{k+1})\n\\end{aligned}\n\\]\n\n\n\n\n\n\nADMM algorithm:\n\\[\n\\begin{aligned}\nx_{k+1} &= \\arg\\min_x \\mathcal{L}_\\rho(x,z_k,y_k) \\\\\nz_{k+1} &= \\arg\\min_z \\mathcal{L}_\\rho(x_{k+1},z,y_k) \\\\\ny_{k+1} &= y_k + \\rho(Ax_{k+1} + Bz_{k+1} - c)\n\\end{aligned}\n\\]\n\n\n\nRegret bound:\n\\[\nR_T \\leq O(\\sqrt{T})\n\\]\nFollow-the-regularized-leader:\n\\[\nx_{t+1} = \\arg\\min_x \\{\\eta\\sum_{s=1}^t \\ell_s(x) + R(x)\\}\n\\]\n\n\n\nGradient estimation:\n\\[\n\\hat{\\nabla} f(x) = \\frac{d}{r}f(x+r\\xi)\\xi\n\\]\nWhere: - \\(\\xi \\sim \\text{Unif}(\\mathbb{S}^{d-1})\\)\n\n\n\n\n\n\n\nProblem Structure:\n\nConvexity\nSmoothness\nConstraints\n\nData Properties:\n\nSize\nDimensionality\nSparsity\n\nComputational Resources:\n\nMemory\nProcessing power\nTime constraints\n\n\n\n\n\n\nInitialization:\n\nParameter scaling\nRandom seeding\nWarm start\n\nMonitoring:\n\nConvergence\nStability\nResource usage\n\nTuning:\n\nLearning rates\nMomentum\nRegularization\n\n\n\n\n\n\n\nTheory:\n\n“Convex Optimization” by Boyd and Vandenberghe\n“Introductory Lectures on Convex Optimization” by Nesterov\n“Optimization Methods for Large-Scale Machine Learning” by Bottou et al.\n\nMethods:\n\n“Numerical Optimization” by Nocedal and Wright\n“First-Order Methods in Optimization” by Beck\n“Proximal Algorithms” by Parikh and Boyd\n\nApplications:\n\n“Deep Learning” by Goodfellow et al.\n“Optimization for Machine Learning” by Sra et al.\n“Large Scale Optimization in Machine Learning” by Shalev-Shwartz and Zhang"
  },
  {
    "objectID": "posts/optimization-theory/index.html#convex-analysis",
    "href": "posts/optimization-theory/index.html#convex-analysis",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Convex set definition:\n\\[\n\\theta x + (1-\\theta)y \\in C, \\forall x,y \\in C, \\theta \\in [0,1]\n\\]\nConvex function:\n\\[\nf(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y)\n\\]\n\n\n\nFirst-order characterization:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x)\n\\]\nSecond-order characterization:\n\\[\n\\nabla^2 f(x) \\succeq 0\n\\]\n\n\n\nDefinition:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu}{2}\\|y-x\\|^2\n\\]\nQuadratic growth:\n\\[\nf(x) - f(x^*) \\geq \\frac{\\mu}{2}\\|x-x^*\\|^2\n\\]"
  },
  {
    "objectID": "posts/optimization-theory/index.html#optimality-conditions",
    "href": "posts/optimization-theory/index.html#optimality-conditions",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Unconstrained:\n\\[\n\\nabla f(x^*) = 0\n\\]\nConstrained (KKT):\n\\[\n\\begin{aligned}\n\\nabla_x \\mathcal{L}(x^*,\\lambda^*) &= 0 \\\\\ng_i(x^*) &\\leq 0 \\\\\n\\lambda_i^* g_i(x^*) &= 0 \\\\\n\\lambda_i^* &\\geq 0\n\\end{aligned}\n\\]\n\n\n\nUnconstrained:\n\\[\n\\nabla^2 f(x^*) \\succeq 0\n\\]\nConstrained:\n\\[\ny^T\\nabla^2_{xx}\\mathcal{L}(x^*,\\lambda^*)y \\geq 0\n\\]\n\n\n\nMinimax:\n\\[\n\\mathcal{L}(x^*,\\lambda) \\leq \\mathcal{L}(x^*,\\lambda^*) \\leq \\mathcal{L}(x,\\lambda^*)\n\\]\nDuality gap:\n\\[\nf(x^*) - g(\\lambda^*) = 0\n\\]"
  },
  {
    "objectID": "posts/optimization-theory/index.html#gradient-methods",
    "href": "posts/optimization-theory/index.html#gradient-methods",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Update rule:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n\\]\nConvergence rate (convex):\n\\[\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n\\]\n\n\n\nNesterov’s acceleration:\n\\[\n\\begin{aligned}\ny_k &= x_k + \\beta_k(x_k - x_{k-1}) \\\\\nx_{k+1} &= y_k - \\eta_k\\nabla f(y_k)\n\\end{aligned}\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{2L\\|x_0-x^*\\|^2}{(k+1)^2}\n\\]\n\n\n\nSGD update:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f_{i_k}(x_k)\n\\]\nConvergence rate:\n\\[\n\\mathbb{E}[f(x_k) - f(x^*)] \\leq \\frac{L\\|x_0-x^*\\|^2}{2k} + \\frac{L\\sigma^2}{2}\\sum_{t=1}^k \\eta_t^2\n\\]"
  },
  {
    "objectID": "posts/optimization-theory/index.html#non-convex-optimization",
    "href": "posts/optimization-theory/index.html#non-convex-optimization",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "First-order condition:\n\\[\n\\|\\nabla f(x^*)\\| \\leq \\epsilon\n\\]\nSecond-order condition:\n\\[\n\\lambda_{\\min}(\\nabla^2 f(x^*)) \\geq -\\sqrt{\\epsilon}\n\\]\n\n\n\nPerturbed gradient descent:\n\\[\nx_{k+1} = x_k - \\eta\\nabla f(x_k) + \\xi_k\n\\]\nWhere: - \\(\\xi_k \\sim \\mathcal{N}(0,\\sigma^2I)\\)\n\n\n\nBranch and bound:\n\\[\n\\text{LB}(R) \\leq \\min_{x \\in R} f(x) \\leq \\text{UB}(R)\n\\]\nSimulated annealing:\n\\[\nP(\\text{accept}) = \\exp(-\\frac{\\Delta E}{T_k})\n\\]"
  },
  {
    "objectID": "posts/optimization-theory/index.html#modern-optimization-methods",
    "href": "posts/optimization-theory/index.html#modern-optimization-methods",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "AdaGrad:\n\\[\nx_{t+1,i} = x_{t,i} - \\frac{\\eta}{\\sqrt{\\sum_{s=1}^t g_{s,i}^2}}g_{t,i}\n\\]\nAdam:\n\\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\\nx_{t+1} &= x_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t\n\\end{aligned}\n\\]\n\n\n\nFisher information:\n\\[\nF(x) = \\mathbb{E}_{p(y|x)}[\\nabla \\log p(y|x)\\nabla \\log p(y|x)^T]\n\\]\nUpdate rule:\n\\[\nx_{k+1} = x_k - \\eta F(x_k)^{-1}\\nabla f(x_k)\n\\]\n\n\n\nNewton’s method:\n\\[\nx_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)\n\\]\nBFGS update:\n\\[\nB_{k+1} = B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}\n\\]"
  },
  {
    "objectID": "posts/optimization-theory/index.html#constrained-optimization",
    "href": "posts/optimization-theory/index.html#constrained-optimization",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Update rule:\n\\[\nx_{k+1} = \\Pi_C(x_k - \\eta_k\\nabla f(x_k))\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n\\]\n\n\n\nProximal operator:\n\\[\n\\text{prox}_{\\eta g}(x) = \\arg\\min_y \\{g(y) + \\frac{1}{2\\eta}\\|y-x\\|^2\\}\n\\]\nISTA update:\n\\[\nx_{k+1} = \\text{prox}_{\\eta g}(x_k - \\eta\\nabla f(x_k))\n\\]\n\n\n\nFunction:\n\\[\n\\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\lambda^Tg(x) + \\frac{\\rho}{2}\\|g(x)\\|^2\n\\]\nUpdate rules:\n\\[\n\\begin{aligned}\nx_{k+1} &= \\arg\\min_x \\mathcal{L}_\\rho(x,\\lambda_k) \\\\\n\\lambda_{k+1} &= \\lambda_k + \\rho g(x_{k+1})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/optimization-theory/index.html#advanced-topics",
    "href": "posts/optimization-theory/index.html#advanced-topics",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "ADMM algorithm:\n\\[\n\\begin{aligned}\nx_{k+1} &= \\arg\\min_x \\mathcal{L}_\\rho(x,z_k,y_k) \\\\\nz_{k+1} &= \\arg\\min_z \\mathcal{L}_\\rho(x_{k+1},z,y_k) \\\\\ny_{k+1} &= y_k + \\rho(Ax_{k+1} + Bz_{k+1} - c)\n\\end{aligned}\n\\]\n\n\n\nRegret bound:\n\\[\nR_T \\leq O(\\sqrt{T})\n\\]\nFollow-the-regularized-leader:\n\\[\nx_{t+1} = \\arg\\min_x \\{\\eta\\sum_{s=1}^t \\ell_s(x) + R(x)\\}\n\\]\n\n\n\nGradient estimation:\n\\[\n\\hat{\\nabla} f(x) = \\frac{d}{r}f(x+r\\xi)\\xi\n\\]\nWhere: - \\(\\xi \\sim \\text{Unif}(\\mathbb{S}^{d-1})\\)"
  },
  {
    "objectID": "posts/optimization-theory/index.html#best-practices",
    "href": "posts/optimization-theory/index.html#best-practices",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Problem Structure:\n\nConvexity\nSmoothness\nConstraints\n\nData Properties:\n\nSize\nDimensionality\nSparsity\n\nComputational Resources:\n\nMemory\nProcessing power\nTime constraints\n\n\n\n\n\n\nInitialization:\n\nParameter scaling\nRandom seeding\nWarm start\n\nMonitoring:\n\nConvergence\nStability\nResource usage\n\nTuning:\n\nLearning rates\nMomentum\nRegularization"
  },
  {
    "objectID": "posts/optimization-theory/index.html#references",
    "href": "posts/optimization-theory/index.html#references",
    "title": "Optimization Theory in Machine Learning",
    "section": "",
    "text": "Theory:\n\n“Convex Optimization” by Boyd and Vandenberghe\n“Introductory Lectures on Convex Optimization” by Nesterov\n“Optimization Methods for Large-Scale Machine Learning” by Bottou et al.\n\nMethods:\n\n“Numerical Optimization” by Nocedal and Wright\n“First-Order Methods in Optimization” by Beck\n“Proximal Algorithms” by Parikh and Boyd\n\nApplications:\n\n“Deep Learning” by Goodfellow et al.\n“Optimization for Machine Learning” by Sra et al.\n“Large Scale Optimization in Machine Learning” by Shalev-Shwartz and Zhang"
  },
  {
    "objectID": "posts/online-learning/index.html",
    "href": "posts/online-learning/index.html",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Learning process: 1. Receive instance \\(x_t\\) 2. Predict \\(\\hat{y}_t\\) 3. Observe true outcome \\(y_t\\) 4. Suffer loss \\(\\ell({\\hat{y}_t, y_t})\\) 5. Update model\nRegret definition:\n\\[\nR_T = \\sum_{t=1}^T \\ell(h_t(x_t), y_t) - \\min_{h \\in \\mathcal{H}}\\sum_{t=1}^T \\ell(h(x_t), y_t)\n\\]\n\n\n\nExternal regret:\n\\[\nR_T^{\\text{ext}} = \\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a_t)\\right] - \\min_{a \\in \\mathcal{A}}\\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a)\\right]\n\\]\nInternal/swap regret:\n\\[\nR_T^{\\text{int}} = \\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a_t)\\right] - \\min_{\\phi: \\mathcal{A} \\to \\mathcal{A}}\\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(\\phi(a_t))\\right]\n\\]\n\n\n\nAverage regret:\n\\[\n\\bar{R}_T = \\frac{R_T}{T}\n\\]\nCompetitive ratio:\n\\[\nCR_T = \\frac{\\sum_{t=1}^T \\ell_t(a_t)}{\\min_{a \\in \\mathcal{A}}\\sum_{t=1}^T \\ell_t(a)}\n\\]\n\n\n\n\n\n\nUpdate rule:\n\\[\nw_{t+1} = \\Pi_{\\mathcal{W}}(w_t - \\eta_t \\nabla \\ell_t(w_t))\n\\]\nRegret bound:\n\\[\nR_T \\leq \\frac{D^2}{2\\eta} + \\frac{\\eta G^2T}{2}\n\\]\nWhere: - \\(D\\) is diameter of feasible set - \\(G\\) is gradient bound - \\(\\eta\\) is learning rate\n\n\n\nFTRL update:\n\\[\nw_{t+1} = \\arg\\min_{w \\in \\mathcal{W}}\\{\\eta\\sum_{s=1}^t \\ell_s(w) + R(w)\\}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\frac{R(w^*)}{\\eta} + \\frac{\\eta G^2T}{2}\n\\]\n\n\n\nUpdate rule:\n\\[\n\\nabla \\psi(w_{t+1}) = \\nabla \\psi(w_t) - \\eta_t \\nabla \\ell_t(w_t)\n\\]\nRegret bound:\n\\[\nR_T \\leq \\frac{D_\\psi(w^*\\|w_1)}{\\eta} + \\frac{\\eta G^2T}{2}\n\\]\n\n\n\n\n\n\nUCB index:\n\\[\nUCB_i(t) = \\hat{\\mu}_i(t) + \\sqrt{\\frac{2\\ln t}{N_i(t)}}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sum_{i:\\Delta_i&gt;0}\\left(\\frac{8\\ln T}{\\Delta_i} + (1+\\pi^2/3)\\Delta_i\\right)\n\\]\n\n\n\nPosterior sampling:\n\\[\n\\theta_i(t) \\sim Beta(\\alpha_i(t), \\beta_i(t))\n\\]\nRegret bound:\n\\[\nR_T \\leq O(\\sqrt{KT\\ln T})\n\\]\n\n\n\nProbability update:\n\\[\np_i(t) = \\frac{(1-\\gamma)\\exp(\\eta G_i(t))}{\\sum_{j=1}^K \\exp(\\eta G_j(t))} + \\frac{\\gamma}{K}\n\\]\nRegret bound:\n\\[\nR_T \\leq 2\\sqrt{KT\\ln K}\n\\]\n\n\n\n\n\n\nWeight update:\n\\[\nw_i(t+1) = w_i(t)(1-\\eta)^{\\ell_i(t)}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sqrt{T\\ln N}\n\\]\n\n\n\nProbability update:\n\\[\np_i(t+1) = \\frac{\\exp(-\\eta L_i(t))}{\\sum_{j=1}^N \\exp(-\\eta L_j(t))}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sqrt{2T\\ln N}\n\\]\n\n\n\nAdaptive learning rate:\n\\[\n\\eta_t = \\frac{\\ln N}{V_t}\n\\]\nWhere: - \\(V_t\\) is cumulative variance - \\(N\\) is number of experts\n\n\n\n\n\n\nAdaGrad update:\n\\[\nw_{t+1,i} = w_{t,i} - \\frac{\\eta}{\\sqrt{\\sum_{s=1}^t g_{s,i}^2}}g_{t,i}\n\\]\nRegret bound:\n\\[\nR_T \\leq O(\\sqrt{T}\\|w^*\\|_2\\sqrt{\\sum_{i=1}^d\\sum_{t=1}^T g_{t,i}^2})\n\\]\n\n\n\nONS update:\n\\[\nw_{t+1} = w_t - \\eta A_t^{-1}\\nabla \\ell_t(w_t)\n\\]\nWhere: - \\(A_t = \\sum_{s=1}^t \\nabla \\ell_s(w_s)\\nabla \\ell_s(w_s)^T\\)\n\n\n\nMetaGrad update:\n\\[\nw_{t+1} = w_t - \\eta_t H_t^{-1/2}\\nabla \\ell_t(w_t)\n\\]\nWhere: - \\(H_t\\) is preconditioner matrix\n\n\n\n\n\n\nObjective:\n\\[\n\\max_{w \\in \\Delta_n} \\sum_{t=1}^T \\log(w^T r_t)\n\\]\nUniversal portfolio:\n\\[\nw_{t+1} = \\int_{\\Delta_n} w P_t(w)dw\n\\]\n\n\n\nFlow update:\n\\[\nf_{t+1}(e) = f_t(e)\\exp(-\\eta \\ell_t(e))\n\\]\nPath selection:\n\\[\nP(p) = \\frac{\\prod_{e \\in p}f_t(e)}{\\sum_{p' \\in \\mathcal{P}}\\prod_{e \\in p'}f_t(e)}\n\\]\n\n\n\nPerceptron update:\n\\[\nw_{t+1} = w_t + y_tx_t\\mathbb{1}[y_tw_t^Tx_t \\leq 0]\n\\]\nMistake bound:\n\\[\nM \\leq \\left(\\frac{R}{\\gamma}\\right)^2\n\\]\n\n\n\n\n\n\nMinimax regret:\n\\[\n\\min_{\\text{ALG}}\\max_{\\text{ADV}} R_T = \\Omega(\\sqrt{T})\n\\]\nExpert setting:\n\\[\nR_T = \\Omega(\\sqrt{T\\ln N})\n\\]\n\n\n\nRedundancy bound:\n\\[\nR_T \\leq \\frac{KL(P\\|Q) + \\ln(1/\\delta)}{\\eta} + \\frac{\\eta T}{8}\n\\]\n\n\n\nNash equilibrium:\n\\[\n\\max_P \\min_Q \\mathbb{E}_{p \\sim P, q \\sim Q}[M(p,q)] = \\min_Q \\max_P \\mathbb{E}_{p \\sim P, q \\sim Q}[M(p,q)]\n\\]\n\n\n\n\n\n\n\nProblem Structure:\n\nConvexity\nSmoothness\nSparsity\n\nComputational Constraints:\n\nMemory limits\nUpdate time\nParallelization\n\nPerformance Requirements:\n\nRegret bounds\nAdaptation speed\nRobustness\n\n\n\n\n\n\nLearning Rates:\n\nFixed vs adaptive\nSchedule design\nInitialization\n\nExploration:\n\nExploration rate\nDecay schedule\nAdaptive schemes\n\nRegularization:\n\nStrength\nType selection\nAdaptation\n\n\n\n\n\n\n\n\n\nRobustness:\n\nAdversarial scenarios\nNoise handling\nDistribution shifts\n\nEfficiency:\n\nMemory usage\nUpdate complexity\nParallelization\n\nAdaptivity:\n\nParameter tuning\nDistribution changes\nModel selection\n\n\n\n\n\n\nData Handling:\n\nStreaming processing\nFeature extraction\nPreprocessing\n\nMonitoring:\n\nRegret tracking\nPerformance metrics\nResource usage\n\nDeployment:\n\nSystem integration\nError handling\nScaling strategy\n\n\n\n\n\n\n\nTheory:\n\n“Introduction to Online Convex Optimization” by Hazan\n“Bandit Algorithms” by Lattimore and Szepesvári\n“Prediction, Learning, and Games” by Cesa-Bianchi and Lugosi\n\nMethods:\n\n“Online Learning and Online Convex Optimization” by Shalev-Shwartz\n“A Modern Introduction to Online Learning” by Orabona\n“Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems” by Bubeck and Cesa-Bianchi\n\nApplications:\n\n“Online Portfolio Selection” by Li and Hoi\n“Online Learning in Routing Games” by Roughgarden\n“Online Methods in Machine Learning” by Bottou"
  },
  {
    "objectID": "posts/online-learning/index.html#fundamental-concepts",
    "href": "posts/online-learning/index.html#fundamental-concepts",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Learning process: 1. Receive instance \\(x_t\\) 2. Predict \\(\\hat{y}_t\\) 3. Observe true outcome \\(y_t\\) 4. Suffer loss \\(\\ell({\\hat{y}_t, y_t})\\) 5. Update model\nRegret definition:\n\\[\nR_T = \\sum_{t=1}^T \\ell(h_t(x_t), y_t) - \\min_{h \\in \\mathcal{H}}\\sum_{t=1}^T \\ell(h(x_t), y_t)\n\\]\n\n\n\nExternal regret:\n\\[\nR_T^{\\text{ext}} = \\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a_t)\\right] - \\min_{a \\in \\mathcal{A}}\\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a)\\right]\n\\]\nInternal/swap regret:\n\\[\nR_T^{\\text{int}} = \\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(a_t)\\right] - \\min_{\\phi: \\mathcal{A} \\to \\mathcal{A}}\\mathbb{E}\\left[\\sum_{t=1}^T \\ell_t(\\phi(a_t))\\right]\n\\]\n\n\n\nAverage regret:\n\\[\n\\bar{R}_T = \\frac{R_T}{T}\n\\]\nCompetitive ratio:\n\\[\nCR_T = \\frac{\\sum_{t=1}^T \\ell_t(a_t)}{\\min_{a \\in \\mathcal{A}}\\sum_{t=1}^T \\ell_t(a)}\n\\]"
  },
  {
    "objectID": "posts/online-learning/index.html#online-convex-optimization",
    "href": "posts/online-learning/index.html#online-convex-optimization",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Update rule:\n\\[\nw_{t+1} = \\Pi_{\\mathcal{W}}(w_t - \\eta_t \\nabla \\ell_t(w_t))\n\\]\nRegret bound:\n\\[\nR_T \\leq \\frac{D^2}{2\\eta} + \\frac{\\eta G^2T}{2}\n\\]\nWhere: - \\(D\\) is diameter of feasible set - \\(G\\) is gradient bound - \\(\\eta\\) is learning rate\n\n\n\nFTRL update:\n\\[\nw_{t+1} = \\arg\\min_{w \\in \\mathcal{W}}\\{\\eta\\sum_{s=1}^t \\ell_s(w) + R(w)\\}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\frac{R(w^*)}{\\eta} + \\frac{\\eta G^2T}{2}\n\\]\n\n\n\nUpdate rule:\n\\[\n\\nabla \\psi(w_{t+1}) = \\nabla \\psi(w_t) - \\eta_t \\nabla \\ell_t(w_t)\n\\]\nRegret bound:\n\\[\nR_T \\leq \\frac{D_\\psi(w^*\\|w_1)}{\\eta} + \\frac{\\eta G^2T}{2}\n\\]"
  },
  {
    "objectID": "posts/online-learning/index.html#multi-armed-bandits",
    "href": "posts/online-learning/index.html#multi-armed-bandits",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "UCB index:\n\\[\nUCB_i(t) = \\hat{\\mu}_i(t) + \\sqrt{\\frac{2\\ln t}{N_i(t)}}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sum_{i:\\Delta_i&gt;0}\\left(\\frac{8\\ln T}{\\Delta_i} + (1+\\pi^2/3)\\Delta_i\\right)\n\\]\n\n\n\nPosterior sampling:\n\\[\n\\theta_i(t) \\sim Beta(\\alpha_i(t), \\beta_i(t))\n\\]\nRegret bound:\n\\[\nR_T \\leq O(\\sqrt{KT\\ln T})\n\\]\n\n\n\nProbability update:\n\\[\np_i(t) = \\frac{(1-\\gamma)\\exp(\\eta G_i(t))}{\\sum_{j=1}^K \\exp(\\eta G_j(t))} + \\frac{\\gamma}{K}\n\\]\nRegret bound:\n\\[\nR_T \\leq 2\\sqrt{KT\\ln K}\n\\]"
  },
  {
    "objectID": "posts/online-learning/index.html#expert-algorithms",
    "href": "posts/online-learning/index.html#expert-algorithms",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Weight update:\n\\[\nw_i(t+1) = w_i(t)(1-\\eta)^{\\ell_i(t)}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sqrt{T\\ln N}\n\\]\n\n\n\nProbability update:\n\\[\np_i(t+1) = \\frac{\\exp(-\\eta L_i(t))}{\\sum_{j=1}^N \\exp(-\\eta L_j(t))}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sqrt{2T\\ln N}\n\\]\n\n\n\nAdaptive learning rate:\n\\[\n\\eta_t = \\frac{\\ln N}{V_t}\n\\]\nWhere: - \\(V_t\\) is cumulative variance - \\(N\\) is number of experts"
  },
  {
    "objectID": "posts/online-learning/index.html#advanced-topics",
    "href": "posts/online-learning/index.html#advanced-topics",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "AdaGrad update:\n\\[\nw_{t+1,i} = w_{t,i} - \\frac{\\eta}{\\sqrt{\\sum_{s=1}^t g_{s,i}^2}}g_{t,i}\n\\]\nRegret bound:\n\\[\nR_T \\leq O(\\sqrt{T}\\|w^*\\|_2\\sqrt{\\sum_{i=1}^d\\sum_{t=1}^T g_{t,i}^2})\n\\]\n\n\n\nONS update:\n\\[\nw_{t+1} = w_t - \\eta A_t^{-1}\\nabla \\ell_t(w_t)\n\\]\nWhere: - \\(A_t = \\sum_{s=1}^t \\nabla \\ell_s(w_s)\\nabla \\ell_s(w_s)^T\\)\n\n\n\nMetaGrad update:\n\\[\nw_{t+1} = w_t - \\eta_t H_t^{-1/2}\\nabla \\ell_t(w_t)\n\\]\nWhere: - \\(H_t\\) is preconditioner matrix"
  },
  {
    "objectID": "posts/online-learning/index.html#applications",
    "href": "posts/online-learning/index.html#applications",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Objective:\n\\[\n\\max_{w \\in \\Delta_n} \\sum_{t=1}^T \\log(w^T r_t)\n\\]\nUniversal portfolio:\n\\[\nw_{t+1} = \\int_{\\Delta_n} w P_t(w)dw\n\\]\n\n\n\nFlow update:\n\\[\nf_{t+1}(e) = f_t(e)\\exp(-\\eta \\ell_t(e))\n\\]\nPath selection:\n\\[\nP(p) = \\frac{\\prod_{e \\in p}f_t(e)}{\\sum_{p' \\in \\mathcal{P}}\\prod_{e \\in p'}f_t(e)}\n\\]\n\n\n\nPerceptron update:\n\\[\nw_{t+1} = w_t + y_tx_t\\mathbb{1}[y_tw_t^Tx_t \\leq 0]\n\\]\nMistake bound:\n\\[\nM \\leq \\left(\\frac{R}{\\gamma}\\right)^2\n\\]"
  },
  {
    "objectID": "posts/online-learning/index.html#theoretical-results",
    "href": "posts/online-learning/index.html#theoretical-results",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Minimax regret:\n\\[\n\\min_{\\text{ALG}}\\max_{\\text{ADV}} R_T = \\Omega(\\sqrt{T})\n\\]\nExpert setting:\n\\[\nR_T = \\Omega(\\sqrt{T\\ln N})\n\\]\n\n\n\nRedundancy bound:\n\\[\nR_T \\leq \\frac{KL(P\\|Q) + \\ln(1/\\delta)}{\\eta} + \\frac{\\eta T}{8}\n\\]\n\n\n\nNash equilibrium:\n\\[\n\\max_P \\min_Q \\mathbb{E}_{p \\sim P, q \\sim Q}[M(p,q)] = \\min_Q \\max_P \\mathbb{E}_{p \\sim P, q \\sim Q}[M(p,q)]\n\\]"
  },
  {
    "objectID": "posts/online-learning/index.html#implementation-considerations",
    "href": "posts/online-learning/index.html#implementation-considerations",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Problem Structure:\n\nConvexity\nSmoothness\nSparsity\n\nComputational Constraints:\n\nMemory limits\nUpdate time\nParallelization\n\nPerformance Requirements:\n\nRegret bounds\nAdaptation speed\nRobustness\n\n\n\n\n\n\nLearning Rates:\n\nFixed vs adaptive\nSchedule design\nInitialization\n\nExploration:\n\nExploration rate\nDecay schedule\nAdaptive schemes\n\nRegularization:\n\nStrength\nType selection\nAdaptation"
  },
  {
    "objectID": "posts/online-learning/index.html#best-practices",
    "href": "posts/online-learning/index.html#best-practices",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Robustness:\n\nAdversarial scenarios\nNoise handling\nDistribution shifts\n\nEfficiency:\n\nMemory usage\nUpdate complexity\nParallelization\n\nAdaptivity:\n\nParameter tuning\nDistribution changes\nModel selection\n\n\n\n\n\n\nData Handling:\n\nStreaming processing\nFeature extraction\nPreprocessing\n\nMonitoring:\n\nRegret tracking\nPerformance metrics\nResource usage\n\nDeployment:\n\nSystem integration\nError handling\nScaling strategy"
  },
  {
    "objectID": "posts/online-learning/index.html#references",
    "href": "posts/online-learning/index.html#references",
    "title": "Online Learning and Regret Minimization",
    "section": "",
    "text": "Theory:\n\n“Introduction to Online Convex Optimization” by Hazan\n“Bandit Algorithms” by Lattimore and Szepesvári\n“Prediction, Learning, and Games” by Cesa-Bianchi and Lugosi\n\nMethods:\n\n“Online Learning and Online Convex Optimization” by Shalev-Shwartz\n“A Modern Introduction to Online Learning” by Orabona\n“Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems” by Bubeck and Cesa-Bianchi\n\nApplications:\n\n“Online Portfolio Selection” by Li and Hoi\n“Online Learning in Routing Games” by Roughgarden\n“Online Methods in Machine Learning” by Bottou"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html",
    "href": "posts/ml-in-everyday-life/index.html",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "You might not realize it, but machine learning is already a big part of your daily routine. From the moment you wake up until you go to bed, ML algorithms are working behind the scenes to make your life easier and more convenient.\n\n\n\n\n\nSmart Thermostats\n\nLearn your temperature preferences\nAdjust based on time of day\nSave energy by predicting when you’re away\nExample: Nest Learning Thermostat\n\nVoice Assistants\n\nRecognize your voice commands\nLearn your accent and speaking patterns\nImprove understanding over time\nExamples: Alexa, Google Assistant, Siri\n\n\n\n\n\n\nFace Recognition\n\nUnlocks your phone securely\nAdapts to changes in appearance\nWorks in different lighting conditions\n\nKeyboard Predictions\n\nLearns your typing patterns\nSuggests words based on context\nAdapts to your vocabulary\n\n\n\n\n\n\n\n\n\nTraffic Prediction\n\nAnalyzes historical traffic patterns\nPredicts delays in real-time\nSuggests faster routes\nExample: Google Maps, Waze\n\n\n\n\n\n\nPrice Optimization\n\nAdjusts prices based on demand\nPredicts busy periods\nMatches drivers efficiently\nExamples: Uber, Lyft\n\n\n\n\n\n\n\n\n\nSpam Filtering\n\nIdentifies unwanted emails\nLearns from your actions\nAdapts to new spam patterns\n\nSmart Categorization\n\nSorts emails automatically\nPrioritizes important messages\nSuggests quick responses\n\n\n\n\n\n\nDocument Search\n\nUnderstands natural language queries\nFinds relevant files quickly\nImproves with usage\n\nMeeting Scheduling\n\nLearns preferred meeting times\nSuggests optimal slots\nConsiders participants’ schedules\n\n\n\n\n\n\n\n\n\nProduct Recommendations\n\nBased on your browsing history\nSimilar items you might like\nPersonalized deals\nExample: Amazon’s recommendations\n\nPrice Tracking\n\nPredicts price changes\nAlerts for best buying times\nFinds similar products\n\n\n\n\n\n\nContent Recommendations\n\nLearns your viewing preferences\nSuggests new shows/movies\nPersonalizes homepage\nExamples: Netflix, Spotify\n\n\n\n\n\n\nFeed Customization\n\nShows relevant content\nLearns from your interactions\nFilters unwanted content\nExamples: Instagram, Facebook\n\n\n\n\n\n\n\n\n\nActivity Recognition\n\nIdentifies exercise types\nCounts steps accurately\nMonitors sleep patterns\n\nHealth Insights\n\nPredicts fitness trends\nSuggests workout improvements\nPersonalizes goals\n\n\n\n\n\n\nSymptom Checking\n\nAnalyzes symptoms\nSuggests possible causes\nRecommends actions\n\nMental Health Support\n\nMood tracking\nPersonalized recommendations\nEarly warning signs\n\n\n\n\n\n\n\n\n\nFraud Detection\n\nSpots unusual transactions\nPrevents unauthorized use\nLearns spending patterns\n\nAutomated Banking\n\nSmart ATMs\nChatbot customer service\nPersonalized financial advice\n\n\n\n\n\n\nBudget Apps\n\nCategorize expenses\nPredict future spending\nSuggest savings opportunities\n\n\n\n\n\n\n\n\n\nIdentifies regular behaviors\nSpots unusual activities\nLearns from historical data\n\n\n\n\n\nAdapts to individual preferences\nImproves with more data\nCreates unique experiences\n\n\n\n\n\nAnticipates needs\nForecasts events\nSuggests actions\n\n\n\n\n\n\n\n\nAutomates routine tasks\nProvides quick answers\nReduces decision time\n\n\n\n\n\nMore informed choices\nPersonalized recommendations\nData-driven insights\n\n\n\n\n\nCustomized content\nSmoother interactions\nProactive assistance\n\n\n\n\n\n\n\n\nWhat data is collected\nHow it’s used\nStorage security\n\n\n\n\n\nPrivacy settings\nOpt-out options\nData access rights\n\n\n\n\n\nReview app permissions\nRegular privacy checks\nUnderstand data usage\n\n\n\n\n\n\n\n\nDeeper understanding\nBetter predictions\nTailored experiences\n\n\n\n\n\nSeamless connections\nCross-device harmony\nUnified experiences\n\n\n\n\n\nBetter data protection\nMore user control\nTransparent practices\n\n\n\n\n\n\n\n\nNotice ML in daily life\nUnderstand basic concepts\nStay informed of changes\n\n\n\n\n\nEnable helpful features\nMaintain privacy\nProvide feedback\n\n\n\n\n\nReview settings regularly\nUnderstand data sharing\nMake informed choices\n\n\n\n\n\nMachine learning is: 1. Already part of daily life 2. Making things easier 3. Constantly improving 4. Working behind the scenes\nRemember: - ML is a tool to help you - You control how to use it - Balance convenience and privacy - Stay informed about changes\n\n\n\n\nFor Learning More:\n\n“AI Basics” courses on Coursera\nYouTube channels about technology\nTech news websites\n\nFor Privacy:\n\nPrivacy setting guides\nData protection websites\nSecurity best practices\n\nFor Updates:\n\nTechnology blogs\nML news websites\nCompany update pages\n\n\nRemember: Machine learning is here to help make your life easier, but it’s important to use it wisely and stay informed about how it affects your daily activities."
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#morning-routine-with-ml",
    "href": "posts/ml-in-everyday-life/index.html#morning-routine-with-ml",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Smart Thermostats\n\nLearn your temperature preferences\nAdjust based on time of day\nSave energy by predicting when you’re away\nExample: Nest Learning Thermostat\n\nVoice Assistants\n\nRecognize your voice commands\nLearn your accent and speaking patterns\nImprove understanding over time\nExamples: Alexa, Google Assistant, Siri\n\n\n\n\n\n\nFace Recognition\n\nUnlocks your phone securely\nAdapts to changes in appearance\nWorks in different lighting conditions\n\nKeyboard Predictions\n\nLearns your typing patterns\nSuggests words based on context\nAdapts to your vocabulary"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#during-your-commute",
    "href": "posts/ml-in-everyday-life/index.html#during-your-commute",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Traffic Prediction\n\nAnalyzes historical traffic patterns\nPredicts delays in real-time\nSuggests faster routes\nExample: Google Maps, Waze\n\n\n\n\n\n\nPrice Optimization\n\nAdjusts prices based on demand\nPredicts busy periods\nMatches drivers efficiently\nExamples: Uber, Lyft"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#at-work",
    "href": "posts/ml-in-everyday-life/index.html#at-work",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Spam Filtering\n\nIdentifies unwanted emails\nLearns from your actions\nAdapts to new spam patterns\n\nSmart Categorization\n\nSorts emails automatically\nPrioritizes important messages\nSuggests quick responses\n\n\n\n\n\n\nDocument Search\n\nUnderstands natural language queries\nFinds relevant files quickly\nImproves with usage\n\nMeeting Scheduling\n\nLearns preferred meeting times\nSuggests optimal slots\nConsiders participants’ schedules"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#shopping-and-entertainment",
    "href": "posts/ml-in-everyday-life/index.html#shopping-and-entertainment",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Product Recommendations\n\nBased on your browsing history\nSimilar items you might like\nPersonalized deals\nExample: Amazon’s recommendations\n\nPrice Tracking\n\nPredicts price changes\nAlerts for best buying times\nFinds similar products\n\n\n\n\n\n\nContent Recommendations\n\nLearns your viewing preferences\nSuggests new shows/movies\nPersonalizes homepage\nExamples: Netflix, Spotify\n\n\n\n\n\n\nFeed Customization\n\nShows relevant content\nLearns from your interactions\nFilters unwanted content\nExamples: Instagram, Facebook"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#health-and-fitness",
    "href": "posts/ml-in-everyday-life/index.html#health-and-fitness",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Activity Recognition\n\nIdentifies exercise types\nCounts steps accurately\nMonitors sleep patterns\n\nHealth Insights\n\nPredicts fitness trends\nSuggests workout improvements\nPersonalizes goals\n\n\n\n\n\n\nSymptom Checking\n\nAnalyzes symptoms\nSuggests possible causes\nRecommends actions\n\nMental Health Support\n\nMood tracking\nPersonalized recommendations\nEarly warning signs"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#financial-services",
    "href": "posts/ml-in-everyday-life/index.html#financial-services",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Fraud Detection\n\nSpots unusual transactions\nPrevents unauthorized use\nLearns spending patterns\n\nAutomated Banking\n\nSmart ATMs\nChatbot customer service\nPersonalized financial advice\n\n\n\n\n\n\nBudget Apps\n\nCategorize expenses\nPredict future spending\nSuggest savings opportunities"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#how-ml-makes-these-possible",
    "href": "posts/ml-in-everyday-life/index.html#how-ml-makes-these-possible",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Identifies regular behaviors\nSpots unusual activities\nLearns from historical data\n\n\n\n\n\nAdapts to individual preferences\nImproves with more data\nCreates unique experiences\n\n\n\n\n\nAnticipates needs\nForecasts events\nSuggests actions"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#benefits-in-daily-life",
    "href": "posts/ml-in-everyday-life/index.html#benefits-in-daily-life",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Automates routine tasks\nProvides quick answers\nReduces decision time\n\n\n\n\n\nMore informed choices\nPersonalized recommendations\nData-driven insights\n\n\n\n\n\nCustomized content\nSmoother interactions\nProactive assistance"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#privacy-considerations",
    "href": "posts/ml-in-everyday-life/index.html#privacy-considerations",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "What data is collected\nHow it’s used\nStorage security\n\n\n\n\n\nPrivacy settings\nOpt-out options\nData access rights\n\n\n\n\n\nReview app permissions\nRegular privacy checks\nUnderstand data usage"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#future-trends",
    "href": "posts/ml-in-everyday-life/index.html#future-trends",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Deeper understanding\nBetter predictions\nTailored experiences\n\n\n\n\n\nSeamless connections\nCross-device harmony\nUnified experiences\n\n\n\n\n\nBetter data protection\nMore user control\nTransparent practices"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#making-the-most-of-ml",
    "href": "posts/ml-in-everyday-life/index.html#making-the-most-of-ml",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Notice ML in daily life\nUnderstand basic concepts\nStay informed of changes\n\n\n\n\n\nEnable helpful features\nMaintain privacy\nProvide feedback\n\n\n\n\n\nReview settings regularly\nUnderstand data sharing\nMake informed choices"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#conclusion",
    "href": "posts/ml-in-everyday-life/index.html#conclusion",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "Machine learning is: 1. Already part of daily life 2. Making things easier 3. Constantly improving 4. Working behind the scenes\nRemember: - ML is a tool to help you - You control how to use it - Balance convenience and privacy - Stay informed about changes"
  },
  {
    "objectID": "posts/ml-in-everyday-life/index.html#additional-resources",
    "href": "posts/ml-in-everyday-life/index.html#additional-resources",
    "title": "Machine Learning in Everyday Life: A Practical Guide",
    "section": "",
    "text": "For Learning More:\n\n“AI Basics” courses on Coursera\nYouTube channels about technology\nTech news websites\n\nFor Privacy:\n\nPrivacy setting guides\nData protection websites\nSecurity best practices\n\nFor Updates:\n\nTechnology blogs\nML news websites\nCompany update pages\n\n\nRemember: Machine learning is here to help make your life easier, but it’s important to use it wisely and stay informed about how it affects your daily activities."
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html",
    "href": "posts/ml-fundamentals-beginners/index.html",
    "title": "Machine Learning: From Theory to Practice",
    "section": "",
    "text": "Have you ever wondered how Spotify seems to read your mind when recommending new songs? Or how your smartphone’s camera instantly knows when you’re smiling? These seemingly magical capabilities are powered by machine learning, and in this post, we’ll not only understand how they work but also build our own machine learning models from scratch."
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#the-art-and-science-of-machine-learning",
    "href": "posts/ml-fundamentals-beginners/index.html#the-art-and-science-of-machine-learning",
    "title": "Machine Learning: From Theory to Practice",
    "section": "The Art and Science of Machine Learning",
    "text": "The Art and Science of Machine Learning\nThink about how you learned to recognize cats and dogs as a child. Nobody gave you a mathematical formula for “cat-ness” or “dog-ness.” Instead, you learned from examples. Machine learning works the same way - it’s about teaching computers to learn from experience rather than following explicit rules.\nLet’s see this in action with a simple example:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Create a simple dataset about house prices\n# Imagine: house size (sq ft) vs price ($)\nnp.random.seed(42)\nhouse_sizes = np.linspace(1000, 5000, 100)  # House sizes from 1000 to 5000 sq ft\nhouse_prices = 200 + 0.3 * house_sizes + np.random.normal(0, 50, 100)  # Price formula with some noise\n\nplt.figure(figsize=(10, 6))\nplt.scatter(house_sizes, house_prices, alpha=0.5)\nplt.xlabel('House Size (sq ft)')\nplt.ylabel('Price ($K)')\nplt.title('House Prices vs Size')\nplt.show()\nJust like you might intuitively understand that larger houses generally cost more, our machine learning model will learn this relationship from data. But unlike humans, it will learn the exact mathematical relationship."
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#three-ways-machines-learn",
    "href": "posts/ml-fundamentals-beginners/index.html#three-ways-machines-learn",
    "title": "Machine Learning: From Theory to Practice",
    "section": "Three Ways Machines Learn",
    "text": "Three Ways Machines Learn\n\n1. Supervised Learning: Learning from Examples\nImagine teaching a child to identify fruits. You show them an apple and say “apple,” a banana and say “banana,” and so on. This is supervised learning - we provide both the question (input) and answer (output). Let’s build a simple supervised learning model:\n# Prepare our housing data for machine learning\nX = house_sizes.reshape(-1, 1)  # Features (input)\ny = house_prices  # Target (output)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train our model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Visualize results\nplt.figure(figsize=(10, 6))\nplt.scatter(X_train, y_train, alpha=0.5, label='Training Data')\nplt.plot(X_test, y_pred, 'r', label='Model Predictions', linewidth=2)\nplt.xlabel('House Size (sq ft)')\nplt.ylabel('Price ($K)')\nplt.title('House Price Prediction Model')\nplt.legend()\nplt.show()\n\nprint(f\"Model's prediction for a 2500 sq ft house: ${model.predict([[2500]])[0]:,.2f}K\")\n\n\n2. Unsupervised Learning: Finding Hidden Patterns\nSometimes we want machines to find patterns on their own. This is like organizing your closet - you group similar items together without being told how to categorize them.\nfrom sklearn.cluster import KMeans\n\n# Create data about customer shopping behavior\n# Features: money spent vs number of items bought\ncustomer_data = np.random.randn(300, 2)  # Generate random customer data\ncustomer_data[:100] += [2, 2]  # High spenders\ncustomer_data[100:200] += [-2, -2]  # Budget shoppers\n\n# Apply clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(customer_data)\n\n# Visualize customer segments\nplt.figure(figsize=(10, 6))\nplt.scatter(customer_data[:, 0], customer_data[:, 1], c=clusters, cmap='viridis')\nplt.title('Customer Segments Based on Shopping Behavior')\nplt.xlabel('Money Spent')\nplt.ylabel('Number of Items')\nplt.colorbar(label='Customer Segment')\nplt.show()\n\n\n3. Reinforcement Learning: Learning through Trial and Error\nThink of teaching a dog new tricks. You reward good behavior and discourage bad behavior. Similarly, reinforcement learning is about learning optimal actions through rewards and penalties."
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#the-machine-learning-pipeline-a-real-world-example",
    "href": "posts/ml-fundamentals-beginners/index.html#the-machine-learning-pipeline-a-real-world-example",
    "title": "Machine Learning: From Theory to Practice",
    "section": "The Machine Learning Pipeline: A Real-World Example",
    "text": "The Machine Learning Pipeline: A Real-World Example\nLet’s walk through a complete machine learning project using real-world data:\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load and prepare data\ndef prepare_data(data):\n    \"\"\"Prepare data for modeling\"\"\"\n    # Handle missing values\n    data = data.fillna(data.mean())\n    \n    # Scale features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(data)\n    \n    return pd.DataFrame(scaled_features, columns=data.columns)\n\n# Create example dataset\ndata = pd.DataFrame({\n    'age': np.random.normal(35, 10, 1000),\n    'income': np.random.normal(50000, 20000, 1000),\n    'credit_score': np.random.normal(700, 50, 1000)\n})\n\n# Add target variable (loan approval)\ndata['loan_approved'] = (data['credit_score'] &gt; 700) & (data['income'] &gt; 45000)\n\n# Prepare features and target\nX = data.drop('loan_approved', axis=1)\ny = data['loan_approved']\n\n# Split and prepare data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_prepared = prepare_data(X_train)\nX_test_prepared = prepare_data(X_test)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train_prepared, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test_prepared)\n\n# Evaluate model\nprint(\"\\nModel Performance:\")\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nplt.bar(feature_importance['feature'], feature_importance['importance'])\nplt.title('Feature Importance in Loan Approval')\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#common-challenges-and-solutions",
    "href": "posts/ml-fundamentals-beginners/index.html#common-challenges-and-solutions",
    "title": "Machine Learning: From Theory to Practice",
    "section": "Common Challenges and Solutions",
    "text": "Common Challenges and Solutions\n\n1. Overfitting: When Your Model Memorizes Instead of Learning\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curves(model, X, y):\n    \"\"\"Visualize model learning process\"\"\"\n    train_sizes, train_scores, val_scores = learning_curve(\n        model, X, y, cv=5, n_jobs=-1, \n        train_sizes=np.linspace(0.1, 1.0, 10))\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')\n    plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation Score')\n    plt.xlabel('Training Examples')\n    plt.ylabel('Score')\n    plt.title('Learning Curves: Diagnosing Overfitting')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_learning_curves(RandomForestClassifier(), X_train_prepared, y_train)"
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#best-practices-and-tips",
    "href": "posts/ml-fundamentals-beginners/index.html#best-practices-and-tips",
    "title": "Machine Learning: From Theory to Practice",
    "section": "Best Practices and Tips",
    "text": "Best Practices and Tips\n\nStart Simple\n\nBegin with basic models\nUnderstand your data thoroughly\nEstablish baseline performance\n\nValidate Properly\n\nUse cross-validation\nTest on unseen data\nMonitor multiple metrics\n\nIterate and Improve\n\nTry different models\nTune hyperparameters\nEngineer better features"
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#next-steps-in-your-ml-journey",
    "href": "posts/ml-fundamentals-beginners/index.html#next-steps-in-your-ml-journey",
    "title": "Machine Learning: From Theory to Practice",
    "section": "Next Steps in Your ML Journey",
    "text": "Next Steps in Your ML Journey\nThe examples we’ve covered are just the beginning. Here are some directions to explore:\n\nAdvanced Techniques\n\nDeep Learning\nNatural Language Processing\nComputer Vision\n\nReal-World Projects\n\nKaggle Competitions\nPersonal Projects\nOpen Source Contributions\n\nBest Practices\n\nModel Deployment\nMLOps\nEthical AI\n\n\nRemember, machine learning is both an art and a science. The science comes from understanding the mathematical foundations, while the art lies in making the right choices about data preparation, feature engineering, and model selection. Keep experimenting, keep learning, and most importantly, enjoy the process of teaching machines to learn!"
  },
  {
    "objectID": "posts/ml-fundamentals-beginners/index.html#resources-for-further-learning",
    "href": "posts/ml-fundamentals-beginners/index.html#resources-for-further-learning",
    "title": "Machine Learning: From Theory to Practice",
    "section": "Resources for Further Learning",
    "text": "Resources for Further Learning\n\nOnline Courses\n\nFast.ai\nCoursera Machine Learning\nGoogle’s ML Crash Course\n\nBooks\n\n“Hands-On Machine Learning” by Aurélien Géron\n“Python Machine Learning” by Sebastian Raschka\n“Introduction to Statistical Learning” by James et al.\n\nPractice Platforms\n\nKaggle\nGoogle Colab\nGitHub Projects\n\n\nThe journey into machine learning is exciting and rewarding. Start with these fundamentals, practice regularly, and gradually tackle more complex problems. The field is constantly evolving, offering endless opportunities to learn and create amazing things!"
  },
  {
    "objectID": "posts/generative-models/index.html",
    "href": "posts/generative-models/index.html",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "The VAE objective maximizes the ELBO:\n\\[\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))\n\\]\nWhere: - \\(q_\\phi(z|x)\\) is the encoder (inference model) - \\(p_\\theta(x|z)\\) is the decoder (generative model) - \\(p(z)\\) is the prior distribution - \\(D_{KL}\\) is the Kullback-Leibler divergence\n\n\n\nEnables backpropagation through sampling:\n\\[\nz = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n\\]\nWhere: - \\(\\mu_\\phi(x)\\) is the mean encoder network - \\(\\sigma_\\phi(x)\\) is the standard deviation encoder network - \\(\\odot\\) denotes element-wise multiplication\n\n\n\n\n\n\nThe original GAN formulation:\n\\[\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\n\\]\nWhere: - \\(G\\) is the generator - \\(D\\) is the discriminator - \\(p_{data}\\) is the real data distribution - \\(p_z\\) is the latent distribution\n\n\n\nWGAN objective using Kantorovich-Rubinstein duality:\n\\[\n\\min_G \\max_{D \\in \\mathcal{F}_L} \\mathbb{E}_{x\\sim p_{data}}[D(x)] - \\mathbb{E}_{z\\sim p_z}[D(G(z))]\n\\]\nWhere: - \\(\\mathcal{F}_L\\) is the set of 1-Lipschitz functions\n\n\n\nWGAN-GP regularization term:\n\\[\n\\lambda \\mathbb{E}_{\\hat{x}\\sim p_{\\hat{x}}}[(\\|\\nabla_{\\hat{x}}D(\\hat{x})\\|_2 - 1)^2]\n\\]\nWhere: - \\(\\hat{x}\\) is sampled along straight lines between real and generated samples - \\(\\lambda\\) is the penalty coefficient\n\n\n\n\n\n\nThe forward diffusion process:\n\\[\nq(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_tI)\n\\]\nWith closed form for arbitrary timestep:\n\\[\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)\n\\]\nWhere: - \\(\\beta_t\\) is the noise schedule - \\(\\alpha_t = 1-\\beta_t\\) - \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\)\n\n\n\nThe reverse diffusion process:\n\\[\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n\\]\nTraining objective:\n\\[\n\\mathcal{L} = \\mathbb{E}_{x_0,\\epsilon,t}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2\\right]\n\\]\nWhere: - \\(\\epsilon_\\theta\\) predicts the noise component - \\(x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon\\)\n\n\n\n\n\n\nChange of variables formula:\n\\[\n\\log p_X(x) = \\log p_Z(f^{-1}(x)) + \\log\\left|\\det\\frac{\\partial f^{-1}}{\\partial x}\\right|\n\\]\nWhere: - \\(f\\) is an invertible transformation - \\(p_Z\\) is a simple base distribution\n\n\n\nFactorized probability:\n\\[\np(x) = \\prod_{i=1}^n p(x_i|x_{&lt;i})\n\\]\nWith masked convolutions:\n\\[\ny_i = \\sum_{j \\leq i} m_{ij}(w_{ij} \\cdot x_j)\n\\]\n\n\n\nProbability density:\n\\[\np(x) = \\frac{1}{Z}e^{-E(x)}\n\\]\nWhere: - \\(E(x)\\) is the energy function - \\(Z = \\int e^{-E(x)}dx\\) is the partition function\n\n\n\n\n\n\nJensen-Shannon divergence:\n\\[\nJSD(P\\|Q) = \\frac{1}{2}D_{KL}(P\\|\\frac{P+Q}{2}) + \\frac{1}{2}D_{KL}(Q\\|\\frac{P+Q}{2})\n\\]\n\n\n\nKL-divergence analysis:\n\\[\nD_{KL}(q_\\phi(z|x)\\|p(z)) = \\frac{1}{2}\\sum_{j=1}^d(\\sigma_j^2 + \\mu_j^2 - \\log\\sigma_j^2 - 1)\n\\]\n\n\n\nDenoising score matching:\n\\[\n\\nabla_x \\log p(x) = \\mathbb{E}_{p(t|x)}[\\nabla_x \\log p(x|x_t)]\n\\]\n\n\n\n\n\n\nResolution-dependent loss:\n\\[\n\\mathcal{L}_\\text{total} = \\sum_{r} \\alpha_r \\mathcal{L}_r\n\\]\nWhere: - \\(r\\) is the resolution level - \\(\\alpha_r\\) is the weighting factor\n\n\n\nStyle transfer in latent space:\n\\[\nw = \\mathcal{M}(z) = f(z + \\Delta z)\n\\]\nWhere: - \\(\\mathcal{M}\\) is the mapping network - \\(f\\) is a non-linear transformation\n\n\n\nStyle transfer operation:\n\\[\n\\text{AdaIN}(x,y) = \\sigma(y)\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right) + \\mu(y)\n\\]\n\n\n\n\n\n\nMeasures quality and diversity:\n\\[\nIS = \\exp(\\mathbb{E}_{x\\sim p_g}[D_{KL}(p(y|x)\\|p(y))])\n\\]\n\n\n\nDistribution similarity metric:\n\\[\nFID = \\|\\mu_r - \\mu_g\\|^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r\\Sigma_g)^{1/2})\n\\]\nWhere: - \\(\\mu_r, \\Sigma_r\\) are real data statistics - \\(\\mu_g, \\Sigma_g\\) are generated data statistics\n\n\n\nTwo-way evaluation:\n\\[\n\\begin{aligned}\n\\text{Precision} &= \\mathbb{E}_{x\\sim p_g}[\\max_{y\\sim p_r} s(x,y)] \\\\\n\\text{Recall} &= \\mathbb{E}_{y\\sim p_r}[\\max_{x\\sim p_g} s(x,y)]\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nGenerator Design:\n\nTransposed convolutions vs upsampling\nSkip connections\nAttention mechanisms\n\nDiscriminator Design:\n\nSpectral normalization\nResidual blocks\nMulti-scale discrimination\n\nLoss Functions:\n\nAdversarial loss\nReconstruction loss\nPerceptual loss\n\n\n\n\n\n\nGradient Penalties:\n\nR1 regularization\nPath length regularization\nConsistency regularization\n\nLearning Rate:\n\nTwo time-scale update rule\nAdaptive learning rates\nWarmup scheduling\n\nBatch Size:\n\nGradient accumulation\nMixed precision training\nMemory-efficient backprop\n\n\n\n\n\n\n\n\n\nVAEs for:\n\nStructured latent spaces\nReconstruction tasks\nInterpretable representations\n\nGANs for:\n\nHigh-quality generation\nStyle transfer\nDomain translation\n\nDiffusion Models for:\n\nHigh-fidelity generation\nControlled generation\nRobust training\n\n\n\n\n\n\nLearning Rates:\n\nGenerator: 1e-4 to 1e-3\nDiscriminator: 2e-4 to 2e-3\nVAE: 1e-3 to 1e-2\n\nBatch Sizes:\n\nGANs: 32 to 128\nVAEs: 64 to 256\nDiffusion: 32 to 64\n\nArchitecture:\n\nLayer depth\nChannel width\nAttention layers\n\n\n\n\n\n\n\nTheory:\n\n“Auto-Encoding Variational Bayes” by Kingma and Welling\n“Generative Adversarial Networks” by Goodfellow et al.\n“Denoising Diffusion Probabilistic Models” by Ho et al.\n\nArchitecture:\n\n“Progressive Growing of GANs” by Karras et al.\n“StyleGAN” by Karras et al.\n“Normalizing Flows” by Rezende and Mohamed\n\nTraining:\n\n“Improved Training of Wasserstein GANs” by Gulrajani et al.\n“Large Scale GAN Training for High Fidelity Natural Image Synthesis” by Brock et al.\n“Improved VQGAN for Image Generation” by Esser et al."
  },
  {
    "objectID": "posts/generative-models/index.html#variational-autoencoders-vaes",
    "href": "posts/generative-models/index.html#variational-autoencoders-vaes",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "The VAE objective maximizes the ELBO:\n\\[\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))\n\\]\nWhere: - \\(q_\\phi(z|x)\\) is the encoder (inference model) - \\(p_\\theta(x|z)\\) is the decoder (generative model) - \\(p(z)\\) is the prior distribution - \\(D_{KL}\\) is the Kullback-Leibler divergence\n\n\n\nEnables backpropagation through sampling:\n\\[\nz = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n\\]\nWhere: - \\(\\mu_\\phi(x)\\) is the mean encoder network - \\(\\sigma_\\phi(x)\\) is the standard deviation encoder network - \\(\\odot\\) denotes element-wise multiplication"
  },
  {
    "objectID": "posts/generative-models/index.html#generative-adversarial-networks-gans",
    "href": "posts/generative-models/index.html#generative-adversarial-networks-gans",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "The original GAN formulation:\n\\[\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\n\\]\nWhere: - \\(G\\) is the generator - \\(D\\) is the discriminator - \\(p_{data}\\) is the real data distribution - \\(p_z\\) is the latent distribution\n\n\n\nWGAN objective using Kantorovich-Rubinstein duality:\n\\[\n\\min_G \\max_{D \\in \\mathcal{F}_L} \\mathbb{E}_{x\\sim p_{data}}[D(x)] - \\mathbb{E}_{z\\sim p_z}[D(G(z))]\n\\]\nWhere: - \\(\\mathcal{F}_L\\) is the set of 1-Lipschitz functions\n\n\n\nWGAN-GP regularization term:\n\\[\n\\lambda \\mathbb{E}_{\\hat{x}\\sim p_{\\hat{x}}}[(\\|\\nabla_{\\hat{x}}D(\\hat{x})\\|_2 - 1)^2]\n\\]\nWhere: - \\(\\hat{x}\\) is sampled along straight lines between real and generated samples - \\(\\lambda\\) is the penalty coefficient"
  },
  {
    "objectID": "posts/generative-models/index.html#diffusion-models",
    "href": "posts/generative-models/index.html#diffusion-models",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "The forward diffusion process:\n\\[\nq(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_tI)\n\\]\nWith closed form for arbitrary timestep:\n\\[\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)\n\\]\nWhere: - \\(\\beta_t\\) is the noise schedule - \\(\\alpha_t = 1-\\beta_t\\) - \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\)\n\n\n\nThe reverse diffusion process:\n\\[\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n\\]\nTraining objective:\n\\[\n\\mathcal{L} = \\mathbb{E}_{x_0,\\epsilon,t}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2\\right]\n\\]\nWhere: - \\(\\epsilon_\\theta\\) predicts the noise component - \\(x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon\\)"
  },
  {
    "objectID": "posts/generative-models/index.html#advanced-architectures",
    "href": "posts/generative-models/index.html#advanced-architectures",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "Change of variables formula:\n\\[\n\\log p_X(x) = \\log p_Z(f^{-1}(x)) + \\log\\left|\\det\\frac{\\partial f^{-1}}{\\partial x}\\right|\n\\]\nWhere: - \\(f\\) is an invertible transformation - \\(p_Z\\) is a simple base distribution\n\n\n\nFactorized probability:\n\\[\np(x) = \\prod_{i=1}^n p(x_i|x_{&lt;i})\n\\]\nWith masked convolutions:\n\\[\ny_i = \\sum_{j \\leq i} m_{ij}(w_{ij} \\cdot x_j)\n\\]\n\n\n\nProbability density:\n\\[\np(x) = \\frac{1}{Z}e^{-E(x)}\n\\]\nWhere: - \\(E(x)\\) is the energy function - \\(Z = \\int e^{-E(x)}dx\\) is the partition function"
  },
  {
    "objectID": "posts/generative-models/index.html#training-dynamics",
    "href": "posts/generative-models/index.html#training-dynamics",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "Jensen-Shannon divergence:\n\\[\nJSD(P\\|Q) = \\frac{1}{2}D_{KL}(P\\|\\frac{P+Q}{2}) + \\frac{1}{2}D_{KL}(Q\\|\\frac{P+Q}{2})\n\\]\n\n\n\nKL-divergence analysis:\n\\[\nD_{KL}(q_\\phi(z|x)\\|p(z)) = \\frac{1}{2}\\sum_{j=1}^d(\\sigma_j^2 + \\mu_j^2 - \\log\\sigma_j^2 - 1)\n\\]\n\n\n\nDenoising score matching:\n\\[\n\\nabla_x \\log p(x) = \\mathbb{E}_{p(t|x)}[\\nabla_x \\log p(x|x_t)]\n\\]"
  },
  {
    "objectID": "posts/generative-models/index.html#advanced-training-techniques",
    "href": "posts/generative-models/index.html#advanced-training-techniques",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "Resolution-dependent loss:\n\\[\n\\mathcal{L}_\\text{total} = \\sum_{r} \\alpha_r \\mathcal{L}_r\n\\]\nWhere: - \\(r\\) is the resolution level - \\(\\alpha_r\\) is the weighting factor\n\n\n\nStyle transfer in latent space:\n\\[\nw = \\mathcal{M}(z) = f(z + \\Delta z)\n\\]\nWhere: - \\(\\mathcal{M}\\) is the mapping network - \\(f\\) is a non-linear transformation\n\n\n\nStyle transfer operation:\n\\[\n\\text{AdaIN}(x,y) = \\sigma(y)\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right) + \\mu(y)\n\\]"
  },
  {
    "objectID": "posts/generative-models/index.html#evaluation-metrics",
    "href": "posts/generative-models/index.html#evaluation-metrics",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "Measures quality and diversity:\n\\[\nIS = \\exp(\\mathbb{E}_{x\\sim p_g}[D_{KL}(p(y|x)\\|p(y))])\n\\]\n\n\n\nDistribution similarity metric:\n\\[\nFID = \\|\\mu_r - \\mu_g\\|^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r\\Sigma_g)^{1/2})\n\\]\nWhere: - \\(\\mu_r, \\Sigma_r\\) are real data statistics - \\(\\mu_g, \\Sigma_g\\) are generated data statistics\n\n\n\nTwo-way evaluation:\n\\[\n\\begin{aligned}\n\\text{Precision} &= \\mathbb{E}_{x\\sim p_g}[\\max_{y\\sim p_r} s(x,y)] \\\\\n\\text{Recall} &= \\mathbb{E}_{y\\sim p_r}[\\max_{x\\sim p_g} s(x,y)]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/generative-models/index.html#implementation-considerations",
    "href": "posts/generative-models/index.html#implementation-considerations",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "Generator Design:\n\nTransposed convolutions vs upsampling\nSkip connections\nAttention mechanisms\n\nDiscriminator Design:\n\nSpectral normalization\nResidual blocks\nMulti-scale discrimination\n\nLoss Functions:\n\nAdversarial loss\nReconstruction loss\nPerceptual loss\n\n\n\n\n\n\nGradient Penalties:\n\nR1 regularization\nPath length regularization\nConsistency regularization\n\nLearning Rate:\n\nTwo time-scale update rule\nAdaptive learning rates\nWarmup scheduling\n\nBatch Size:\n\nGradient accumulation\nMixed precision training\nMemory-efficient backprop"
  },
  {
    "objectID": "posts/generative-models/index.html#best-practices",
    "href": "posts/generative-models/index.html#best-practices",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "VAEs for:\n\nStructured latent spaces\nReconstruction tasks\nInterpretable representations\n\nGANs for:\n\nHigh-quality generation\nStyle transfer\nDomain translation\n\nDiffusion Models for:\n\nHigh-fidelity generation\nControlled generation\nRobust training\n\n\n\n\n\n\nLearning Rates:\n\nGenerator: 1e-4 to 1e-3\nDiscriminator: 2e-4 to 2e-3\nVAE: 1e-3 to 1e-2\n\nBatch Sizes:\n\nGANs: 32 to 128\nVAEs: 64 to 256\nDiffusion: 32 to 64\n\nArchitecture:\n\nLayer depth\nChannel width\nAttention layers"
  },
  {
    "objectID": "posts/generative-models/index.html#references",
    "href": "posts/generative-models/index.html#references",
    "title": "Generative Models: Mathematical Foundations and Architectures",
    "section": "",
    "text": "Theory:\n\n“Auto-Encoding Variational Bayes” by Kingma and Welling\n“Generative Adversarial Networks” by Goodfellow et al.\n“Denoising Diffusion Probabilistic Models” by Ho et al.\n\nArchitecture:\n\n“Progressive Growing of GANs” by Karras et al.\n“StyleGAN” by Karras et al.\n“Normalizing Flows” by Rezende and Mohamed\n\nTraining:\n\n“Improved Training of Wasserstein GANs” by Gulrajani et al.\n“Large Scale GAN Training for High Fidelity Natural Image Synthesis” by Brock et al.\n“Improved VQGAN for Image Generation” by Esser et al."
  },
  {
    "objectID": "posts/computational-learning-theory/index.html",
    "href": "posts/computational-learning-theory/index.html",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "PAC learning bound:\n\\[\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{1}{\\delta}\\right)\n\\]\nVC dimension bound:\n\\[\nm = O\\left(\\frac{d}{\\epsilon^2}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon^2}\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nLearning algorithm runtime:\n\\[\nT(m,n,\\epsilon,\\delta) = \\text{poly}(m,n,\\frac{1}{\\epsilon},\\frac{1}{\\delta})\n\\]\nWhere: - \\(m\\) is sample size - \\(n\\) is input dimension - \\(\\epsilon\\) is accuracy - \\(\\delta\\) is confidence\n\n\n\nMemory requirements:\n\\[\nS(m,n) = O(mn)\n\\]\nStreaming bound:\n\\[\nS = O(\\log m + \\log n)\n\\]\n\n\n\n\n\n\nDefinition: - Polynomial sample complexity - Polynomial time complexity - Polynomial space complexity\nRequirements:\n\\[\n\\begin{aligned}\nm &= \\text{poly}(n,\\frac{1}{\\epsilon},\\frac{1}{\\delta}) \\\\\nT &= \\text{poly}(n,\\frac{1}{\\epsilon},\\frac{1}{\\delta}) \\\\\nS &= \\text{poly}(n,\\frac{1}{\\epsilon},\\frac{1}{\\delta})\n\\end{aligned}\n\\]\n\n\n\nCryptographic assumptions:\n\\[\nP \\neq NP \\implies \\text{Not efficiently learnable}\n\\]\nReduction techniques:\n\\[\n\\text{Problem A} \\leq_p \\text{Problem B}\n\\]\n\n\n\nExpected runtime:\n\\[\n\\mathbb{E}[T(X)] = \\sum_{x} T(x)P(x)\n\\]\nSmoothed analysis:\n\\[\n\\max_{\\text{input } I} \\mathbb{E}_{\\text{noise } \\xi}[T(I + \\xi)]\n\\]\n\n\n\n\n\n\nMembership queries:\n\\[\n\\text{MQ}(x) = c(x)\n\\]\nEquivalence queries:\n\\[\n\\text{EQ}(h) = \\begin{cases}\n\\text{Yes} & \\text{if } h \\equiv c \\\\\nx & \\text{where } h(x) \\neq c(x)\n\\end{cases}\n\\]\n\n\n\nMistake bound:\n\\[\nM \\leq \\text{poly}(n,\\text{size}(c))\n\\]\nHalving algorithm:\n\\[\n|\\mathcal{H}_t| \\leq |\\mathcal{H}_0|/2^t\n\\]\n\n\n\nLabel complexity:\n\\[\n\\Lambda(\\epsilon,\\delta) = O(\\theta d\\log(1/\\epsilon)\\log(1/\\delta))\n\\]\nWhere: - \\(\\theta\\) is disagreement coefficient - \\(d\\) is VC dimension\n\n\n\n\n\n\nAdaBoost iterations:\n\\[\nT = O\\left(\\frac{\\log(1/\\epsilon)}{\\gamma^2}\\right)\n\\]\nWhere: - \\(\\gamma\\) is edge over random\n\n\n\nKernel evaluation:\n\\[\nT = O(m^2n)\n\\]\nNyström approximation:\n\\[\n\\|K - \\tilde{K}\\|_2 \\leq \\epsilon\n\\]\n\n\n\nBackpropagation complexity:\n\\[\nT = O(mnd)\n\\]\nWhere: - \\(d\\) is network depth\n\n\n\n\n\n\nMemory-runtime relationship:\n\\[\nT \\cdot S = \\Omega(n^2)\n\\]\nStreaming algorithms:\n\\[\nS \\cdot \\text{passes} = \\Omega(n)\n\\]\n\n\n\nActive learning:\n\\[\n\\text{queries} \\cdot \\text{computation} = O(m\\log m)\n\\]\nParallel speedup:\n\\[\nT_p = \\frac{T_1}{p} + O(\\log p)\n\\]\n\n\n\nApproximation guarantee:\n\\[\nf(x) \\leq (1+\\epsilon)\\text{OPT}\n\\]\nRuntime dependency:\n\\[\nT = O(\\text{poly}(n,1/\\epsilon))\n\\]\n\n\n\n\n\n\nTwo-party protocol:\n\\[\nCC(f) = \\min_P \\max_{x,y} \\text{bits}(P(x,y))\n\\]\nLower bound:\n\\[\nCC(f) \\geq \\log_2 \\text{rank}(M_f)\n\\]\n\n\n\nBoolean circuit size:\n\\[\n\\text{size}(f) = \\min_{C: C \\text{ computes } f} \\text{gates}(C)\n\\]\nDepth bound:\n\\[\n\\text{depth}(f) \\geq \\log_2 \\text{sensitivity}(f)\n\\]\n\n\n\nInformation cost:\n\\[\nIC(P) = I(X;M|Y) + I(Y;M|X)\n\\]\nProtocol compression:\n\\[\n|P'| \\leq O(IC(P)\\log|P|)\n\\]\n\n\n\n\n\n\n\nResource Constraints:\n\nTime efficiency\nMemory usage\nCommunication cost\n\nScalability:\n\nInput size\nDimensionality\nSample complexity\n\nParallelization:\n\nTask decomposition\nCommunication overhead\nLoad balancing\n\n\n\n\n\n\nArchitecture:\n\nProcessing units\nMemory hierarchy\nNetwork topology\n\nOptimization:\n\nCaching strategies\nData structures\nAlgorithm selection\n\nTrade-offs:\n\nAccuracy vs speed\nMemory vs computation\nCommunication vs local processing\n\n\n\n\n\n\n\n\nLWE assumption:\n\\[\n(A,As+e) \\approx_c (A,u)\n\\]\nWhere: - \\(A\\) is random matrix - \\(s\\) is secret - \\(e\\) is error - \\(u\\) is uniform\n\n\n\nQuery complexity:\n\\[\n\\text{SQ-DIM}(\\mathcal{C}) = \\min_{D} \\max_{f \\in \\mathcal{C}} |\\langle f,D \\rangle|\n\\]\n\n\n\nQuery complexity:\n\\[\nQ(\\epsilon) = O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\epsilon})\n\\]\nDistance measure:\n\\[\n\\text{dist}(f,g) = \\text{Pr}_{x}[f(x) \\neq g(x)]\n\\]\n\n\n\n\n\n\n\nComplexity Measures:\n\nAsymptotic bounds\nAverage case\nWorst case\n\nResource Usage:\n\nMemory footprint\nCPU utilization\nI/O operations\n\nScalability:\n\nData size\nDimensionality\nParallelization\n\n\n\n\n\n\nOptimization:\n\nAlgorithm choice\nData structures\nMemory management\n\nTrade-offs:\n\nTime vs space\nAccuracy vs speed\nCommunication vs computation\n\nEvaluation:\n\nBenchmarking\nProfiling\nPerformance analysis\n\n\n\n\n\n\n\nTheory:\n\n“Foundations of Machine Learning” by Mohri et al.\n“Understanding Machine Learning” by Shalev-Shwartz and Ben-David\n“Computational Learning Theory” by Kearns and Vazirani\n\nComplexity:\n\n“Computational Complexity” by Arora and Barak\n“Communication Complexity” by Kushilevitz and Nisan\n“The Nature of Computation” by Moore and Mertens\n\nApplications:\n\n“Algorithmic Learning Theory” by Anthony and Biggs\n“Learning with Kernels” by Schölkopf and Smola\n“Theoretical Computer Science” by Hopcroft and Ullman"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#complexity-measures",
    "href": "posts/computational-learning-theory/index.html#complexity-measures",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "PAC learning bound:\n\\[\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{1}{\\delta}\\right)\n\\]\nVC dimension bound:\n\\[\nm = O\\left(\\frac{d}{\\epsilon^2}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon^2}\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nLearning algorithm runtime:\n\\[\nT(m,n,\\epsilon,\\delta) = \\text{poly}(m,n,\\frac{1}{\\epsilon},\\frac{1}{\\delta})\n\\]\nWhere: - \\(m\\) is sample size - \\(n\\) is input dimension - \\(\\epsilon\\) is accuracy - \\(\\delta\\) is confidence\n\n\n\nMemory requirements:\n\\[\nS(m,n) = O(mn)\n\\]\nStreaming bound:\n\\[\nS = O(\\log m + \\log n)\n\\]"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#learnability-analysis",
    "href": "posts/computational-learning-theory/index.html#learnability-analysis",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Definition: - Polynomial sample complexity - Polynomial time complexity - Polynomial space complexity\nRequirements:\n\\[\n\\begin{aligned}\nm &= \\text{poly}(n,\\frac{1}{\\epsilon},\\frac{1}{\\delta}) \\\\\nT &= \\text{poly}(n,\\frac{1}{\\epsilon},\\frac{1}{\\delta}) \\\\\nS &= \\text{poly}(n,\\frac{1}{\\epsilon},\\frac{1}{\\delta})\n\\end{aligned}\n\\]\n\n\n\nCryptographic assumptions:\n\\[\nP \\neq NP \\implies \\text{Not efficiently learnable}\n\\]\nReduction techniques:\n\\[\n\\text{Problem A} \\leq_p \\text{Problem B}\n\\]\n\n\n\nExpected runtime:\n\\[\n\\mathbb{E}[T(X)] = \\sum_{x} T(x)P(x)\n\\]\nSmoothed analysis:\n\\[\n\\max_{\\text{input } I} \\mathbb{E}_{\\text{noise } \\xi}[T(I + \\xi)]\n\\]"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#learning-models",
    "href": "posts/computational-learning-theory/index.html#learning-models",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Membership queries:\n\\[\n\\text{MQ}(x) = c(x)\n\\]\nEquivalence queries:\n\\[\n\\text{EQ}(h) = \\begin{cases}\n\\text{Yes} & \\text{if } h \\equiv c \\\\\nx & \\text{where } h(x) \\neq c(x)\n\\end{cases}\n\\]\n\n\n\nMistake bound:\n\\[\nM \\leq \\text{poly}(n,\\text{size}(c))\n\\]\nHalving algorithm:\n\\[\n|\\mathcal{H}_t| \\leq |\\mathcal{H}_0|/2^t\n\\]\n\n\n\nLabel complexity:\n\\[\n\\Lambda(\\epsilon,\\delta) = O(\\theta d\\log(1/\\epsilon)\\log(1/\\delta))\n\\]\nWhere: - \\(\\theta\\) is disagreement coefficient - \\(d\\) is VC dimension"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#algorithmic-efficiency",
    "href": "posts/computational-learning-theory/index.html#algorithmic-efficiency",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "AdaBoost iterations:\n\\[\nT = O\\left(\\frac{\\log(1/\\epsilon)}{\\gamma^2}\\right)\n\\]\nWhere: - \\(\\gamma\\) is edge over random\n\n\n\nKernel evaluation:\n\\[\nT = O(m^2n)\n\\]\nNyström approximation:\n\\[\n\\|K - \\tilde{K}\\|_2 \\leq \\epsilon\n\\]\n\n\n\nBackpropagation complexity:\n\\[\nT = O(mnd)\n\\]\nWhere: - \\(d\\) is network depth"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#computational-trade-offs",
    "href": "posts/computational-learning-theory/index.html#computational-trade-offs",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Memory-runtime relationship:\n\\[\nT \\cdot S = \\Omega(n^2)\n\\]\nStreaming algorithms:\n\\[\nS \\cdot \\text{passes} = \\Omega(n)\n\\]\n\n\n\nActive learning:\n\\[\n\\text{queries} \\cdot \\text{computation} = O(m\\log m)\n\\]\nParallel speedup:\n\\[\nT_p = \\frac{T_1}{p} + O(\\log p)\n\\]\n\n\n\nApproximation guarantee:\n\\[\nf(x) \\leq (1+\\epsilon)\\text{OPT}\n\\]\nRuntime dependency:\n\\[\nT = O(\\text{poly}(n,1/\\epsilon))\n\\]"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#advanced-topics",
    "href": "posts/computational-learning-theory/index.html#advanced-topics",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Two-party protocol:\n\\[\nCC(f) = \\min_P \\max_{x,y} \\text{bits}(P(x,y))\n\\]\nLower bound:\n\\[\nCC(f) \\geq \\log_2 \\text{rank}(M_f)\n\\]\n\n\n\nBoolean circuit size:\n\\[\n\\text{size}(f) = \\min_{C: C \\text{ computes } f} \\text{gates}(C)\n\\]\nDepth bound:\n\\[\n\\text{depth}(f) \\geq \\log_2 \\text{sensitivity}(f)\n\\]\n\n\n\nInformation cost:\n\\[\nIC(P) = I(X;M|Y) + I(Y;M|X)\n\\]\nProtocol compression:\n\\[\n|P'| \\leq O(IC(P)\\log|P|)\n\\]"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#practical-implications",
    "href": "posts/computational-learning-theory/index.html#practical-implications",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Resource Constraints:\n\nTime efficiency\nMemory usage\nCommunication cost\n\nScalability:\n\nInput size\nDimensionality\nSample complexity\n\nParallelization:\n\nTask decomposition\nCommunication overhead\nLoad balancing\n\n\n\n\n\n\nArchitecture:\n\nProcessing units\nMemory hierarchy\nNetwork topology\n\nOptimization:\n\nCaching strategies\nData structures\nAlgorithm selection\n\nTrade-offs:\n\nAccuracy vs speed\nMemory vs computation\nCommunication vs local processing"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#theoretical-frameworks",
    "href": "posts/computational-learning-theory/index.html#theoretical-frameworks",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "LWE assumption:\n\\[\n(A,As+e) \\approx_c (A,u)\n\\]\nWhere: - \\(A\\) is random matrix - \\(s\\) is secret - \\(e\\) is error - \\(u\\) is uniform\n\n\n\nQuery complexity:\n\\[\n\\text{SQ-DIM}(\\mathcal{C}) = \\min_{D} \\max_{f \\in \\mathcal{C}} |\\langle f,D \\rangle|\n\\]\n\n\n\nQuery complexity:\n\\[\nQ(\\epsilon) = O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\epsilon})\n\\]\nDistance measure:\n\\[\n\\text{dist}(f,g) = \\text{Pr}_{x}[f(x) \\neq g(x)]\n\\]"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#best-practices",
    "href": "posts/computational-learning-theory/index.html#best-practices",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Complexity Measures:\n\nAsymptotic bounds\nAverage case\nWorst case\n\nResource Usage:\n\nMemory footprint\nCPU utilization\nI/O operations\n\nScalability:\n\nData size\nDimensionality\nParallelization\n\n\n\n\n\n\nOptimization:\n\nAlgorithm choice\nData structures\nMemory management\n\nTrade-offs:\n\nTime vs space\nAccuracy vs speed\nCommunication vs computation\n\nEvaluation:\n\nBenchmarking\nProfiling\nPerformance analysis"
  },
  {
    "objectID": "posts/computational-learning-theory/index.html#references",
    "href": "posts/computational-learning-theory/index.html#references",
    "title": "Computational Learning Theory",
    "section": "",
    "text": "Theory:\n\n“Foundations of Machine Learning” by Mohri et al.\n“Understanding Machine Learning” by Shalev-Shwartz and Ben-David\n“Computational Learning Theory” by Kearns and Vazirani\n\nComplexity:\n\n“Computational Complexity” by Arora and Barak\n“Communication Complexity” by Kushilevitz and Nisan\n“The Nature of Computation” by Moore and Mertens\n\nApplications:\n\n“Algorithmic Learning Theory” by Anthony and Biggs\n“Learning with Kernels” by Schölkopf and Smola\n“Theoretical Computer Science” by Hopcroft and Ullman"
  },
  {
    "objectID": "posts/bend-the-curve-intro/index.html",
    "href": "posts/bend-the-curve-intro/index.html",
    "title": "Welcome to BendTheCurve",
    "section": "",
    "text": "I’m excited to introduce BendTheCurve, a platform dedicated to exploring and sharing insights about data science, machine learning, and their real-world applications. Here, we’ll dive deep into:\n\n\n\nData Science Insights: Regular posts about data analysis, visualization, and interpretation\nMachine Learning Deep Dives: Exploring both basic and advanced ML concepts\nPractical Tutorials: Hands-on guides and code examples\nIndustry Applications: Real-world case studies and applications"
  },
  {
    "objectID": "posts/bend-the-curve-intro/index.html#what-to-expect",
    "href": "posts/bend-the-curve-intro/index.html#what-to-expect",
    "title": "Welcome to BendTheCurve",
    "section": "",
    "text": "Data Science Insights: Regular posts about data analysis, visualization, and interpretation\nMachine Learning Deep Dives: Exploring both basic and advanced ML concepts\nPractical Tutorials: Hands-on guides and code examples\nIndustry Applications: Real-world case studies and applications"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html",
    "href": "posts/advanced-neural-architectures/index.html",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "The fundamental building block of modern architectures:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nWhere: - \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) is the query matrix - \\(K \\in \\mathbb{R}^{m \\times d_k}\\) is the key matrix - \\(V \\in \\mathbb{R}^{m \\times d_v}\\) is the value matrix - \\(d_k\\) is the dimension of keys - \\(\\sqrt{d_k}\\) is the scaling factor\n\n\n\nParallel attention computations:\n\\[\n\\begin{aligned}\n\\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O \\\\\n\\text{where head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\end{aligned}\n\\]\nWhere: - \\(W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}\\) - \\(W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}\\) - \\(W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\) - \\(W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}\\)\n\n\n\n\n\n\nComplete encoder block computation:\n\\[\n\\begin{aligned}\n\\text{MultiHeadAttn} &= \\text{LayerNorm}(x + \\text{MultiHead}(x, x, x)) \\\\\n\\text{FFN}(x) &= \\text{max}(0, xW_1 + b_1)W_2 + b_2 \\\\\n\\text{Output} &= \\text{LayerNorm}(\\text{MultiHeadAttn} + \\text{FFN}(\\text{MultiHeadAttn}))\n\\end{aligned}\n\\]\n\n\n\nSinusoidal position encoding:\n\\[\n\\begin{aligned}\nPE_{(pos,2i)} &= \\sin(pos/10000^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} &= \\cos(pos/10000^{2i/d_{model}})\n\\end{aligned}\n\\]\nWhere: - \\(pos\\) is the position - \\(i\\) is the dimension\n\n\n\n\n\n\nResNet block formulation:\n\\[\ny = F(x, \\{W_i\\}) + x\n\\]\nWith pre-activation variant:\n\\[\n\\begin{aligned}\nh &= \\text{ReLU}(\\text{BN}(x)) \\\\\ny &= W_2\\text{ReLU}(\\text{BN}(W_1h)) + x\n\\end{aligned}\n\\]\n\n\n\nDenseNet connectivity pattern:\n\\[\nx_l = H_l([x_0, x_1, ..., x_{l-1}])\n\\]\nWhere: - \\(x_l\\) is the output of layer \\(l\\) - \\(H_l\\) is a composite function - \\([...]\\) represents concatenation\n\n\n\nChannel attention mechanism:\n\\[\n\\begin{aligned}\nz &= F_{sq}(u) = \\frac{1}{H \\times W}\\sum_{i=1}^H\\sum_{j=1}^W u_c(i,j) \\\\\ns &= F_{ex}(z) = \\sigma(W_2\\text{ReLU}(W_1z))\n\\end{aligned}\n\\]\n\n\n\n\n\n\nPosition-aware attention scoring:\n\\[\n\\text{Attention}(Q, K, V, R) = \\text{softmax}\\left(\\frac{QK^T + QR^T}{\\sqrt{d_k}}\\right)V\n\\]\nWhere: - \\(R\\) is the relative position encoding matrix\n\n\n\nEfficient attention computation:\n\\[\n\\text{LinearAttention}(Q, K, V) = \\phi(Q)(\\phi(K)^TV)\n\\]\nWhere: - \\(\\phi\\) is a feature map (e.g., elu(x) + 1)\n\n\n\nStructured sparsity pattern:\n\\[\n\\text{SparseAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{M \\odot (QK^T)}{\\sqrt{d_k}}\\right)V\n\\]\nWhere: - \\(M\\) is a binary mask matrix - \\(\\odot\\) is element-wise multiplication\n\n\n\n\n\n\nComputation across features:\n\\[\n\\text{LN}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n\\]\nWhere: - \\(\\mu\\) and \\(\\sigma\\) are computed across feature dimension\n\n\n\nFeature group normalization:\n\\[\n\\text{GN}(x) = \\gamma \\odot \\frac{x - \\mu_g}{\\sqrt{\\sigma_g^2 + \\epsilon}} + \\beta\n\\]\nWhere: - \\(\\mu_g\\) and \\(\\sigma_g\\) are computed within groups\n\n\n\n\n\n\nSmooth approximation:\n\\[\n\\text{GELU}(x) = x\\Phi(x) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]\n\\]\n\n\n\nSelf-gated activation:\n\\[\n\\text{Swish}(x) = x \\cdot \\sigma(\\beta x)\n\\]\nWhere: - \\(\\sigma\\) is the sigmoid function - \\(\\beta\\) is a learnable parameter\n\n\n\n\n\n\nOptimization objective:\n\\[\n\\begin{aligned}\n\\min_{\\alpha} & \\quad \\mathcal{L}_{val}(w^*(\\alpha), \\alpha) \\\\\n\\text{s.t.} & \\quad w^*(\\alpha) = \\argmin_w \\mathcal{L}_{train}(w, \\alpha)\n\\end{aligned}\n\\]\n\n\n\nRouting probability:\n\\[\np_{ij} = \\frac{\\exp(\\hat{u}_j|u_i)}{\\sum_k \\exp(\\hat{u}_k|u_i)}\n\\]\nWhere: - \\(u_i\\) is the input capsule - \\(\\hat{u}_j\\) is the prediction vector\n\n\n\n\n\n\nGradient checkpointing:\n\\[\n\\text{memory} = O(\\sqrt{N}) \\text{ instead of } O(N)\n\\]\nWhere: - \\(N\\) is the number of layers\n\n\n\nMixed precision training:\n\\[\n\\begin{aligned}\n\\text{FP16 Forward} &: y = \\text{cast}_{\\text{FP16}}(Wx) \\\\\n\\text{FP32 Master} &: w_{\\text{master}} = w_{\\text{FP32}}\n\\end{aligned}\n\\]\n\n\n\nGradient clipping with norm:\n\\[\ng = \\min\\left(1, \\frac{\\theta}{\\|g\\|}\\right)g\n\\]\nWhere: - \\(\\theta\\) is the clipping threshold - \\(g\\) is the gradient\n\n\n\n\n\n\nDistillation objective:\n\\[\n\\mathcal{L} = \\alpha T^2 \\text{KL}\\left(\\text{softmax}\\left(\\frac{z_t}{T}\\right), \\text{softmax}\\left(\\frac{z_s}{T}\\right)\\right) + (1-\\alpha)\\mathcal{L}_{\\text{CE}}\n\\]\nWhere: - \\(z_t\\) and \\(z_s\\) are teacher and student logits - \\(T\\) is temperature - \\(\\alpha\\) is balancing factor\n\n\n\nCurriculum learning schedule:\n\\[\n\\lambda(t) = \\min\\left(1, \\frac{t}{\\tau}\\right)\n\\]\nWhere: - \\(t\\) is current step - \\(\\tau\\) is ramp-up period\n\n\n\n\n\n\nAttention complexity:\n\\[\n\\begin{aligned}\n\\text{Space} &: O(n^2d) \\\\\n\\text{Time} &: O(n^2d)\n\\end{aligned}\n\\]\nWhere: - \\(n\\) is sequence length - \\(d\\) is hidden dimension\n\n\n\nMaximum path length:\n\\[\n\\text{PathLength} = \\begin{cases}\nO(1) & \\text{for transformers} \\\\\nO(n) & \\text{for RNNs}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nResidual Connections:\n\nUse in deep networks\nMaintain gradient flow\nEnable deeper architectures\n\nNormalization:\n\nPre-normalization for stability\nLayer normalization for transformers\nBatch normalization for CNNs\n\nAttention Mechanisms:\n\nMulti-head attention for diverse features\nRelative position encoding for sequences\nSparse attention for long sequences\n\n\n\n\n\n\nLearning Rate:\n\nLinear warmup\nCosine decay\nLayer-wise learning rates\n\nRegularization:\n\nDropout in attention\nWeight decay\nLabel smoothing\n\nOptimization:\n\nAdam with weight decay\nGradient clipping\nMixed precision training\n\n\n\n\n\n\n\nArchitecture:\n\n“Attention Is All You Need” by Vaswani et al.\n“Deep Residual Learning” by He et al.\n“Densely Connected Networks” by Huang et al.\n\nTraining:\n\n“On Layer Normalization in the Transformer Architecture” by Xiong et al.\n“Understanding the Difficulty of Training Deep Feedforward Neural Networks” by Glorot and Bengio\n“Mixed Precision Training” by Micikevicius et al.\n\nAnalysis:\n\n“On the Relationship between Self-Attention and Convolutional Layers” by Cordonnier et al.\n“The Transformer Family” by Tay et al.\n“What Does BERT Look At?” by Clark et al."
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#self-attention-mechanisms",
    "href": "posts/advanced-neural-architectures/index.html#self-attention-mechanisms",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "The fundamental building block of modern architectures:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nWhere: - \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) is the query matrix - \\(K \\in \\mathbb{R}^{m \\times d_k}\\) is the key matrix - \\(V \\in \\mathbb{R}^{m \\times d_v}\\) is the value matrix - \\(d_k\\) is the dimension of keys - \\(\\sqrt{d_k}\\) is the scaling factor\n\n\n\nParallel attention computations:\n\\[\n\\begin{aligned}\n\\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O \\\\\n\\text{where head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\end{aligned}\n\\]\nWhere: - \\(W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}\\) - \\(W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}\\) - \\(W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\) - \\(W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}\\)"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#transformer-architecture",
    "href": "posts/advanced-neural-architectures/index.html#transformer-architecture",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Complete encoder block computation:\n\\[\n\\begin{aligned}\n\\text{MultiHeadAttn} &= \\text{LayerNorm}(x + \\text{MultiHead}(x, x, x)) \\\\\n\\text{FFN}(x) &= \\text{max}(0, xW_1 + b_1)W_2 + b_2 \\\\\n\\text{Output} &= \\text{LayerNorm}(\\text{MultiHeadAttn} + \\text{FFN}(\\text{MultiHeadAttn}))\n\\end{aligned}\n\\]\n\n\n\nSinusoidal position encoding:\n\\[\n\\begin{aligned}\nPE_{(pos,2i)} &= \\sin(pos/10000^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} &= \\cos(pos/10000^{2i/d_{model}})\n\\end{aligned}\n\\]\nWhere: - \\(pos\\) is the position - \\(i\\) is the dimension"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#modern-architectural-patterns",
    "href": "posts/advanced-neural-architectures/index.html#modern-architectural-patterns",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "ResNet block formulation:\n\\[\ny = F(x, \\{W_i\\}) + x\n\\]\nWith pre-activation variant:\n\\[\n\\begin{aligned}\nh &= \\text{ReLU}(\\text{BN}(x)) \\\\\ny &= W_2\\text{ReLU}(\\text{BN}(W_1h)) + x\n\\end{aligned}\n\\]\n\n\n\nDenseNet connectivity pattern:\n\\[\nx_l = H_l([x_0, x_1, ..., x_{l-1}])\n\\]\nWhere: - \\(x_l\\) is the output of layer \\(l\\) - \\(H_l\\) is a composite function - \\([...]\\) represents concatenation\n\n\n\nChannel attention mechanism:\n\\[\n\\begin{aligned}\nz &= F_{sq}(u) = \\frac{1}{H \\times W}\\sum_{i=1}^H\\sum_{j=1}^W u_c(i,j) \\\\\ns &= F_{ex}(z) = \\sigma(W_2\\text{ReLU}(W_1z))\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#advanced-attention-variants",
    "href": "posts/advanced-neural-architectures/index.html#advanced-attention-variants",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Position-aware attention scoring:\n\\[\n\\text{Attention}(Q, K, V, R) = \\text{softmax}\\left(\\frac{QK^T + QR^T}{\\sqrt{d_k}}\\right)V\n\\]\nWhere: - \\(R\\) is the relative position encoding matrix\n\n\n\nEfficient attention computation:\n\\[\n\\text{LinearAttention}(Q, K, V) = \\phi(Q)(\\phi(K)^TV)\n\\]\nWhere: - \\(\\phi\\) is a feature map (e.g., elu(x) + 1)\n\n\n\nStructured sparsity pattern:\n\\[\n\\text{SparseAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{M \\odot (QK^T)}{\\sqrt{d_k}}\\right)V\n\\]\nWhere: - \\(M\\) is a binary mask matrix - \\(\\odot\\) is element-wise multiplication"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#advanced-normalization-techniques",
    "href": "posts/advanced-neural-architectures/index.html#advanced-normalization-techniques",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Computation across features:\n\\[\n\\text{LN}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n\\]\nWhere: - \\(\\mu\\) and \\(\\sigma\\) are computed across feature dimension\n\n\n\nFeature group normalization:\n\\[\n\\text{GN}(x) = \\gamma \\odot \\frac{x - \\mu_g}{\\sqrt{\\sigma_g^2 + \\epsilon}} + \\beta\n\\]\nWhere: - \\(\\mu_g\\) and \\(\\sigma_g\\) are computed within groups"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#advanced-activation-functions",
    "href": "posts/advanced-neural-architectures/index.html#advanced-activation-functions",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Smooth approximation:\n\\[\n\\text{GELU}(x) = x\\Phi(x) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]\n\\]\n\n\n\nSelf-gated activation:\n\\[\n\\text{Swish}(x) = x \\cdot \\sigma(\\beta x)\n\\]\nWhere: - \\(\\sigma\\) is the sigmoid function - \\(\\beta\\) is a learnable parameter"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#architectural-optimization",
    "href": "posts/advanced-neural-architectures/index.html#architectural-optimization",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Optimization objective:\n\\[\n\\begin{aligned}\n\\min_{\\alpha} & \\quad \\mathcal{L}_{val}(w^*(\\alpha), \\alpha) \\\\\n\\text{s.t.} & \\quad w^*(\\alpha) = \\argmin_w \\mathcal{L}_{train}(w, \\alpha)\n\\end{aligned}\n\\]\n\n\n\nRouting probability:\n\\[\np_{ij} = \\frac{\\exp(\\hat{u}_j|u_i)}{\\sum_k \\exp(\\hat{u}_k|u_i)}\n\\]\nWhere: - \\(u_i\\) is the input capsule - \\(\\hat{u}_j\\) is the prediction vector"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#implementation-considerations",
    "href": "posts/advanced-neural-architectures/index.html#implementation-considerations",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Gradient checkpointing:\n\\[\n\\text{memory} = O(\\sqrt{N}) \\text{ instead of } O(N)\n\\]\nWhere: - \\(N\\) is the number of layers\n\n\n\nMixed precision training:\n\\[\n\\begin{aligned}\n\\text{FP16 Forward} &: y = \\text{cast}_{\\text{FP16}}(Wx) \\\\\n\\text{FP32 Master} &: w_{\\text{master}} = w_{\\text{FP32}}\n\\end{aligned}\n\\]\n\n\n\nGradient clipping with norm:\n\\[\ng = \\min\\left(1, \\frac{\\theta}{\\|g\\|}\\right)g\n\\]\nWhere: - \\(\\theta\\) is the clipping threshold - \\(g\\) is the gradient"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#advanced-training-techniques",
    "href": "posts/advanced-neural-architectures/index.html#advanced-training-techniques",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Distillation objective:\n\\[\n\\mathcal{L} = \\alpha T^2 \\text{KL}\\left(\\text{softmax}\\left(\\frac{z_t}{T}\\right), \\text{softmax}\\left(\\frac{z_s}{T}\\right)\\right) + (1-\\alpha)\\mathcal{L}_{\\text{CE}}\n\\]\nWhere: - \\(z_t\\) and \\(z_s\\) are teacher and student logits - \\(T\\) is temperature - \\(\\alpha\\) is balancing factor\n\n\n\nCurriculum learning schedule:\n\\[\n\\lambda(t) = \\min\\left(1, \\frac{t}{\\tau}\\right)\n\\]\nWhere: - \\(t\\) is current step - \\(\\tau\\) is ramp-up period"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#performance-analysis",
    "href": "posts/advanced-neural-architectures/index.html#performance-analysis",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Attention complexity:\n\\[\n\\begin{aligned}\n\\text{Space} &: O(n^2d) \\\\\n\\text{Time} &: O(n^2d)\n\\end{aligned}\n\\]\nWhere: - \\(n\\) is sequence length - \\(d\\) is hidden dimension\n\n\n\nMaximum path length:\n\\[\n\\text{PathLength} = \\begin{cases}\nO(1) & \\text{for transformers} \\\\\nO(n) & \\text{for RNNs}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#best-practices",
    "href": "posts/advanced-neural-architectures/index.html#best-practices",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Residual Connections:\n\nUse in deep networks\nMaintain gradient flow\nEnable deeper architectures\n\nNormalization:\n\nPre-normalization for stability\nLayer normalization for transformers\nBatch normalization for CNNs\n\nAttention Mechanisms:\n\nMulti-head attention for diverse features\nRelative position encoding for sequences\nSparse attention for long sequences\n\n\n\n\n\n\nLearning Rate:\n\nLinear warmup\nCosine decay\nLayer-wise learning rates\n\nRegularization:\n\nDropout in attention\nWeight decay\nLabel smoothing\n\nOptimization:\n\nAdam with weight decay\nGradient clipping\nMixed precision training"
  },
  {
    "objectID": "posts/advanced-neural-architectures/index.html#references",
    "href": "posts/advanced-neural-architectures/index.html#references",
    "title": "Advanced Neural Network Architectures: A Technical Deep Dive",
    "section": "",
    "text": "Architecture:\n\n“Attention Is All You Need” by Vaswani et al.\n“Deep Residual Learning” by He et al.\n“Densely Connected Networks” by Huang et al.\n\nTraining:\n\n“On Layer Normalization in the Transformer Architecture” by Xiong et al.\n“Understanding the Difficulty of Training Deep Feedforward Neural Networks” by Glorot and Bengio\n“Mixed Precision Training” by Micikevicius et al.\n\nAnalysis:\n\n“On the Relationship between Self-Attention and Convolutional Layers” by Cordonnier et al.\n“The Transformer Family” by Tay et al.\n“What Does BERT Look At?” by Clark et al."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BendThe-Curve",
    "section": "",
    "text": "I’ve always wanted to start a blog, but something held me back. The thought of not having a fixed topic or wondering if it would be interesting enough kept me from taking that first step. As time passed, my interests evolved, and I gained more experience in different areas.\nNow, after diving deep into AI, machine learning, and data science, I’ve finally decided to take the leap. This blog isn’t just about sharing knowledge - it’s about documenting this exciting journey through the ever-evolving world of artificial intelligence and helping others who might be on a similar path.\n\n\nCheck out recent posts below. Find more in the Posts section."
  },
  {
    "objectID": "index.html#why-this-blog",
    "href": "index.html#why-this-blog",
    "title": "BendThe-Curve",
    "section": "",
    "text": "I’ve always wanted to start a blog, but something held me back. The thought of not having a fixed topic or wondering if it would be interesting enough kept me from taking that first step. As time passed, my interests evolved, and I gained more experience in different areas.\nNow, after diving deep into AI, machine learning, and data science, I’ve finally decided to take the leap. This blog isn’t just about sharing knowledge - it’s about documenting this exciting journey through the ever-evolving world of artificial intelligence and helping others who might be on a similar path.\n\n\nCheck out recent posts below. Find more in the Posts section."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Ram Polisetti, a passionate learner exploring the world of Machine Learning and Data Science. Through BendThe-Curve, I document my learning journey, share insights, and build a community of fellow ML enthusiasts."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Ram Polisetti, a passionate learner exploring the world of Machine Learning and Data Science. Through BendThe-Curve, I document my learning journey, share insights, and build a community of fellow ML enthusiasts."
  },
  {
    "objectID": "about.html#why-bendthe-curve",
    "href": "about.html#why-bendthe-curve",
    "title": "About",
    "section": "Why “BendThe-Curve”?",
    "text": "Why “BendThe-Curve”?\nThe name “BendThe-Curve” represents the exponential growth in learning and understanding that comes with consistent practice and exploration in the field of Machine Learning. It’s about:\n\n📈 Accelerating the learning process\n🎯 Finding optimal solutions\n🔄 Continuous improvement\n🌱 Growing together as a community"
  },
  {
    "objectID": "about.html#blog-focus",
    "href": "about.html#blog-focus",
    "title": "About",
    "section": "Blog Focus",
    "text": "Blog Focus\nThis blog covers:\n\nMachine Learning Algorithms & Applications\nData Analysis & Visualization\nPython Programming\nReal-world ML Projects\nLearning Resources & Tips\n\nFeel free to reach out through any of the social links above. Let’s learn and grow together!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "ML Journey Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nWelcome to BendTheCurve\n\n\n\n\n\n\nwelcome\n\n\nintroduction\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAdvanced Neural Network Architectures: A Technical Deep Dive\n\n\n\n\n\n\ndeep-learning\n\n\nneural-networks\n\n\narchitectures\n\n\nmathematics\n\n\n\nA comprehensive technical exploration of advanced neural network architectures, including transformers, attention mechanisms, and modern architectural patterns.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nAlgorithmic Stability and Learning Theory\n\n\n\n\n\n\nmachine-learning\n\n\ntheory\n\n\nmathematics\n\n\nstability\n\n\n\nA rigorous exploration of algorithmic stability and its role in learning theory, covering different notions of stability and their implications for generalization.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nCausal Inference and Structural Learning\n\n\n\n\n\n\nmachine-learning\n\n\ncausality\n\n\nstatistics\n\n\nmathematics\n\n\n\nA rigorous exploration of causal inference and structural learning, covering identification, estimation, and structural causal models.\n\n\n\n\n\nMar 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\nComputational Learning Theory\n\n\n\n\n\n\nmachine-learning\n\n\ntheory\n\n\ncomplexity\n\n\nalgorithms\n\n\n\nA rigorous exploration of computational learning theory, covering complexity analysis, learnability, and algorithmic efficiency in machine learning.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nDeep Learning: The Technology Behind AI’s Recent Breakthroughs\n\n\n\n\n\n\ndeep-learning\n\n\nneural-networks\n\n\nbeginner\n\n\n\nDiscover how deep learning is revolutionizing artificial intelligence and why it’s become the driving force behind recent AI breakthroughs.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nGenerative Models: Mathematical Foundations and Architectures\n\n\n\n\n\n\nmachine-learning\n\n\ngenerative-models\n\n\ndeep-learning\n\n\nmathematics\n\n\n\nA rigorous mathematical exploration of generative models, including GANs, VAEs, and diffusion models.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nInformation Theory in Machine Learning\n\n\n\n\n\n\nmachine-learning\n\n\ninformation-theory\n\n\nmathematics\n\n\ntheory\n\n\n\nA rigorous exploration of information theory principles and their applications in machine learning algorithms.\n\n\n\n\n\nMar 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\nMachine Learning: From Theory to Practice\n\n\n\n\n\n\nmachine-learning\n\n\nfundamentals\n\n\npython\n\n\nhands-on\n\n\n\nA practical journey through machine learning fundamentals, combining intuitive explanations with hands-on Python examples.\n\n\n\n\n\nMar 19, 2024\n\n\n6 min\n\n\n\n\n\n\n\nML Fundamentals: Understanding the Basics\n\n\n\n\n\n\nmachine-learning\n\n\nfundamentals\n\n\ntheory\n\n\n\nA comprehensive introduction to machine learning fundamentals, core concepts, and essential terminology that every aspiring ML practitioner should know.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nMachine Learning in Everyday Life: A Practical Guide\n\n\n\n\n\n\nmachine-learning\n\n\napplications\n\n\nreal-world\n\n\nbeginner\n\n\n\nDiscover how machine learning is already part of your daily life and how it makes things easier, explained in simple terms with real-world examples.\n\n\n\n\n\nMar 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\nMachine Learning Theory: Mathematical Foundations\n\n\n\n\n\n\nmachine-learning\n\n\ntheory\n\n\nmathematics\n\n\nstatistics\n\n\n\nA rigorous exploration of machine learning theory, covering statistical learning theory, optimization theory, and fundamental bounds.\n\n\n\n\n\nMar 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\nOnline Learning and Regret Minimization\n\n\n\n\n\n\nmachine-learning\n\n\ntheory\n\n\nmathematics\n\n\nonline-learning\n\n\n\nA rigorous exploration of online learning theory and regret minimization, covering fundamental algorithms, bounds, and applications.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nOptimization Algorithms in Machine Learning: A Deep Dive\n\n\n\n\n\n\nmachine-learning\n\n\noptimization\n\n\nmathematics\n\n\nalgorithms\n\n\n\nA comprehensive technical exploration of optimization algorithms in machine learning, covering mathematical foundations and implementation details.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nOptimization Theory in Machine Learning\n\n\n\n\n\n\nmachine-learning\n\n\noptimization\n\n\nmathematics\n\n\ntheory\n\n\n\nA rigorous exploration of optimization theory in machine learning, covering convex optimization, non-convex optimization, and modern algorithms.\n\n\n\n\n\nMar 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\nPAC Learning Theory and VC Dimension\n\n\n\n\n\n\nmachine-learning\n\n\ntheory\n\n\nmathematics\n\n\nstatistics\n\n\n\nA rigorous exploration of PAC learning theory and VC dimension, covering fundamental bounds, sample complexity, and learnability.\n\n\n\n\n\nMar 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\nProbabilistic Graphical Models: Mathematical Foundations\n\n\n\n\n\n\nmachine-learning\n\n\nprobabilistic-models\n\n\nmathematics\n\n\nbayesian\n\n\n\nA rigorous exploration of probabilistic graphical models, covering mathematical foundations, inference algorithms, and learning methods.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\nUnderstanding Reinforcement Learning: A Beginner’s Guide\n\n\n\n\n\n\nmachine-learning\n\n\nreinforcement-learning\n\n\ntheory\n\n\nbeginner\n\n\n\nA beginner-friendly introduction to reinforcement learning concepts, explained through simple analogies and real-world examples.\n\n\n\n\n\nMar 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\nStatistical Learning Theory and Concentration Inequalities\n\n\n\n\n\n\nmachine-learning\n\n\ntheory\n\n\nmathematics\n\n\nstatistics\n\n\n\nA rigorous exploration of statistical learning theory and concentration inequalities, covering fundamental bounds and their applications in machine learning.\n\n\n\n\n\nMar 19, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html",
    "href": "posts/algorithmic-stability/index.html",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Hypothesis stability:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta_m\n\\]\nWhere: - \\(A_S\\) is algorithm output on dataset \\(S\\) - \\(S^i\\) is dataset with i-th example replaced - \\(\\beta_m\\) is stability coefficient\nUniform stability:\n\\[\n\\sup_{S,z,i}|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\n\n\n\nPoint-wise loss stability:\n\\[\n|\\ell(h_S,z) - \\ell(h_{S^i},z)| \\leq \\beta\n\\]\nAverage loss stability:\n\\[\n|\\mathbb{E}_{z \\sim \\mathcal{D}}[\\ell(h_S,z) - \\ell(h_{S^i},z)]| \\leq \\beta\n\\]\n\n\n\nMcDiarmid’s inequality based bound:\n\\[\nP(|R(A_S) - \\hat{R}_S(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{2m\\epsilon^2}{(4\\beta)^2})\n\\]\nExpected generalization error:\n\\[\n|\\mathbb{E}[R(A_S) - \\hat{R}_S(A_S)]| \\leq \\beta\n\\]\n\n\n\n\n\n\nDefinition:\n\\[\n\\sup_{S,S': |S \\triangle S'| = 2}\\|A_S - A_{S'}\\| \\leq \\beta_m\n\\]\nGeneralization bound:\n\\[\nP(|R(A_S) - \\hat{R}_S(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{m\\epsilon^2}{2\\beta_m^2})\n\\]\n\n\n\nLeave-one-out stability:\n\\[\n|\\mathbb{E}_{S,z}[\\ell(A_S,z) - \\ell(A_{S^{-i}},z)]| \\leq \\beta_m\n\\]\nk-fold stability:\n\\[\n|\\mathbb{E}_{S,z}[\\ell(A_S,z) - \\ell(A_{S_k},z)]| \\leq \\beta_m\n\\]\n\n\n\n\\((K,\\epsilon(\\cdot))\\)-robustness:\n\\[\nP_{S,z}(|\\ell(A_S,z) - \\ell(A_S,z')| &gt; \\epsilon(m)) \\leq K/m\n\\]\nWhere: - \\(z,z'\\) are in same partition - \\(K\\) is number of partitions - \\(\\epsilon(m)\\) is robustness parameter\n\n\n\n\n\n\nTikhonov regularization:\n\\[\nA_S = \\arg\\min_{h \\in \\mathcal{H}} \\frac{1}{m}\\sum_{i=1}^m \\ell(h,z_i) + \\lambda\\|h\\|^2\n\\]\nStability bound:\n\\[\n\\beta \\leq \\frac{L^2}{2m\\lambda}\n\\]\nWhere: - \\(L\\) is Lipschitz constant - \\(\\lambda\\) is regularization parameter\n\n\n\nGradient descent stability:\n\\[\n\\|w_t - w_t'\\| \\leq (1+\\eta L)^t\\|w_0 - w_0'\\|\n\\]\nSGD stability:\n\\[\n\\mathbb{E}[\\|w_t - w_t'\\|^2] \\leq \\frac{\\eta^2L^2}{2m}\n\\]\n\n\n\nBagging stability:\n\\[\n\\beta_{\\text{bag}} \\leq \\frac{\\beta}{\\sqrt{B}}\n\\]\nWhere: - \\(B\\) is number of bootstrap samples - \\(\\beta\\) is base learner stability\n\n\n\n\n\n\nRidge regression stability:\n\\[\n\\beta_{\\text{ridge}} \\leq \\frac{4M^2}{m\\lambda}\n\\]\nWhere: - \\(M\\) is bound on features - \\(\\lambda\\) is regularization\n\n\n\nOnline stability:\n\\[\n\\mathbb{E}[\\|w_t - w_t'\\|] \\leq \\frac{2G}{\\lambda\\sqrt{t}}\n\\]\nWhere: - \\(G\\) is gradient bound - \\(t\\) is iteration number\n\n\n\nDropout stability:\n\\[\n\\beta_{\\text{dropout}} \\leq \\frac{p(1-p)L^2}{m}\n\\]\nWhere: - \\(p\\) is dropout probability - \\(L\\) is network Lipschitz constant\n\n\n\n\n\n\nDefinition:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta(z)\n\\]\nAdaptive bound:\n\\[\nP(|R(A_S) - \\hat{R}_S(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{2m\\epsilon^2}{\\mathbb{E}[\\beta(Z)^2]})\n\\]\n\n\n\nDefinition:\n\\[\n\\|\\mathcal{D}_{A_S} - \\mathcal{D}_{A_{S^i}}\\|_1 \\leq \\beta\n\\]\nGeneralization:\n\\[\n|\\mathbb{E}[R(A_S)] - \\mathbb{E}[\\hat{R}_S(A_S)]| \\leq \\beta\n\\]\n\n\n\nDifferential privacy:\n\\[\nP(A_S \\in E) \\leq e^\\epsilon P(A_{S'} \\in E)\n\\]\nPrivacy-stability relationship:\n\\[\n\\beta \\leq \\epsilon L\n\\]\n\n\n\n\n\n\nRelationships:\n\\[\n\\text{Uniform} \\implies \\text{Hypothesis} \\implies \\text{Point-wise} \\implies \\text{Average}\n\\]\nEquivalence conditions:\n\\[\n\\beta_{\\text{uniform}} = \\beta_{\\text{hypothesis}} \\iff \\text{convex loss}\n\\]\n\n\n\nMinimal stability:\n\\[\n\\beta_m \\geq \\Omega(\\frac{1}{\\sqrt{m}})\n\\]\nOptimal rates:\n\\[\n\\beta_m = \\Theta(\\frac{1}{m})\n\\]\n\n\n\nSerial composition:\n\\[\n\\beta_{A \\circ B} \\leq \\beta_A + \\beta_B\n\\]\nParallel composition:\n\\[\n\\beta_{\\text{parallel}} \\leq \\max_i \\beta_i\n\\]\n\n\n\n\n\n\n\nRegularization:\n\nChoose appropriate \\(\\lambda\\)\nBalance stability-accuracy\nAdaptive regularization\n\nOptimization:\n\nStep size selection\nBatch size impact\nMomentum effects\n\nArchitecture:\n\nLayer stability\nSkip connections\nNormalization impact\n\n\n\n\n\n\nEmpirical Stability:\n\nLeave-one-out estimates\nBootstrap estimates\nCross-validation\n\nTheoretical Bounds:\n\nLipschitz constants\nCondition numbers\nSpectral norms\n\nMonitoring:\n\nStability metrics\nGeneralization gaps\nValidation curves\n\n\n\n\n\n\n\n\n\nStability Analysis:\n\nCross-validation stability\nParameter sensitivity\nModel robustness\n\nRegularization:\n\nMultiple techniques\nAdaptive schemes\nStability-based selection\n\nValidation:\n\nStability metrics\nGeneralization bounds\nRobustness checks\n\n\n\n\n\n\nOptimization:\n\nStable algorithms\nAdaptive methods\nEarly stopping\n\nData Processing:\n\nRobust preprocessing\nFeature stability\nOutlier handling\n\nEvaluation:\n\nStability measures\nConfidence bounds\nSensitivity analysis\n\n\n\n\n\n\n\nTheory:\n\n“Stability and Generalization” by Bousquet and Elisseeff\n“Learning, Testing, and the Stability Approach” by Shalev-Shwartz et al.\n“Stability and Learning Theory” by Mukherjee et al.\n\nMethods:\n\n“Algorithmic Stability and Uniform Convergence” by Kearns and Ron\n“Stability and Instance-Based Learning” by Devroye and Wagner\n“Stable Learning Algorithms” by Kutin and Niyogi\n\nApplications:\n\n“Stability in Machine Learning” by Hardt et al.\n“Deep Learning and Stability” by Hardt and Ma\n“Stability-Based Generalization Analysis” by Poggio et al."
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#fundamental-concepts",
    "href": "posts/algorithmic-stability/index.html#fundamental-concepts",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Hypothesis stability:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta_m\n\\]\nWhere: - \\(A_S\\) is algorithm output on dataset \\(S\\) - \\(S^i\\) is dataset with i-th example replaced - \\(\\beta_m\\) is stability coefficient\nUniform stability:\n\\[\n\\sup_{S,z,i}|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\n\n\n\nPoint-wise loss stability:\n\\[\n|\\ell(h_S,z) - \\ell(h_{S^i},z)| \\leq \\beta\n\\]\nAverage loss stability:\n\\[\n|\\mathbb{E}_{z \\sim \\mathcal{D}}[\\ell(h_S,z) - \\ell(h_{S^i},z)]| \\leq \\beta\n\\]\n\n\n\nMcDiarmid’s inequality based bound:\n\\[\nP(|R(A_S) - \\hat{R}_S(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{2m\\epsilon^2}{(4\\beta)^2})\n\\]\nExpected generalization error:\n\\[\n|\\mathbb{E}[R(A_S) - \\hat{R}_S(A_S)]| \\leq \\beta\n\\]"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#types-of-stability",
    "href": "posts/algorithmic-stability/index.html#types-of-stability",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Definition:\n\\[\n\\sup_{S,S': |S \\triangle S'| = 2}\\|A_S - A_{S'}\\| \\leq \\beta_m\n\\]\nGeneralization bound:\n\\[\nP(|R(A_S) - \\hat{R}_S(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{m\\epsilon^2}{2\\beta_m^2})\n\\]\n\n\n\nLeave-one-out stability:\n\\[\n|\\mathbb{E}_{S,z}[\\ell(A_S,z) - \\ell(A_{S^{-i}},z)]| \\leq \\beta_m\n\\]\nk-fold stability:\n\\[\n|\\mathbb{E}_{S,z}[\\ell(A_S,z) - \\ell(A_{S_k},z)]| \\leq \\beta_m\n\\]\n\n\n\n\\((K,\\epsilon(\\cdot))\\)-robustness:\n\\[\nP_{S,z}(|\\ell(A_S,z) - \\ell(A_S,z')| &gt; \\epsilon(m)) \\leq K/m\n\\]\nWhere: - \\(z,z'\\) are in same partition - \\(K\\) is number of partitions - \\(\\epsilon(m)\\) is robustness parameter"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#stability-analysis",
    "href": "posts/algorithmic-stability/index.html#stability-analysis",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Tikhonov regularization:\n\\[\nA_S = \\arg\\min_{h \\in \\mathcal{H}} \\frac{1}{m}\\sum_{i=1}^m \\ell(h,z_i) + \\lambda\\|h\\|^2\n\\]\nStability bound:\n\\[\n\\beta \\leq \\frac{L^2}{2m\\lambda}\n\\]\nWhere: - \\(L\\) is Lipschitz constant - \\(\\lambda\\) is regularization parameter\n\n\n\nGradient descent stability:\n\\[\n\\|w_t - w_t'\\| \\leq (1+\\eta L)^t\\|w_0 - w_0'\\|\n\\]\nSGD stability:\n\\[\n\\mathbb{E}[\\|w_t - w_t'\\|^2] \\leq \\frac{\\eta^2L^2}{2m}\n\\]\n\n\n\nBagging stability:\n\\[\n\\beta_{\\text{bag}} \\leq \\frac{\\beta}{\\sqrt{B}}\n\\]\nWhere: - \\(B\\) is number of bootstrap samples - \\(\\beta\\) is base learner stability"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#applications",
    "href": "posts/algorithmic-stability/index.html#applications",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Ridge regression stability:\n\\[\n\\beta_{\\text{ridge}} \\leq \\frac{4M^2}{m\\lambda}\n\\]\nWhere: - \\(M\\) is bound on features - \\(\\lambda\\) is regularization\n\n\n\nOnline stability:\n\\[\n\\mathbb{E}[\\|w_t - w_t'\\|] \\leq \\frac{2G}{\\lambda\\sqrt{t}}\n\\]\nWhere: - \\(G\\) is gradient bound - \\(t\\) is iteration number\n\n\n\nDropout stability:\n\\[\n\\beta_{\\text{dropout}} \\leq \\frac{p(1-p)L^2}{m}\n\\]\nWhere: - \\(p\\) is dropout probability - \\(L\\) is network Lipschitz constant"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#advanced-topics",
    "href": "posts/algorithmic-stability/index.html#advanced-topics",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Definition:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta(z)\n\\]\nAdaptive bound:\n\\[\nP(|R(A_S) - \\hat{R}_S(A_S)| &gt; \\epsilon) \\leq 2\\exp(-\\frac{2m\\epsilon^2}{\\mathbb{E}[\\beta(Z)^2]})\n\\]\n\n\n\nDefinition:\n\\[\n\\|\\mathcal{D}_{A_S} - \\mathcal{D}_{A_{S^i}}\\|_1 \\leq \\beta\n\\]\nGeneralization:\n\\[\n|\\mathbb{E}[R(A_S)] - \\mathbb{E}[\\hat{R}_S(A_S)]| \\leq \\beta\n\\]\n\n\n\nDifferential privacy:\n\\[\nP(A_S \\in E) \\leq e^\\epsilon P(A_{S'} \\in E)\n\\]\nPrivacy-stability relationship:\n\\[\n\\beta \\leq \\epsilon L\n\\]"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#theoretical-results",
    "href": "posts/algorithmic-stability/index.html#theoretical-results",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Relationships:\n\\[\n\\text{Uniform} \\implies \\text{Hypothesis} \\implies \\text{Point-wise} \\implies \\text{Average}\n\\]\nEquivalence conditions:\n\\[\n\\beta_{\\text{uniform}} = \\beta_{\\text{hypothesis}} \\iff \\text{convex loss}\n\\]\n\n\n\nMinimal stability:\n\\[\n\\beta_m \\geq \\Omega(\\frac{1}{\\sqrt{m}})\n\\]\nOptimal rates:\n\\[\n\\beta_m = \\Theta(\\frac{1}{m})\n\\]\n\n\n\nSerial composition:\n\\[\n\\beta_{A \\circ B} \\leq \\beta_A + \\beta_B\n\\]\nParallel composition:\n\\[\n\\beta_{\\text{parallel}} \\leq \\max_i \\beta_i\n\\]"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#implementation-considerations",
    "href": "posts/algorithmic-stability/index.html#implementation-considerations",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Regularization:\n\nChoose appropriate \\(\\lambda\\)\nBalance stability-accuracy\nAdaptive regularization\n\nOptimization:\n\nStep size selection\nBatch size impact\nMomentum effects\n\nArchitecture:\n\nLayer stability\nSkip connections\nNormalization impact\n\n\n\n\n\n\nEmpirical Stability:\n\nLeave-one-out estimates\nBootstrap estimates\nCross-validation\n\nTheoretical Bounds:\n\nLipschitz constants\nCondition numbers\nSpectral norms\n\nMonitoring:\n\nStability metrics\nGeneralization gaps\nValidation curves"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#best-practices",
    "href": "posts/algorithmic-stability/index.html#best-practices",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Stability Analysis:\n\nCross-validation stability\nParameter sensitivity\nModel robustness\n\nRegularization:\n\nMultiple techniques\nAdaptive schemes\nStability-based selection\n\nValidation:\n\nStability metrics\nGeneralization bounds\nRobustness checks\n\n\n\n\n\n\nOptimization:\n\nStable algorithms\nAdaptive methods\nEarly stopping\n\nData Processing:\n\nRobust preprocessing\nFeature stability\nOutlier handling\n\nEvaluation:\n\nStability measures\nConfidence bounds\nSensitivity analysis"
  },
  {
    "objectID": "posts/algorithmic-stability/index.html#references",
    "href": "posts/algorithmic-stability/index.html#references",
    "title": "Algorithmic Stability and Learning Theory",
    "section": "",
    "text": "Theory:\n\n“Stability and Generalization” by Bousquet and Elisseeff\n“Learning, Testing, and the Stability Approach” by Shalev-Shwartz et al.\n“Stability and Learning Theory” by Mukherjee et al.\n\nMethods:\n\n“Algorithmic Stability and Uniform Convergence” by Kearns and Ron\n“Stability and Instance-Based Learning” by Devroye and Wagner\n“Stable Learning Algorithms” by Kutin and Niyogi\n\nApplications:\n\n“Stability in Machine Learning” by Hardt et al.\n“Deep Learning and Stability” by Hardt and Ma\n“Stability-Based Generalization Analysis” by Poggio et al."
  },
  {
    "objectID": "posts/causal-inference/index.html",
    "href": "posts/causal-inference/index.html",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Definition of SCM:\n\\[\n\\begin{aligned}\nX_i &= f_i(\\text{PA}_i, U_i) \\\\\nU_i &\\sim P(U_i)\n\\end{aligned}\n\\]\nWhere: - \\(X_i\\) are endogenous variables - \\(\\text{PA}_i\\) are parents of \\(X_i\\) - \\(U_i\\) are exogenous variables - \\(f_i\\) are structural equations\n\n\n\nDo-operator formalization:\n\\[\nP(Y|\\text{do}(X=x)) = \\sum_z P(Y|X=x,Z=z)P(Z=z)\n\\]\nBackdoor adjustment:\n\\[\nP(Y|\\text{do}(X=x)) = \\sum_z P(Y|X=x,Z=z)P(Z=z)\n\\]\nWhere Z satisfies the backdoor criterion.\n\n\n\n\n\n\nA set Z satisfies the backdoor criterion relative to (X,Y) if: 1. No node in Z is a descendant of X 2. Z blocks all backdoor paths from X to Y\nFormal criterion:\n\\[\nP(Y|\\text{do}(X)) = \\sum_z P(Y|X,Z)P(Z)\n\\]\n\n\n\nThree conditions: 1. M blocks all directed paths from X to Y 2. No unblocked backdoor path from X to M 3. All backdoor paths from M to Y are blocked by X\nFormula:\n\\[\nP(Y|\\text{do}(X)) = \\sum_m P(m|X)\\sum_{x'}P(Y|m,x')P(x')\n\\]\n\n\n\nRule 1 (Insertion/deletion of observations):\n\\[\nP(y|\\text{do}(x),z,w) = P(y|\\text{do}(x),w)\n\\]\nif (Y ⊥⊥ Z|X,W)\\(_{G_{\\overline{X}}}\\)\nRule 2 (Action/observation exchange):\n\\[\nP(y|\\text{do}(x),\\text{do}(z),w) = P(y|\\text{do}(x),z,w)\n\\]\nif (Y ⊥⊥ Z|X,W)\\(_{G_{\\overline{X}\\underline{Z}}}\\)\nRule 3 (Insertion/deletion of actions):\n\\[\nP(y|\\text{do}(x),\\text{do}(z),w) = P(y|\\text{do}(x),w)\n\\]\nif (Y ⊥⊥ Z|X,W)\\(_{G_{\\overline{X}\\overline{Z(W)}}}\\)\n\n\n\n\n\n\nPropensity score:\n\\[\ne(X) = P(T=1|X)\n\\]\nAverage Treatment Effect (ATE):\n\\[\n\\text{ATE} = \\mathbb{E}[Y(1) - Y(0)] = \\mathbb{E}\\left[\\frac{TY}{e(X)} - \\frac{(1-T)Y}{1-e(X)}\\right]\n\\]\n\n\n\nTwo-stage least squares (2SLS):\nFirst stage: \\[\nX = \\gamma_0 + \\gamma_1Z + \\eta\n\\]\nSecond stage: \\[\nY = \\beta_0 + \\beta_1\\hat{X} + \\epsilon\n\\]\n\n\n\nSharp RD estimator:\n\\[\n\\tau_{SRD} = \\lim_{x \\downarrow c} \\mathbb{E}[Y|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y|X=x]\n\\]\nFuzzy RD estimator:\n\\[\n\\tau_{FRD} = \\frac{\\lim_{x \\downarrow c} \\mathbb{E}[Y|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y|X=x]}{\\lim_{x \\downarrow c} \\mathbb{E}[D|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[D|X=x]}\n\\]\n\n\n\n\n\n\nPC Algorithm steps: 1. Start with complete undirected graph 2. Remove edges based on conditional independence 3. Orient v-structures 4. Orient remaining edges\nIndependence test statistic:\n\\[\n\\chi^2 = n\\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\n\n\n\nBIC score:\n\\[\n\\text{BIC}(G) = \\ell(D|G) - \\frac{\\log n}{2}|G|\n\\]\nWhere: - \\(\\ell(D|G)\\) is log-likelihood - \\(|G|\\) is model complexity - \\(n\\) is sample size\n\n\n\nMMHC algorithm: 1. Learn skeleton using constraint-based method 2. Orient edges using score-based method\nScore function:\n\\[\n\\text{Score}(G) = \\text{BIC}(G) + \\lambda \\text{Sparsity}(G)\n\\]\n\n\n\n\n\n\nFundamental problem of causal inference:\n\\[\n\\text{ACE} = \\mathbb{E}[Y(1) - Y(0)]\n\\]\nBut we only observe:\n\\[\nY = TY(1) + (1-T)Y(0)\n\\]\n\n\n\nDirect and indirect effects:\n\\[\n\\begin{aligned}\n\\text{NDE} &= \\mathbb{E}[Y(t,M(t'))] - \\mathbb{E}[Y(t',M(t'))] \\\\\n\\text{NIE} &= \\mathbb{E}[Y(t,M(t))] - \\mathbb{E}[Y(t,M(t'))]\n\\end{aligned}\n\\]\n\n\n\nG-computation formula:\n\\[\n\\mathbb{E}[Y_{\\bar{a}}] = \\sum_{\\bar{l}} \\prod_{t=0}^K P(l_t|l_{t-1},a_{t-1})P(y|\\bar{l},\\bar{a})\n\\]\n\n\n\n\n\n\nRosenbaum bounds:\n\\[\n\\frac{1}{\\Gamma} \\leq \\frac{P(Z=1|X)P(Z=0|X')}{P(Z=0|X)P(Z=1|X')} \\leq \\Gamma\n\\]\n\n\n\nMultiple imputation:\n\\[\n\\hat{\\theta} = \\frac{1}{M}\\sum_{m=1}^M \\hat{\\theta}_m\n\\]\n\n\n\nConditional average treatment effect:\n\\[\n\\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0)|X=x]\n\\]\n\n\n\n\n\n\n\nRandomization:\n\nComplete randomization\nStratified randomization\nCluster randomization\n\nSample Size:\n\nPower analysis\nEffect size estimation\nVariance components\n\nMeasurement:\n\nReliability\nValidity\nMissing data handling\n\n\n\n\n\n\nIdentification:\n\nCheck assumptions\nSensitivity analysis\nMultiple methods\n\nEstimation:\n\nRobust methods\nBootstrap\nCross-validation\n\nInterpretation:\n\nEffect sizes\nConfidence intervals\nMultiple testing\n\n\n\n\n\n\n\n\nProgram evaluation: - Treatment effects - Policy analysis - Market interventions\n\n\n\nClinical trials: - Drug efficacy - Treatment comparison - Side effects\n\n\n\nPolicy research: - Educational interventions - Social programs - Behavioral studies\n\n\n\n\n\nTheory:\n\n“Causality” by Pearl\n“Causal Inference in Statistics” by Pearl et al.\n“Elements of Causal Inference” by Peters et al.\n\nMethods:\n\n“Mostly Harmless Econometrics” by Angrist and Pischke\n“Counterfactuals and Causal Inference” by Morgan and Winship\n“Causal Inference for Statistics” by Hernán and Robins\n\nApplications:\n\n“Causal Machine Learning” by Athey and Imbens\n“The Book of Why” by Pearl and Mackenzie\n“Observation and Experiment” by Rosenbaum"
  },
  {
    "objectID": "posts/causal-inference/index.html#structural-causal-models",
    "href": "posts/causal-inference/index.html#structural-causal-models",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Definition of SCM:\n\\[\n\\begin{aligned}\nX_i &= f_i(\\text{PA}_i, U_i) \\\\\nU_i &\\sim P(U_i)\n\\end{aligned}\n\\]\nWhere: - \\(X_i\\) are endogenous variables - \\(\\text{PA}_i\\) are parents of \\(X_i\\) - \\(U_i\\) are exogenous variables - \\(f_i\\) are structural equations\n\n\n\nDo-operator formalization:\n\\[\nP(Y|\\text{do}(X=x)) = \\sum_z P(Y|X=x,Z=z)P(Z=z)\n\\]\nBackdoor adjustment:\n\\[\nP(Y|\\text{do}(X=x)) = \\sum_z P(Y|X=x,Z=z)P(Z=z)\n\\]\nWhere Z satisfies the backdoor criterion."
  },
  {
    "objectID": "posts/causal-inference/index.html#identification-methods",
    "href": "posts/causal-inference/index.html#identification-methods",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "A set Z satisfies the backdoor criterion relative to (X,Y) if: 1. No node in Z is a descendant of X 2. Z blocks all backdoor paths from X to Y\nFormal criterion:\n\\[\nP(Y|\\text{do}(X)) = \\sum_z P(Y|X,Z)P(Z)\n\\]\n\n\n\nThree conditions: 1. M blocks all directed paths from X to Y 2. No unblocked backdoor path from X to M 3. All backdoor paths from M to Y are blocked by X\nFormula:\n\\[\nP(Y|\\text{do}(X)) = \\sum_m P(m|X)\\sum_{x'}P(Y|m,x')P(x')\n\\]\n\n\n\nRule 1 (Insertion/deletion of observations):\n\\[\nP(y|\\text{do}(x),z,w) = P(y|\\text{do}(x),w)\n\\]\nif (Y ⊥⊥ Z|X,W)\\(_{G_{\\overline{X}}}\\)\nRule 2 (Action/observation exchange):\n\\[\nP(y|\\text{do}(x),\\text{do}(z),w) = P(y|\\text{do}(x),z,w)\n\\]\nif (Y ⊥⊥ Z|X,W)\\(_{G_{\\overline{X}\\underline{Z}}}\\)\nRule 3 (Insertion/deletion of actions):\n\\[\nP(y|\\text{do}(x),\\text{do}(z),w) = P(y|\\text{do}(x),w)\n\\]\nif (Y ⊥⊥ Z|X,W)\\(_{G_{\\overline{X}\\overline{Z(W)}}}\\)"
  },
  {
    "objectID": "posts/causal-inference/index.html#estimation-methods",
    "href": "posts/causal-inference/index.html#estimation-methods",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Propensity score:\n\\[\ne(X) = P(T=1|X)\n\\]\nAverage Treatment Effect (ATE):\n\\[\n\\text{ATE} = \\mathbb{E}[Y(1) - Y(0)] = \\mathbb{E}\\left[\\frac{TY}{e(X)} - \\frac{(1-T)Y}{1-e(X)}\\right]\n\\]\n\n\n\nTwo-stage least squares (2SLS):\nFirst stage: \\[\nX = \\gamma_0 + \\gamma_1Z + \\eta\n\\]\nSecond stage: \\[\nY = \\beta_0 + \\beta_1\\hat{X} + \\epsilon\n\\]\n\n\n\nSharp RD estimator:\n\\[\n\\tau_{SRD} = \\lim_{x \\downarrow c} \\mathbb{E}[Y|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y|X=x]\n\\]\nFuzzy RD estimator:\n\\[\n\\tau_{FRD} = \\frac{\\lim_{x \\downarrow c} \\mathbb{E}[Y|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y|X=x]}{\\lim_{x \\downarrow c} \\mathbb{E}[D|X=x] - \\lim_{x \\uparrow c} \\mathbb{E}[D|X=x]}\n\\]"
  },
  {
    "objectID": "posts/causal-inference/index.html#structural-learning",
    "href": "posts/causal-inference/index.html#structural-learning",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "PC Algorithm steps: 1. Start with complete undirected graph 2. Remove edges based on conditional independence 3. Orient v-structures 4. Orient remaining edges\nIndependence test statistic:\n\\[\n\\chi^2 = n\\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\n\n\n\nBIC score:\n\\[\n\\text{BIC}(G) = \\ell(D|G) - \\frac{\\log n}{2}|G|\n\\]\nWhere: - \\(\\ell(D|G)\\) is log-likelihood - \\(|G|\\) is model complexity - \\(n\\) is sample size\n\n\n\nMMHC algorithm: 1. Learn skeleton using constraint-based method 2. Orient edges using score-based method\nScore function:\n\\[\n\\text{Score}(G) = \\text{BIC}(G) + \\lambda \\text{Sparsity}(G)\n\\]"
  },
  {
    "objectID": "posts/causal-inference/index.html#advanced-topics",
    "href": "posts/causal-inference/index.html#advanced-topics",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Fundamental problem of causal inference:\n\\[\n\\text{ACE} = \\mathbb{E}[Y(1) - Y(0)]\n\\]\nBut we only observe:\n\\[\nY = TY(1) + (1-T)Y(0)\n\\]\n\n\n\nDirect and indirect effects:\n\\[\n\\begin{aligned}\n\\text{NDE} &= \\mathbb{E}[Y(t,M(t'))] - \\mathbb{E}[Y(t',M(t'))] \\\\\n\\text{NIE} &= \\mathbb{E}[Y(t,M(t))] - \\mathbb{E}[Y(t,M(t'))]\n\\end{aligned}\n\\]\n\n\n\nG-computation formula:\n\\[\n\\mathbb{E}[Y_{\\bar{a}}] = \\sum_{\\bar{l}} \\prod_{t=0}^K P(l_t|l_{t-1},a_{t-1})P(y|\\bar{l},\\bar{a})\n\\]"
  },
  {
    "objectID": "posts/causal-inference/index.html#implementation-considerations",
    "href": "posts/causal-inference/index.html#implementation-considerations",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Rosenbaum bounds:\n\\[\n\\frac{1}{\\Gamma} \\leq \\frac{P(Z=1|X)P(Z=0|X')}{P(Z=0|X)P(Z=1|X')} \\leq \\Gamma\n\\]\n\n\n\nMultiple imputation:\n\\[\n\\hat{\\theta} = \\frac{1}{M}\\sum_{m=1}^M \\hat{\\theta}_m\n\\]\n\n\n\nConditional average treatment effect:\n\\[\n\\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0)|X=x]\n\\]"
  },
  {
    "objectID": "posts/causal-inference/index.html#best-practices",
    "href": "posts/causal-inference/index.html#best-practices",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Randomization:\n\nComplete randomization\nStratified randomization\nCluster randomization\n\nSample Size:\n\nPower analysis\nEffect size estimation\nVariance components\n\nMeasurement:\n\nReliability\nValidity\nMissing data handling\n\n\n\n\n\n\nIdentification:\n\nCheck assumptions\nSensitivity analysis\nMultiple methods\n\nEstimation:\n\nRobust methods\nBootstrap\nCross-validation\n\nInterpretation:\n\nEffect sizes\nConfidence intervals\nMultiple testing"
  },
  {
    "objectID": "posts/causal-inference/index.html#applications",
    "href": "posts/causal-inference/index.html#applications",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Program evaluation: - Treatment effects - Policy analysis - Market interventions\n\n\n\nClinical trials: - Drug efficacy - Treatment comparison - Side effects\n\n\n\nPolicy research: - Educational interventions - Social programs - Behavioral studies"
  },
  {
    "objectID": "posts/causal-inference/index.html#references",
    "href": "posts/causal-inference/index.html#references",
    "title": "Causal Inference and Structural Learning",
    "section": "",
    "text": "Theory:\n\n“Causality” by Pearl\n“Causal Inference in Statistics” by Pearl et al.\n“Elements of Causal Inference” by Peters et al.\n\nMethods:\n\n“Mostly Harmless Econometrics” by Angrist and Pischke\n“Counterfactuals and Causal Inference” by Morgan and Winship\n“Causal Inference for Statistics” by Hernán and Robins\n\nApplications:\n\n“Causal Machine Learning” by Athey and Imbens\n“The Book of Why” by Pearl and Mackenzie\n“Observation and Experiment” by Rosenbaum"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html",
    "href": "posts/deep-learning-beginners/index.html",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "",
    "text": "The first time I encountered deep learning, I was amazed by its ability to solve problems that seemed impossible just a few years ago. From beating world champions at complex games to generating art that could pass for human-made, deep learning has transformed the landscape of artificial intelligence. But what makes this technology so powerful, and how does it actually work?"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#the-brain-inspired-technology",
    "href": "posts/deep-learning-beginners/index.html#the-brain-inspired-technology",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "The Brain-Inspired Technology",
    "text": "The Brain-Inspired Technology\nImagine trying to teach a ### The Brain Connection Think of it like this: - Your brain has billions of neurons - Each neuron connects to thousands of others - They work together to process information - Deep learning mimics this structure"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#neural-networks-explained",
    "href": "posts/deep-learning-beginners/index.html#neural-networks-explained",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Neural Networks Explained",
    "text": "Neural Networks Explained\n\n1. The Basic Building Block: Neurons\nImagine a neuron as a simple calculator that: - Takes in multiple numbers (inputs) - Multiplies each by a weight (importance) - Adds them all up - Decides whether to “fire” based on the sum\nExample: - Inputs: [0.2, 0.7, 0.1] - Weights: [0.8, 0.3, 0.5] - Sum: (0.2 × 0.8) + (0.7 × 0.3) + (0.1 × 0.5) = 0.41 - Then decides whether to activate based on this sum\n\n\n2. Layers of Neurons\nThink of layers like assembly lines: 1. Input Layer - Receives raw data - Like our senses receiving information\n\nHidden Layers\n\nProcess information\nFind patterns\nTransform data\n\nOutput Layer\n\nMakes final decisions\nProvides predictions"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#how-neural-networks-learn",
    "href": "posts/deep-learning-beginners/index.html#how-neural-networks-learn",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "How Neural Networks Learn",
    "text": "How Neural Networks Learn\n\n1. The Learning Process\nLike learning to ride a bike: 1. Try something 2. See how well it works 3. Adjust based on mistakes 4. Try again 5. Get better over time\n\n\n2. Training Steps\n\nForward Pass:\n\nData flows through the network\nNetwork makes a prediction\nLike making a guess\n\nError Calculation:\n\nCompare prediction with truth\nCalculate how wrong it was\nLike measuring mistakes\n\nBackward Pass:\n\nAdjust weights based on errors\nLike learning from mistakes\nSmall improvements each time"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#types-of-neural-networks",
    "href": "posts/deep-learning-beginners/index.html#types-of-neural-networks",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Types of Neural Networks",
    "text": "Types of Neural Networks\n\n1. Feedforward Networks\nThe simplest type: - Information flows one way - Good for basic patterns - Like classifying images - Example: Identifying numbers\n\n\n2. Convolutional Networks (CNNs)\nSpecialized for images: - Look at small parts at a time - Combine information - Find patterns in images - Example: Face recognition\n\n\n3. Recurrent Networks (RNNs)\nGood for sequences: - Remember previous information - Process data over time - Good for text and speech - Example: Translation"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#common-applications",
    "href": "posts/deep-learning-beginners/index.html#common-applications",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Common Applications",
    "text": "Common Applications\n\n1. Computer Vision\nWhat it can do: - Recognize objects - Detect faces - Read text - Find patterns in images\nReal Examples: - Face ID on phones - Medical image analysis - Self-driving cars - Security cameras\n\n\n2. Natural Language\nUnderstanding text: - Translation - Summarization - Question answering - Text generation\nReal Examples: - Google Translate - Chatbots - Voice assistants - Email filters\n\n\n3. Speech Processing\nWorking with audio: - Speech recognition - Voice synthesis - Language translation - Music generation\nReal Examples: - Siri/Alexa - Transcription services - Voice assistants - Music recommendations"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#how-deep-learning-works",
    "href": "posts/deep-learning-beginners/index.html#how-deep-learning-works",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "How Deep Learning Works",
    "text": "How Deep Learning Works\n\n1. Feature Learning\nAutomatic pattern finding: - Low-level features (edges, colors) - Mid-level features (shapes, textures) - High-level features (objects, concepts)\nExample in Vision: 1. First layer sees edges 2. Next layer combines edges into shapes 3. Final layers recognize objects\n\n\n2. Representation Learning\nBuilding understanding: - Converts raw data to useful form - Learns important characteristics - Creates meaningful representations\nExample in Text: 1. Words to numbers 2. Understanding context 3. Capturing meaning\n\n\n3. Deep Learning vs Traditional ML\nKey differences: - Automatic feature extraction - Multiple layers of processing - Better with large datasets - More complex patterns"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#important-concepts",
    "href": "posts/deep-learning-beginners/index.html#important-concepts",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Important Concepts",
    "text": "Important Concepts\n\n1. Training Data\nWhat’s needed: - Large amounts of data - Good quality examples - Diverse cases - Clear labels (for supervised learning)\n\n\n2. Computing Power\nRequirements: - Powerful processors (GPUs) - Lots of memory - Long training times - Efficient algorithms\n\n\n3. Model Architecture\nDesign choices: - Number of layers - Types of layers - Connection patterns - Activation functions"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#common-challenges",
    "href": "posts/deep-learning-beginners/index.html#common-challenges",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Common Challenges",
    "text": "Common Challenges\n\n1. Data Issues\nCommon problems: - Not enough data - Poor quality data - Biased data - Inconsistent labels\n\n\n2. Training Problems\nTypical issues: - Long training times - Unstable training - Overfitting - Resource limitations\n\n\n3. Deployment Challenges\nReal-world issues: - Model size - Computation needs - Integration - Maintenance"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#best-practices",
    "href": "posts/deep-learning-beginners/index.html#best-practices",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Start Simple\nBasic approach: - Use proven architectures - Start with small models - Understand the basics - Build complexity gradually\n\n\n2. Data Preparation\nKey steps: - Clean your data - Normalize inputs - Handle missing values - Balance datasets\n\n\n3. Model Development\nGood habits: - Start with baselines - Experiment systematically - Document everything - Test thoroughly"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#getting-started",
    "href": "posts/deep-learning-beginners/index.html#getting-started",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Getting Started",
    "text": "Getting Started\n\n1. Prerequisites\nWhat you need: - Python programming - Basic math - Machine learning basics - Development tools\n\n\n2. Learning Path\nSteps to follow: 1. Learn Python 2. Study ML basics 3. Understand neural networks 4. Practice with frameworks\n\n\n3. Tools and Frameworks\nPopular options: - PyTorch - TensorFlow - Keras - Fast.ai"
  },
  {
    "objectID": "posts/deep-learning-beginners/index.html#resources",
    "href": "posts/deep-learning-beginners/index.html#resources",
    "title": "Deep Learning: The Technology Behind AI’s Recent Breakthroughs",
    "section": "Resources",
    "text": "Resources\n\n1. Online Courses\nBest options: - Fast.ai - Coursera Deep Learning - Stanford CS231n - Deep Learning.AI\n\n\n2. Books\nRecommended reading: - “Deep Learning with Python” - “Grokking Deep Learning” - “Deep Learning” by Goodfellow - “Neural Networks from Scratch”\n\n\n3. Practice Resources\nWhere to practice: - Kaggle competitions - Google Colab - GitHub projects - Online tutorials\nRemember: Deep learning is powerful but requires patience to learn. Start with simple concepts, practice regularly, and gradually tackle more complex topics. Focus on understanding rather than memorizing, and always experiment with code to reinforce your learning."
  },
  {
    "objectID": "posts/information-theory-ml/index.html",
    "href": "posts/information-theory-ml/index.html",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Self-information of an event:\n\\[\nI(x) = -\\log_2 p(x)\n\\]\nEntropy of a discrete distribution:\n\\[\nH(X) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log_2 p(x)\n\\]\nProperties: 1. Non-negativity: \\(H(X) \\geq 0\\) 2. Maximum entropy: \\(H(X) \\leq \\log_2|\\mathcal{X}|\\) 3. Chain rule: \\(H(X,Y) = H(X) + H(Y|X)\\)\n\n\n\nFor continuous distributions:\n\\[\nh(X) = -\\int_{\\mathcal{X}} p(x)\\log p(x)dx\n\\]\nGaussian distribution entropy:\n\\[\nh(\\mathcal{N}(\\mu,\\sigma^2)) = \\frac{1}{2}\\log_2(2\\pi e\\sigma^2)\n\\]\n\n\n\nDefinition:\n\\[\nI(X;Y) = \\sum_{x,y} p(x,y)\\log_2\\frac{p(x,y)}{p(x)p(y)}\n\\]\nAlternative forms:\n\\[\n\\begin{aligned}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(Y) - H(Y|X) \\\\\n&= H(X) + H(Y) - H(X,Y)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nDefinition:\n\\[\nD_{KL}(P\\|Q) = \\sum_{x} P(x)\\log_2\\frac{P(x)}{Q(x)}\n\\]\nProperties: 1. Non-negativity: \\(D_{KL}(P\\|Q) \\geq 0\\) 2. \\(D_{KL}(P\\|Q) = 0\\) iff P = Q 3. Asymmetry: \\(D_{KL}(P\\|Q) \\neq D_{KL}(Q\\|P)\\)\n\n\n\nSymmetric measure:\n\\[\nJSD(P\\|Q) = \\frac{1}{2}D_{KL}(P\\|M) + \\frac{1}{2}D_{KL}(Q\\|M)\n\\]\nWhere: - \\(M = \\frac{1}{2}(P + Q)\\)\nProperties: 1. Symmetry: \\(JSD(P\\|Q) = JSD(Q\\|P)\\) 2. Bounded: \\(0 \\leq JSD(P\\|Q) \\leq 1\\) 3. Square root is a metric\n\n\n\nDefinition:\n\\[\nH(P,Q) = -\\sum_{x} P(x)\\log Q(x)\n\\]\nRelation to KL divergence:\n\\[\nH(P,Q) = H(P) + D_{KL}(P\\|Q)\n\\]\n\n\n\n\n\n\nObjective function:\n\\[\n\\max_{p} H(p) \\text{ subject to } \\mathbb{E}_p[f_i] = \\mu_i\n\\]\nSolution form:\n\\[\np^*(x) = \\frac{1}{Z(\\lambda)}\\exp\\left(\\sum_i \\lambda_i f_i(x)\\right)\n\\]\n\n\n\nObjective:\n\\[\n\\min_{p(t|x)} I(X;T) - \\beta I(T;Y)\n\\]\nSolution characterization:\n\\[\np(t|x) = \\frac{p(t)}{Z(x,\\beta)}\\exp\\left(-\\beta D_{KL}(p(y|x)\\|p(y|t))\\right)\n\\]\n\n\n\nMINE estimator:\n\\[\nI_\\theta(X,Y) = \\sup_{\\theta \\in \\Theta} \\mathbb{E}_{P_{XY}}[T_\\theta] - \\log\\mathbb{E}_{P_X \\otimes P_Y}[e^{T_\\theta}]\n\\]\n\n\n\n\n\n\nLayer-wise information:\n\\[\n\\begin{aligned}\nI(X;T_l) &= \\text{Information about input} \\\\\nI(T_l;Y) &= \\text{Information about output}\n\\end{aligned}\n\\]\n\n\n\nLower bound:\n\\[\nI(X;Y) \\geq \\mathbb{E}_{p(x,y)}[\\log q_\\theta(y|x)] + h(Y)\n\\]\n\n\n\nDropout probability:\n\\[\np(z|x) = \\mathcal{N}(z|\\mu(x), \\alpha(x)\\sigma^2)\n\\]\nCost function:\n\\[\n\\mathcal{L} = \\mathbb{E}[\\log p(y|z)] - \\beta D_{KL}(p(z|x)\\|r(z))\n\\]\n\n\n\n\n\n\nVAE ELBO:\n\\[\n\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)\\|p(z))\n\\]\nGAN objective:\n\\[\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\n\\]\n\n\n\nInfoNCE loss:\n\\[\n\\mathcal{L}_N = -\\mathbb{E}\\left[\\log \\frac{e^{f(x,y)/\\tau}}{\\sum_{i=1}^N e^{f(x,y_i)/\\tau}}\\right]\n\\]\nContrastive predictive coding:\n\\[\n\\mathcal{L}_{CPC} = -\\sum_{k=1}^K \\mathbb{E}\\left[\\log \\frac{e^{f_k(c_t,x_{t+k})}}{e^{f_k(c_t,x_{t+k})} + \\sum_{j=1}^N e^{f_k(c_t,x_j)}}\\right]\n\\]\n\n\n\nInformation bottleneck objective:\n\\[\n\\min_{p(t|x)} I(X;T) \\text{ subject to } I(T;Y) \\geq I_0\n\\]\nRate-distortion theory:\n\\[\nR(D) = \\min_{p(t|x): \\mathbb{E}[d(X,T)] \\leq D} I(X;T)\n\\]\n\n\n\n\n\n\nInformation dynamics:\n\\[\n\\frac{d}{dt}I(X;T_t) = \\mathbb{E}\\left[\\text{tr}\\left(\\frac{\\partial^2 I}{\\partial T^2}\\frac{dT}{dt}\\frac{dT}{dt}^T\\right)\\right]\n\\]\n\n\n\nPAC-Bayes bound:\n\\[\n\\text{gen-error} \\leq \\frac{D_{KL}(Q\\|P) + \\log\\frac{2\\sqrt{n}}{\\delta}}{2n}\n\\]\n\n\n\nNatural gradient:\n\\[\n\\dot{\\theta} = F^{-1}(\\theta)\\nabla_\\theta \\mathcal{L}\n\\]\nWhere: - \\(F(\\theta)\\) is the Fisher information matrix\n\n\n\n\n\n\nLog-sum-exp trick:\n\\[\n\\log\\sum_i e^{x_i} = a + \\log\\sum_i e^{x_i-a}\n\\]\nWhere: - \\(a = \\max_i x_i\\)\n\n\n\nKDE entropy estimator:\n\\[\n\\hat{H}(X) = -\\frac{1}{n}\\sum_{i=1}^n \\log\\frac{1}{n}\\sum_{j=1}^n K_h(x_i-x_j)\n\\]\n\n\n\nMini-batch mutual information:\n\\[\n\\hat{I}(X;Y) = \\frac{1}{B}\\sum_{i=1}^B \\log\\frac{p(x_i,y_i)}{\\hat{p}(x_i)\\hat{p}(y_i)}\n\\]\n\n\n\n\n\n\n\nInformation Flow:\n\nMonitor layer-wise information\nBalance compression and preservation\nUse information bottlenecks\n\nArchitecture Choice:\n\nConsider mutual information\nInformation capacity\nBottleneck dimensions\n\nRegularization:\n\nInformation dropout\nMutual information constraints\nEntropy regularization\n\n\n\n\n\n\nOptimization:\n\nNatural gradients\nInformation-based learning rates\nAdaptive methods\n\nMonitoring:\n\nInformation plane dynamics\nCompression metrics\nMutual information estimates\n\nValidation:\n\nCross-entropy\nKL divergence\nInformation efficiency\n\n\n\n\n\n\n\nTheory:\n\n“Elements of Information Theory” by Cover and Thomas\n“Information Theory, Inference, and Learning Algorithms” by MacKay\n“Deep Learning and the Information Bottleneck Principle” by Tishby and Zaslavsky\n\nMethods:\n\n“Deep Learning with Information Theoretic Learning” by Principe\n“Information Theory and Statistics” by Csiszár and Shields\n“Information Theory and Machine Learning” by Amari\n\nApplications:\n\n“Deep Learning” by Goodfellow et al.\n“Pattern Recognition and Machine Learning” by Bishop\n“Machine Learning: A Probabilistic Perspective” by Murphy"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#fundamental-concepts",
    "href": "posts/information-theory-ml/index.html#fundamental-concepts",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Self-information of an event:\n\\[\nI(x) = -\\log_2 p(x)\n\\]\nEntropy of a discrete distribution:\n\\[\nH(X) = -\\sum_{x \\in \\mathcal{X}} p(x)\\log_2 p(x)\n\\]\nProperties: 1. Non-negativity: \\(H(X) \\geq 0\\) 2. Maximum entropy: \\(H(X) \\leq \\log_2|\\mathcal{X}|\\) 3. Chain rule: \\(H(X,Y) = H(X) + H(Y|X)\\)\n\n\n\nFor continuous distributions:\n\\[\nh(X) = -\\int_{\\mathcal{X}} p(x)\\log p(x)dx\n\\]\nGaussian distribution entropy:\n\\[\nh(\\mathcal{N}(\\mu,\\sigma^2)) = \\frac{1}{2}\\log_2(2\\pi e\\sigma^2)\n\\]\n\n\n\nDefinition:\n\\[\nI(X;Y) = \\sum_{x,y} p(x,y)\\log_2\\frac{p(x,y)}{p(x)p(y)}\n\\]\nAlternative forms:\n\\[\n\\begin{aligned}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(Y) - H(Y|X) \\\\\n&= H(X) + H(Y) - H(X,Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#information-theoretic-measures",
    "href": "posts/information-theory-ml/index.html#information-theoretic-measures",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Definition:\n\\[\nD_{KL}(P\\|Q) = \\sum_{x} P(x)\\log_2\\frac{P(x)}{Q(x)}\n\\]\nProperties: 1. Non-negativity: \\(D_{KL}(P\\|Q) \\geq 0\\) 2. \\(D_{KL}(P\\|Q) = 0\\) iff P = Q 3. Asymmetry: \\(D_{KL}(P\\|Q) \\neq D_{KL}(Q\\|P)\\)\n\n\n\nSymmetric measure:\n\\[\nJSD(P\\|Q) = \\frac{1}{2}D_{KL}(P\\|M) + \\frac{1}{2}D_{KL}(Q\\|M)\n\\]\nWhere: - \\(M = \\frac{1}{2}(P + Q)\\)\nProperties: 1. Symmetry: \\(JSD(P\\|Q) = JSD(Q\\|P)\\) 2. Bounded: \\(0 \\leq JSD(P\\|Q) \\leq 1\\) 3. Square root is a metric\n\n\n\nDefinition:\n\\[\nH(P,Q) = -\\sum_{x} P(x)\\log Q(x)\n\\]\nRelation to KL divergence:\n\\[\nH(P,Q) = H(P) + D_{KL}(P\\|Q)\n\\]"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#applications-in-machine-learning",
    "href": "posts/information-theory-ml/index.html#applications-in-machine-learning",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Objective function:\n\\[\n\\max_{p} H(p) \\text{ subject to } \\mathbb{E}_p[f_i] = \\mu_i\n\\]\nSolution form:\n\\[\np^*(x) = \\frac{1}{Z(\\lambda)}\\exp\\left(\\sum_i \\lambda_i f_i(x)\\right)\n\\]\n\n\n\nObjective:\n\\[\n\\min_{p(t|x)} I(X;T) - \\beta I(T;Y)\n\\]\nSolution characterization:\n\\[\np(t|x) = \\frac{p(t)}{Z(x,\\beta)}\\exp\\left(-\\beta D_{KL}(p(y|x)\\|p(y|t))\\right)\n\\]\n\n\n\nMINE estimator:\n\\[\nI_\\theta(X,Y) = \\sup_{\\theta \\in \\Theta} \\mathbb{E}_{P_{XY}}[T_\\theta] - \\log\\mathbb{E}_{P_X \\otimes P_Y}[e^{T_\\theta}]\n\\]"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#information-theory-in-deep-learning",
    "href": "posts/information-theory-ml/index.html#information-theory-in-deep-learning",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Layer-wise information:\n\\[\n\\begin{aligned}\nI(X;T_l) &= \\text{Information about input} \\\\\nI(T_l;Y) &= \\text{Information about output}\n\\end{aligned}\n\\]\n\n\n\nLower bound:\n\\[\nI(X;Y) \\geq \\mathbb{E}_{p(x,y)}[\\log q_\\theta(y|x)] + h(Y)\n\\]\n\n\n\nDropout probability:\n\\[\np(z|x) = \\mathcal{N}(z|\\mu(x), \\alpha(x)\\sigma^2)\n\\]\nCost function:\n\\[\n\\mathcal{L} = \\mathbb{E}[\\log p(y|z)] - \\beta D_{KL}(p(z|x)\\|r(z))\n\\]"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#advanced-applications",
    "href": "posts/information-theory-ml/index.html#advanced-applications",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "VAE ELBO:\n\\[\n\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)\\|p(z))\n\\]\nGAN objective:\n\\[\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\n\\]\n\n\n\nInfoNCE loss:\n\\[\n\\mathcal{L}_N = -\\mathbb{E}\\left[\\log \\frac{e^{f(x,y)/\\tau}}{\\sum_{i=1}^N e^{f(x,y_i)/\\tau}}\\right]\n\\]\nContrastive predictive coding:\n\\[\n\\mathcal{L}_{CPC} = -\\sum_{k=1}^K \\mathbb{E}\\left[\\log \\frac{e^{f_k(c_t,x_{t+k})}}{e^{f_k(c_t,x_{t+k})} + \\sum_{j=1}^N e^{f_k(c_t,x_j)}}\\right]\n\\]\n\n\n\nInformation bottleneck objective:\n\\[\n\\min_{p(t|x)} I(X;T) \\text{ subject to } I(T;Y) \\geq I_0\n\\]\nRate-distortion theory:\n\\[\nR(D) = \\min_{p(t|x): \\mathbb{E}[d(X,T)] \\leq D} I(X;T)\n\\]"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#theoretical-insights",
    "href": "posts/information-theory-ml/index.html#theoretical-insights",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Information dynamics:\n\\[\n\\frac{d}{dt}I(X;T_t) = \\mathbb{E}\\left[\\text{tr}\\left(\\frac{\\partial^2 I}{\\partial T^2}\\frac{dT}{dt}\\frac{dT}{dt}^T\\right)\\right]\n\\]\n\n\n\nPAC-Bayes bound:\n\\[\n\\text{gen-error} \\leq \\frac{D_{KL}(Q\\|P) + \\log\\frac{2\\sqrt{n}}{\\delta}}{2n}\n\\]\n\n\n\nNatural gradient:\n\\[\n\\dot{\\theta} = F^{-1}(\\theta)\\nabla_\\theta \\mathcal{L}\n\\]\nWhere: - \\(F(\\theta)\\) is the Fisher information matrix"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#implementation-considerations",
    "href": "posts/information-theory-ml/index.html#implementation-considerations",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Log-sum-exp trick:\n\\[\n\\log\\sum_i e^{x_i} = a + \\log\\sum_i e^{x_i-a}\n\\]\nWhere: - \\(a = \\max_i x_i\\)\n\n\n\nKDE entropy estimator:\n\\[\n\\hat{H}(X) = -\\frac{1}{n}\\sum_{i=1}^n \\log\\frac{1}{n}\\sum_{j=1}^n K_h(x_i-x_j)\n\\]\n\n\n\nMini-batch mutual information:\n\\[\n\\hat{I}(X;Y) = \\frac{1}{B}\\sum_{i=1}^B \\log\\frac{p(x_i,y_i)}{\\hat{p}(x_i)\\hat{p}(y_i)}\n\\]"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#best-practices",
    "href": "posts/information-theory-ml/index.html#best-practices",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Information Flow:\n\nMonitor layer-wise information\nBalance compression and preservation\nUse information bottlenecks\n\nArchitecture Choice:\n\nConsider mutual information\nInformation capacity\nBottleneck dimensions\n\nRegularization:\n\nInformation dropout\nMutual information constraints\nEntropy regularization\n\n\n\n\n\n\nOptimization:\n\nNatural gradients\nInformation-based learning rates\nAdaptive methods\n\nMonitoring:\n\nInformation plane dynamics\nCompression metrics\nMutual information estimates\n\nValidation:\n\nCross-entropy\nKL divergence\nInformation efficiency"
  },
  {
    "objectID": "posts/information-theory-ml/index.html#references",
    "href": "posts/information-theory-ml/index.html#references",
    "title": "Information Theory in Machine Learning",
    "section": "",
    "text": "Theory:\n\n“Elements of Information Theory” by Cover and Thomas\n“Information Theory, Inference, and Learning Algorithms” by MacKay\n“Deep Learning and the Information Bottleneck Principle” by Tishby and Zaslavsky\n\nMethods:\n\n“Deep Learning with Information Theoretic Learning” by Principe\n“Information Theory and Statistics” by Csiszár and Shields\n“Information Theory and Machine Learning” by Amari\n\nApplications:\n\n“Deep Learning” by Goodfellow et al.\n“Pattern Recognition and Machine Learning” by Bishop\n“Machine Learning: A Probabilistic Perspective” by Murphy"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html",
    "href": "posts/ml-fundamentals/index.html",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Machine learning (ML) is a transformative field that enables computers to learn from data without being explicitly programmed. Before diving into specific algorithms or frameworks, it’s crucial to understand the fundamental concepts that form the foundation of machine learning.\n\n\nMachine learning is a subset of artificial intelligence that focuses on developing systems that can learn and improve from experience. Instead of following explicit instructions, ML systems identify patterns in data to make decisions or predictions.\n\n\n\nData-Driven: ML systems learn from examples rather than following predefined rules\nPattern Recognition: They identify patterns and relationships in data\nAutomation: They can automatically improve with more experience/data\nGeneralization: They can handle new, unseen data based on learned patterns\n\n\n\n\n\n\n\n\nLearning from labeled data\nExamples: Classification, Regression\nUse cases: Spam detection, Price prediction, Image recognition\n\n\n\n\n\nLearning from unlabeled data\nExamples: Clustering, Dimensionality Reduction\nUse cases: Customer segmentation, Feature learning\n\n\n\n\n\nLearning through interaction with an environment\nExamples: Game playing, Robot navigation\nUse cases: Autonomous systems, Game AI\n\n\n\n\n\nUnderstanding the ML workflow is crucial for successful implementation:\n\nProblem Definition\n\nDefine objectives\nIdentify success metrics\nUnderstand constraints\n\nData Collection\n\nGather relevant data\nEnsure data quality\nConsider data privacy and ethics\n\nData Preprocessing\n\nClean the data\nHandle missing values\nFormat data appropriately\n\nFeature Engineering\n\nSelect relevant features\nCreate new features\nTransform existing features\n\nModel Selection\n\nChoose appropriate algorithms\nConsider model complexity\nBalance bias and variance\n\nModel Training\n\nSplit data into training/validation sets\nTrain the model\nTune hyperparameters\n\nModel Evaluation\n\nAssess performance\nValidate on test data\nConsider business metrics\n\nDeployment\n\nIntegrate with systems\nMonitor performance\nMaintain and update\n\n\n\n\n\n\n\n\nFeatures: Input variables used for prediction\nLabels: Target variables we’re trying to predict\nParameters: Values learned during training\nHyperparameters: Configuration values set before training\n\n\n\n\n\nBias: Model’s tendency to consistently miss the true relationship\nVariance: Model’s sensitivity to fluctuations in the training data\nOverfitting: Model learns noise in training data\nUnderfitting: Model fails to capture underlying patterns\n\n\n\n\n\nAccuracy: Proportion of correct predictions\nPrecision: Accuracy of positive predictions\nRecall: Ability to find all positive instances\nF1 Score: Harmonic mean of precision and recall\n\n\n\n\n\n\nData Quality Issues\n\nMissing values\nNoisy data\nInconsistent formatting\n\nFeature Selection\n\nIdentifying relevant features\nHandling high dimensionality\nCreating meaningful features\n\nModel Selection\n\nChoosing appropriate algorithms\nBalancing complexity and performance\nHandling computational constraints\n\nOverfitting and Underfitting\n\nFinding the right model complexity\nGathering sufficient training data\nUsing appropriate regularization\n\n\n\n\n\n\nStart Simple\n\nBegin with basic models\nEstablish baselines\nGradually increase complexity\n\nCross-Validation\n\nUse multiple data splits\nValidate model stability\nEnsure generalization\n\nFeature Engineering\n\nCreate meaningful features\nRemove irrelevant features\nHandle categorical variables appropriately\n\nModel Evaluation\n\nUse appropriate metrics\nConsider business impact\nTest on unseen data\n\n\n\n\n\nUnderstanding these fundamentals is crucial before diving into specific algorithms or frameworks. In the next posts, we’ll explore:\n\nData Understanding and Preprocessing\nFeature Engineering and Selection\nModel Selection and Evaluation\nAdvanced Topics and Deep Learning\n\nStay tuned for more detailed explorations of each topic!\n\n\n\n\nBooks:\n\n“Introduction to Machine Learning with Python” by Andreas Müller & Sarah Guido\n“The Hundred-Page Machine Learning Book” by Andriy Burkov\n\nOnline Courses:\n\nAndrew Ng’s Machine Learning Course on Coursera\nFast.ai’s Practical Deep Learning Course\n\nWebsites:\n\nScikit-learn Documentation\nTowards Data Science\nMachine Learning Mastery\n\n\nRemember: Building a strong foundation in these fundamentals is crucial for success in machine learning. Take time to understand these concepts thoroughly before moving on to more advanced topics."
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#what-is-machine-learning",
    "href": "posts/ml-fundamentals/index.html#what-is-machine-learning",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Machine learning is a subset of artificial intelligence that focuses on developing systems that can learn and improve from experience. Instead of following explicit instructions, ML systems identify patterns in data to make decisions or predictions.\n\n\n\nData-Driven: ML systems learn from examples rather than following predefined rules\nPattern Recognition: They identify patterns and relationships in data\nAutomation: They can automatically improve with more experience/data\nGeneralization: They can handle new, unseen data based on learned patterns"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#types-of-machine-learning",
    "href": "posts/ml-fundamentals/index.html#types-of-machine-learning",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Learning from labeled data\nExamples: Classification, Regression\nUse cases: Spam detection, Price prediction, Image recognition\n\n\n\n\n\nLearning from unlabeled data\nExamples: Clustering, Dimensionality Reduction\nUse cases: Customer segmentation, Feature learning\n\n\n\n\n\nLearning through interaction with an environment\nExamples: Game playing, Robot navigation\nUse cases: Autonomous systems, Game AI"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#the-machine-learning-workflow",
    "href": "posts/ml-fundamentals/index.html#the-machine-learning-workflow",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Understanding the ML workflow is crucial for successful implementation:\n\nProblem Definition\n\nDefine objectives\nIdentify success metrics\nUnderstand constraints\n\nData Collection\n\nGather relevant data\nEnsure data quality\nConsider data privacy and ethics\n\nData Preprocessing\n\nClean the data\nHandle missing values\nFormat data appropriately\n\nFeature Engineering\n\nSelect relevant features\nCreate new features\nTransform existing features\n\nModel Selection\n\nChoose appropriate algorithms\nConsider model complexity\nBalance bias and variance\n\nModel Training\n\nSplit data into training/validation sets\nTrain the model\nTune hyperparameters\n\nModel Evaluation\n\nAssess performance\nValidate on test data\nConsider business metrics\n\nDeployment\n\nIntegrate with systems\nMonitor performance\nMaintain and update"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#essential-terminology",
    "href": "posts/ml-fundamentals/index.html#essential-terminology",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Features: Input variables used for prediction\nLabels: Target variables we’re trying to predict\nParameters: Values learned during training\nHyperparameters: Configuration values set before training\n\n\n\n\n\nBias: Model’s tendency to consistently miss the true relationship\nVariance: Model’s sensitivity to fluctuations in the training data\nOverfitting: Model learns noise in training data\nUnderfitting: Model fails to capture underlying patterns\n\n\n\n\n\nAccuracy: Proportion of correct predictions\nPrecision: Accuracy of positive predictions\nRecall: Ability to find all positive instances\nF1 Score: Harmonic mean of precision and recall"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#common-challenges-in-machine-learning",
    "href": "posts/ml-fundamentals/index.html#common-challenges-in-machine-learning",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Data Quality Issues\n\nMissing values\nNoisy data\nInconsistent formatting\n\nFeature Selection\n\nIdentifying relevant features\nHandling high dimensionality\nCreating meaningful features\n\nModel Selection\n\nChoosing appropriate algorithms\nBalancing complexity and performance\nHandling computational constraints\n\nOverfitting and Underfitting\n\nFinding the right model complexity\nGathering sufficient training data\nUsing appropriate regularization"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#best-practices",
    "href": "posts/ml-fundamentals/index.html#best-practices",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Start Simple\n\nBegin with basic models\nEstablish baselines\nGradually increase complexity\n\nCross-Validation\n\nUse multiple data splits\nValidate model stability\nEnsure generalization\n\nFeature Engineering\n\nCreate meaningful features\nRemove irrelevant features\nHandle categorical variables appropriately\n\nModel Evaluation\n\nUse appropriate metrics\nConsider business impact\nTest on unseen data"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#next-steps",
    "href": "posts/ml-fundamentals/index.html#next-steps",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Understanding these fundamentals is crucial before diving into specific algorithms or frameworks. In the next posts, we’ll explore:\n\nData Understanding and Preprocessing\nFeature Engineering and Selection\nModel Selection and Evaluation\nAdvanced Topics and Deep Learning\n\nStay tuned for more detailed explorations of each topic!"
  },
  {
    "objectID": "posts/ml-fundamentals/index.html#additional-resources",
    "href": "posts/ml-fundamentals/index.html#additional-resources",
    "title": "ML Fundamentals: Understanding the Basics",
    "section": "",
    "text": "Books:\n\n“Introduction to Machine Learning with Python” by Andreas Müller & Sarah Guido\n“The Hundred-Page Machine Learning Book” by Andriy Burkov\n\nOnline Courses:\n\nAndrew Ng’s Machine Learning Course on Coursera\nFast.ai’s Practical Deep Learning Course\n\nWebsites:\n\nScikit-learn Documentation\nTowards Data Science\nMachine Learning Mastery\n\n\nRemember: Building a strong foundation in these fundamentals is crucial for success in machine learning. Take time to understand these concepts thoroughly before moving on to more advanced topics."
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html",
    "href": "posts/ml-theory-foundations/index.html",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Risk minimization:\n\\[\nR(f) = \\mathbb{E}_{(X,Y)\\sim P}[L(f(X),Y)]\n\\]\nEmpirical risk:\n\\[\n\\hat{R}_n(f) = \\frac{1}{n}\\sum_{i=1}^n L(f(x_i),y_i)\n\\]\n\n\n\nHoeffding’s inequality:\n\\[\nP(|\\hat{R}_n(f) - R(f)| &gt; \\epsilon) \\leq 2\\exp(-2n\\epsilon^2)\n\\]\nUnion bound for finite hypothesis class:\n\\[\nP(\\sup_{f \\in \\mathcal{F}}|\\hat{R}_n(f) - R(f)| &gt; \\epsilon) \\leq 2|\\mathcal{F}|\\exp(-2n\\epsilon^2)\n\\]\n\n\n\nVC dimension definition: - Maximum number of points that can be shattered - Growth function: \\(\\Pi_{\\mathcal{F}}(n)\\) - Sauer’s Lemma: \\(\\Pi_{\\mathcal{F}}(n) \\leq \\sum_{i=0}^d \\binom{n}{i}\\)\nVC generalization bound:\n\\[\nP(\\sup_{f \\in \\mathcal{F}}|\\hat{R}_n(f) - R(f)| &gt; \\epsilon) \\leq 8\\Pi_{\\mathcal{F}}(2n)\\exp(-n\\epsilon^2/32)\n\\]\n\n\n\n\n\n\nFirst-order condition:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x)\n\\]\nSecond-order condition:\n\\[\n\\nabla^2 f(x) \\succeq 0\n\\]\n\n\n\nDefinition:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu}{2}\\|y-x\\|^2\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq (1-\\frac{\\mu}{L})^k[f(x_0) - f(x^*)]\n\\]\n\n\n\nL-smoothness condition:\n\\[\n\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L\\|x-y\\|\n\\]\nGradient descent convergence:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{2L\\|x_0-x^*\\|^2}{k+4}\n\\]\n\n\n\n\n\n\nEntropy:\n\\[\nH(X) = -\\sum_{x} P(x)\\log P(x)\n\\]\nMutual information:\n\\[\nI(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n\\]\n\n\n\nObjective:\n\\[\n\\min_{P(T|X)} I(X;T) - \\beta I(T;Y)\n\\]\nSolution characterization:\n\\[\nP(t|x) = \\frac{P(t)}{Z(x,\\beta)}\\exp(-\\beta D_{KL}(P(Y|x)\\|P(Y|t)))\n\\]\n\n\n\nPAC-Bayes bound:\n\\[\nP(R(Q) \\leq \\hat{R}(Q) + \\sqrt{\\frac{D_{KL}(Q\\|P) + \\ln\\frac{2\\sqrt{n}}{\\delta}}{2n}}) \\geq 1-\\delta\n\\]\n\n\n\n\n\n\nDefinition:\n\\[\n\\mathfrak{R}_n(\\mathcal{F}) = \\mathbb{E}_{\\sigma,S}[\\sup_{f \\in \\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^n \\sigma_i f(x_i)]\n\\]\nGeneralization bound:\n\\[\nP(\\sup_{f \\in \\mathcal{F}}|R(f) - \\hat{R}_n(f)| \\leq 2\\mathfrak{R}_n(\\mathcal{F}) + \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}) \\geq 1-\\delta\n\\]\n\n\n\nDefinition: - \\(\\epsilon\\)-cover of \\(\\mathcal{F}\\) - Metric entropy: \\(\\ln N(\\epsilon,\\mathcal{F},L_2)\\)\nDudley’s entropy integral:\n\\[\n\\mathfrak{R}_n(\\mathcal{F}) \\leq \\frac{12}{\\sqrt{n}}\\int_0^\\infty \\sqrt{\\ln N(\\epsilon,\\mathcal{F},L_2)}d\\epsilon\n\\]\n\n\n\nAlgorithmic stability:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\nGeneralization bound:\n\\[\nP(|R(A_S) - \\hat{R}_n(A_S)| &gt; \\epsilon) \\leq 2\\exp(-2n\\epsilon^2/\\beta^2)\n\\]\n\n\n\n\n\n\nGradient descent:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n\\]\nConvergence rate (convex):\n\\[\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n\\]\n\n\n\nSGD update:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f_{i_k}(x_k)\n\\]\nConvergence rate:\n\\[\n\\mathbb{E}[f(x_k) - f(x^*)] \\leq \\frac{L\\|x_0-x^*\\|^2}{2k} + \\frac{L\\sigma^2}{2}\\sum_{t=1}^k \\eta_t^2\n\\]\n\n\n\nNesterov’s method:\n\\[\n\\begin{aligned}\ny_k &= x_k + \\beta_k(x_k - x_{k-1}) \\\\\nx_{k+1} &= y_k - \\eta_k\\nabla f(y_k)\n\\end{aligned}\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{4L\\|x_0-x^*\\|^2}{(k+2)^2}\n\\]\n\n\n\n\n\n\nRegret bound:\n\\[\nR_T = \\sum_{t=1}^T \\ell_t(x_t) - \\min_{x \\in \\mathcal{X}}\\sum_{t=1}^T \\ell_t(x)\n\\]\nOnline gradient descent:\n\\[\nR_T \\leq \\frac{D^2}{2\\eta} + \\frac{\\eta G^2T}{2}\n\\]\n\n\n\nUCB algorithm:\n\\[\n\\text{UCB}_i(t) = \\hat{\\mu}_i(t) + \\sqrt{\\frac{2\\ln t}{N_i(t)}}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sum_{i:\\Delta_i&gt;0} \\frac{8\\ln T}{\\Delta_i} + (1+\\frac{\\pi^2}{3})\\sum_{i=1}^K \\Delta_i\n\\]\n\n\n\nDisagreement coefficient:\n\\[\n\\theta = \\sup_{r&gt;0} \\frac{\\text{P}(DIS(B(h^*,r)))}{r}\n\\]\nLabel complexity:\n\\[\n\\tilde{O}(\\theta d\\log(1/\\epsilon))\n\\]\n\n\n\n\n\n\nMargin bound:\n\\[\nP(R(f) \\leq \\hat{R}_\\gamma(f) + O(\\sqrt{\\frac{d\\log(1/\\gamma)}{n\\gamma^2}})) \\geq 1-\\delta\n\\]\n\n\n\nRepresenter theorem:\n\\[\nf^*(x) = \\sum_{i=1}^n \\alpha_i K(x,x_i)\n\\]\nRKHS norm:\n\\[\n\\|f\\|_{\\mathcal{H}}^2 = \\sum_{i,j=1}^n \\alpha_i\\alpha_j K(x_i,x_j)\n\\]\n\n\n\nAdaBoost bound:\n\\[\n\\hat{R}(H_T) \\leq \\exp(-2\\sum_{t=1}^T(\\frac{1}{2}-\\gamma_t)^2)\n\\]\n\n\n\n\n\n\n\nBias-Variance Trade-off:\n\nEmpirical risk minimization\nStructural risk minimization\nCross-validation bounds\n\nRegularization:\n\nL1/L2 regularization theory\nEarly stopping\nModel averaging\n\nValidation:\n\nHold-out bounds\nBootstrap theory\nCross-validation theory\n\n\n\n\n\n\nOptimization:\n\nConvergence analysis\nStep size selection\nMomentum methods\n\nArchitecture:\n\nDepth vs width theory\nUniversal approximation\nExpressivity bounds\n\nLearning:\n\nSample complexity\nComputational complexity\nStatistical efficiency\n\n\n\n\n\n\n\nTheory:\n\n“Foundations of Machine Learning” by Mohri et al.\n“Understanding Machine Learning” by Shalev-Shwartz and Ben-David\n“Statistical Learning Theory” by Vapnik\n\nOptimization:\n\n“Convex Optimization” by Boyd and Vandenberghe\n“Introductory Lectures on Convex Optimization” by Nesterov\n“Optimization Methods for Large-Scale Machine Learning” by Bottou et al.\n\nAdvanced Topics:\n\n“Theory of Classification” by Devroye et al.\n“Information Theory, Inference, and Learning Algorithms” by MacKay\n“Theoretical Foundations of Deep Learning” by Vidal et al."
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#statistical-learning-theory",
    "href": "posts/ml-theory-foundations/index.html#statistical-learning-theory",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Risk minimization:\n\\[\nR(f) = \\mathbb{E}_{(X,Y)\\sim P}[L(f(X),Y)]\n\\]\nEmpirical risk:\n\\[\n\\hat{R}_n(f) = \\frac{1}{n}\\sum_{i=1}^n L(f(x_i),y_i)\n\\]\n\n\n\nHoeffding’s inequality:\n\\[\nP(|\\hat{R}_n(f) - R(f)| &gt; \\epsilon) \\leq 2\\exp(-2n\\epsilon^2)\n\\]\nUnion bound for finite hypothesis class:\n\\[\nP(\\sup_{f \\in \\mathcal{F}}|\\hat{R}_n(f) - R(f)| &gt; \\epsilon) \\leq 2|\\mathcal{F}|\\exp(-2n\\epsilon^2)\n\\]\n\n\n\nVC dimension definition: - Maximum number of points that can be shattered - Growth function: \\(\\Pi_{\\mathcal{F}}(n)\\) - Sauer’s Lemma: \\(\\Pi_{\\mathcal{F}}(n) \\leq \\sum_{i=0}^d \\binom{n}{i}\\)\nVC generalization bound:\n\\[\nP(\\sup_{f \\in \\mathcal{F}}|\\hat{R}_n(f) - R(f)| &gt; \\epsilon) \\leq 8\\Pi_{\\mathcal{F}}(2n)\\exp(-n\\epsilon^2/32)\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#optimization-theory",
    "href": "posts/ml-theory-foundations/index.html#optimization-theory",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "First-order condition:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x)\n\\]\nSecond-order condition:\n\\[\n\\nabla^2 f(x) \\succeq 0\n\\]\n\n\n\nDefinition:\n\\[\nf(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu}{2}\\|y-x\\|^2\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq (1-\\frac{\\mu}{L})^k[f(x_0) - f(x^*)]\n\\]\n\n\n\nL-smoothness condition:\n\\[\n\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L\\|x-y\\|\n\\]\nGradient descent convergence:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{2L\\|x_0-x^*\\|^2}{k+4}\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#information-theory-in-learning",
    "href": "posts/ml-theory-foundations/index.html#information-theory-in-learning",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Entropy:\n\\[\nH(X) = -\\sum_{x} P(x)\\log P(x)\n\\]\nMutual information:\n\\[\nI(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n\\]\n\n\n\nObjective:\n\\[\n\\min_{P(T|X)} I(X;T) - \\beta I(T;Y)\n\\]\nSolution characterization:\n\\[\nP(t|x) = \\frac{P(t)}{Z(x,\\beta)}\\exp(-\\beta D_{KL}(P(Y|x)\\|P(Y|t)))\n\\]\n\n\n\nPAC-Bayes bound:\n\\[\nP(R(Q) \\leq \\hat{R}(Q) + \\sqrt{\\frac{D_{KL}(Q\\|P) + \\ln\\frac{2\\sqrt{n}}{\\delta}}{2n}}) \\geq 1-\\delta\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#learning-theory-bounds",
    "href": "posts/ml-theory-foundations/index.html#learning-theory-bounds",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Definition:\n\\[\n\\mathfrak{R}_n(\\mathcal{F}) = \\mathbb{E}_{\\sigma,S}[\\sup_{f \\in \\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^n \\sigma_i f(x_i)]\n\\]\nGeneralization bound:\n\\[\nP(\\sup_{f \\in \\mathcal{F}}|R(f) - \\hat{R}_n(f)| \\leq 2\\mathfrak{R}_n(\\mathcal{F}) + \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}) \\geq 1-\\delta\n\\]\n\n\n\nDefinition: - \\(\\epsilon\\)-cover of \\(\\mathcal{F}\\) - Metric entropy: \\(\\ln N(\\epsilon,\\mathcal{F},L_2)\\)\nDudley’s entropy integral:\n\\[\n\\mathfrak{R}_n(\\mathcal{F}) \\leq \\frac{12}{\\sqrt{n}}\\int_0^\\infty \\sqrt{\\ln N(\\epsilon,\\mathcal{F},L_2)}d\\epsilon\n\\]\n\n\n\nAlgorithmic stability:\n\\[\n|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\nGeneralization bound:\n\\[\nP(|R(A_S) - \\hat{R}_n(A_S)| &gt; \\epsilon) \\leq 2\\exp(-2n\\epsilon^2/\\beta^2)\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#optimization-convergence",
    "href": "posts/ml-theory-foundations/index.html#optimization-convergence",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Gradient descent:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n\\]\nConvergence rate (convex):\n\\[\nf(x_k) - f(x^*) \\leq \\frac{\\|x_0-x^*\\|^2}{2\\eta k}\n\\]\n\n\n\nSGD update:\n\\[\nx_{k+1} = x_k - \\eta_k\\nabla f_{i_k}(x_k)\n\\]\nConvergence rate:\n\\[\n\\mathbb{E}[f(x_k) - f(x^*)] \\leq \\frac{L\\|x_0-x^*\\|^2}{2k} + \\frac{L\\sigma^2}{2}\\sum_{t=1}^k \\eta_t^2\n\\]\n\n\n\nNesterov’s method:\n\\[\n\\begin{aligned}\ny_k &= x_k + \\beta_k(x_k - x_{k-1}) \\\\\nx_{k+1} &= y_k - \\eta_k\\nabla f(y_k)\n\\end{aligned}\n\\]\nConvergence rate:\n\\[\nf(x_k) - f(x^*) \\leq \\frac{4L\\|x_0-x^*\\|^2}{(k+2)^2}\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#advanced-topics",
    "href": "posts/ml-theory-foundations/index.html#advanced-topics",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Regret bound:\n\\[\nR_T = \\sum_{t=1}^T \\ell_t(x_t) - \\min_{x \\in \\mathcal{X}}\\sum_{t=1}^T \\ell_t(x)\n\\]\nOnline gradient descent:\n\\[\nR_T \\leq \\frac{D^2}{2\\eta} + \\frac{\\eta G^2T}{2}\n\\]\n\n\n\nUCB algorithm:\n\\[\n\\text{UCB}_i(t) = \\hat{\\mu}_i(t) + \\sqrt{\\frac{2\\ln t}{N_i(t)}}\n\\]\nRegret bound:\n\\[\nR_T \\leq \\sum_{i:\\Delta_i&gt;0} \\frac{8\\ln T}{\\Delta_i} + (1+\\frac{\\pi^2}{3})\\sum_{i=1}^K \\Delta_i\n\\]\n\n\n\nDisagreement coefficient:\n\\[\n\\theta = \\sup_{r&gt;0} \\frac{\\text{P}(DIS(B(h^*,r)))}{r}\n\\]\nLabel complexity:\n\\[\n\\tilde{O}(\\theta d\\log(1/\\epsilon))\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#theoretical-frameworks",
    "href": "posts/ml-theory-foundations/index.html#theoretical-frameworks",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Margin bound:\n\\[\nP(R(f) \\leq \\hat{R}_\\gamma(f) + O(\\sqrt{\\frac{d\\log(1/\\gamma)}{n\\gamma^2}})) \\geq 1-\\delta\n\\]\n\n\n\nRepresenter theorem:\n\\[\nf^*(x) = \\sum_{i=1}^n \\alpha_i K(x,x_i)\n\\]\nRKHS norm:\n\\[\n\\|f\\|_{\\mathcal{H}}^2 = \\sum_{i,j=1}^n \\alpha_i\\alpha_j K(x_i,x_j)\n\\]\n\n\n\nAdaBoost bound:\n\\[\n\\hat{R}(H_T) \\leq \\exp(-2\\sum_{t=1}^T(\\frac{1}{2}-\\gamma_t)^2)\n\\]"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#best-practices",
    "href": "posts/ml-theory-foundations/index.html#best-practices",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Bias-Variance Trade-off:\n\nEmpirical risk minimization\nStructural risk minimization\nCross-validation bounds\n\nRegularization:\n\nL1/L2 regularization theory\nEarly stopping\nModel averaging\n\nValidation:\n\nHold-out bounds\nBootstrap theory\nCross-validation theory\n\n\n\n\n\n\nOptimization:\n\nConvergence analysis\nStep size selection\nMomentum methods\n\nArchitecture:\n\nDepth vs width theory\nUniversal approximation\nExpressivity bounds\n\nLearning:\n\nSample complexity\nComputational complexity\nStatistical efficiency"
  },
  {
    "objectID": "posts/ml-theory-foundations/index.html#references",
    "href": "posts/ml-theory-foundations/index.html#references",
    "title": "Machine Learning Theory: Mathematical Foundations",
    "section": "",
    "text": "Theory:\n\n“Foundations of Machine Learning” by Mohri et al.\n“Understanding Machine Learning” by Shalev-Shwartz and Ben-David\n“Statistical Learning Theory” by Vapnik\n\nOptimization:\n\n“Convex Optimization” by Boyd and Vandenberghe\n“Introductory Lectures on Convex Optimization” by Nesterov\n“Optimization Methods for Large-Scale Machine Learning” by Bottou et al.\n\nAdvanced Topics:\n\n“Theory of Classification” by Devroye et al.\n“Information Theory, Inference, and Learning Algorithms” by MacKay\n“Theoretical Foundations of Deep Learning” by Vidal et al."
  },
  {
    "objectID": "posts/optimization-algorithms/index.html",
    "href": "posts/optimization-algorithms/index.html",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "The core of optimization in machine learning is minimizing (or maximizing) an objective function:\n\\[\n\\min_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L(f_\\theta(x_i), y_i) + \\lambda R(\\theta)\n\\]\nWhere: - \\(J(\\theta)\\) is the objective function - \\(\\theta\\) represents model parameters - \\(L\\) is the loss function - \\(f_\\theta\\) is the model prediction - \\(R(\\theta)\\) is the regularization term - \\(\\lambda\\) is the regularization strength\n\n\n\nThe basic update rule for gradient descent:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta_t)\n\\]\nWhere: - \\(\\theta_t\\) is the parameter at iteration t - \\(\\eta\\) is the learning rate - \\(\\nabla_\\theta J(\\theta_t)\\) is the gradient of the objective function\n\n\n\nFor convex functions, gradient descent converges at rate:\n\\[\nJ(\\theta_t) - J(\\theta^*) \\leq \\frac{\\|\\theta_0 - \\theta^*\\|^2}{2\\eta t}\n\\]\nWhere: - \\(\\theta^*\\) is the optimal parameter - \\(\\theta_0\\) is the initial parameter - \\(t\\) is the number of iterations\n\n\n\n\n\n\nUpdate rule:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_\\theta L(f_\\theta(x_i), y_i)\n\\]\nConvergence rate for strongly convex functions:\n\\[\n\\mathbb{E}[J(\\theta_t) - J(\\theta^*)] \\leq \\frac{L}{2\\mu t}\n\\]\nWhere: - \\(L\\) is the Lipschitz constant - \\(\\mu\\) is the strong convexity parameter\n\n\n\nIncorporates velocity in updates:\n\\[\n\\begin{aligned}\nv_{t+1} &= \\gamma v_t + \\eta \\nabla_\\theta J(\\theta_t) \\\\\n\\theta_{t+1} &= \\theta_t - v_{t+1}\n\\end{aligned}\n\\]\nWhere: - \\(v_t\\) is the velocity at time t - \\(\\gamma\\) is the momentum coefficient\n\n\n\nLooks ahead for gradient computation:\n\\[\n\\begin{aligned}\nv_{t+1} &= \\gamma v_t + \\eta \\nabla_\\theta J(\\theta_t + \\gamma v_t) \\\\\n\\theta_{t+1} &= \\theta_t - v_{t+1}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nAdapts learning rates per parameter:\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot g_t\n\\]\nWhere: - \\(G_t\\) is the sum of squared gradients up to time t - \\(g_t\\) is the current gradient - \\(\\odot\\) represents element-wise multiplication\n\n\n\nExponentially decaying average of squared gradients:\n\\[\n\\begin{aligned}\nG_t &= \\gamma G_{t-1} + (1-\\gamma)g_t^2 \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot g_t\n\\end{aligned}\n\\]\n\n\n\nCombines momentum and adaptive learning rates:\n\\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t\n\\end{aligned}\n\\]\n\n\n\n\n\n\nUpdate rule using Hessian:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta H^{-1}\\nabla_\\theta J(\\theta_t)\n\\]\nWhere: - \\(H\\) is the Hessian matrix of second derivatives\n\n\n\nApproximates Hessian inverse:\n\\[\n\\begin{aligned}\ns_k &= \\theta_{k+1} - \\theta_k \\\\\ny_k &= \\nabla J(\\theta_{k+1}) - \\nabla J(\\theta_k) \\\\\nB_{k+1} &= B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nCommon schedules include:\n\nStep decay: \\[\n\\eta_t = \\eta_0 \\gamma^{\\lfloor t/k \\rfloor}\n\\]\nExponential decay: \\[\n\\eta_t = \\eta_0 e^{-kt}\n\\]\nCosine annealing: \\[\n\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{t\\pi}{T}))\n\\]\n\n\n\n\nThe relationship between batch size and learning rate:\n\\[\n\\eta_{effective} = \\eta \\sqrt{\\frac{b}{b_{base}}}\n\\]\nWhere: - \\(b\\) is the current batch size - \\(b_{base}\\) is the reference batch size\n\n\n\nFor handling exploding gradients:\n\\[\ng_t = \\begin{cases}\ng_t & \\text{if } \\|g_t\\| \\leq c \\\\\nc\\frac{g_t}{\\|g_t\\|} & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nUpdate rule using Fisher Information Matrix:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta F^{-1}\\nabla_\\theta J(\\theta_t)\n\\]\nWhere: - \\(F\\) is the Fisher Information Matrix\n\n\n\nFor parallel SGD with K workers:\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{K}\\sum_{k=1}^K \\nabla_\\theta J_k(\\theta_t)\n\\]\n\n\n\nAveraging weights along the trajectory:\n\\[\n\\theta_{SWA} = \\frac{1}{n}\\sum_{i=1}^n \\theta_i\n\\]\n\n\n\n\n\n\n\nFirst try Adam with default parameters:\n\nLearning rate: \\(10^{-3}\\)\n\\(\\beta_1 = 0.9\\)\n\\(\\beta_2 = 0.999\\)\n\\(\\epsilon = 10^{-8}\\)\n\nIf training is unstable, try:\n\nReducing learning rate\nGradient clipping\nLayer normalization\n\nFor fine-tuning, consider:\n\nSGD with momentum\nCosine annealing\nSWA\n\n\n\n\n\n\nLearning rate search:\n\nStart with logarithmic grid\nUse learning rate finder algorithm\n\nBatch size selection:\n\nStart with power of 2\nConsider memory constraints\nScale learning rate accordingly\n\nMomentum tuning:\n\nDefault: 0.9\nIncrease for noisy gradients\nDecrease for stable training\n\n\n\n\n\n\n\n\nSolutions: 1. Proper initialization: \\[\nW \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}})\n\\]\n\nGradient clipping\nLayer normalization: \\[\n\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\]\n\n\n\n\nSolutions: 1. Add noise to gradients: \\[\ng_t = \\nabla_\\theta J(\\theta_t) + \\mathcal{N}(0, \\sigma^2)\n\\]\n\nUse momentum-based methods\nImplement trust region methods\n\n\n\n\nSolutions: 1. Preconditioning: \\[\n\\theta_{t+1} = \\theta_t - \\eta P^{-1}\\nabla_\\theta J(\\theta_t)\n\\]\n\nAdaptive methods (Adam, RMSprop)\nSecond-order methods when feasible\n\n\n\n\n\nKey takeaways: 1. Understanding optimization fundamentals is crucial 2. Different algorithms suit different problems 3. Practical considerations often outweigh theoretical guarantees 4. Monitoring and debugging optimization is essential\n\n\n\n\nMathematical Foundations:\n\n“Convex Optimization” by Boyd and Vandenberghe\n“Optimization Methods for Large-Scale Machine Learning” by Bottou et al.\n\nImplementation Details:\n\n“Deep Learning” by Goodfellow et al.\n“Adaptive Methods for Machine Learning” by Duchi et al.\n\nAdvanced Topics:\n\n“Natural Gradient Works Efficiently in Learning” by Amari\n“On the Convergence of Adam and Beyond” by Reddi et al."
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#mathematical-foundations",
    "href": "posts/optimization-algorithms/index.html#mathematical-foundations",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "The core of optimization in machine learning is minimizing (or maximizing) an objective function:\n\\[\n\\min_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L(f_\\theta(x_i), y_i) + \\lambda R(\\theta)\n\\]\nWhere: - \\(J(\\theta)\\) is the objective function - \\(\\theta\\) represents model parameters - \\(L\\) is the loss function - \\(f_\\theta\\) is the model prediction - \\(R(\\theta)\\) is the regularization term - \\(\\lambda\\) is the regularization strength\n\n\n\nThe basic update rule for gradient descent:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta_t)\n\\]\nWhere: - \\(\\theta_t\\) is the parameter at iteration t - \\(\\eta\\) is the learning rate - \\(\\nabla_\\theta J(\\theta_t)\\) is the gradient of the objective function\n\n\n\nFor convex functions, gradient descent converges at rate:\n\\[\nJ(\\theta_t) - J(\\theta^*) \\leq \\frac{\\|\\theta_0 - \\theta^*\\|^2}{2\\eta t}\n\\]\nWhere: - \\(\\theta^*\\) is the optimal parameter - \\(\\theta_0\\) is the initial parameter - \\(t\\) is the number of iterations"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#first-order-methods",
    "href": "posts/optimization-algorithms/index.html#first-order-methods",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Update rule:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_\\theta L(f_\\theta(x_i), y_i)\n\\]\nConvergence rate for strongly convex functions:\n\\[\n\\mathbb{E}[J(\\theta_t) - J(\\theta^*)] \\leq \\frac{L}{2\\mu t}\n\\]\nWhere: - \\(L\\) is the Lipschitz constant - \\(\\mu\\) is the strong convexity parameter\n\n\n\nIncorporates velocity in updates:\n\\[\n\\begin{aligned}\nv_{t+1} &= \\gamma v_t + \\eta \\nabla_\\theta J(\\theta_t) \\\\\n\\theta_{t+1} &= \\theta_t - v_{t+1}\n\\end{aligned}\n\\]\nWhere: - \\(v_t\\) is the velocity at time t - \\(\\gamma\\) is the momentum coefficient\n\n\n\nLooks ahead for gradient computation:\n\\[\n\\begin{aligned}\nv_{t+1} &= \\gamma v_t + \\eta \\nabla_\\theta J(\\theta_t + \\gamma v_t) \\\\\n\\theta_{t+1} &= \\theta_t - v_{t+1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#adaptive-methods",
    "href": "posts/optimization-algorithms/index.html#adaptive-methods",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Adapts learning rates per parameter:\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot g_t\n\\]\nWhere: - \\(G_t\\) is the sum of squared gradients up to time t - \\(g_t\\) is the current gradient - \\(\\odot\\) represents element-wise multiplication\n\n\n\nExponentially decaying average of squared gradients:\n\\[\n\\begin{aligned}\nG_t &= \\gamma G_{t-1} + (1-\\gamma)g_t^2 \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot g_t\n\\end{aligned}\n\\]\n\n\n\nCombines momentum and adaptive learning rates:\n\\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#second-order-methods",
    "href": "posts/optimization-algorithms/index.html#second-order-methods",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Update rule using Hessian:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta H^{-1}\\nabla_\\theta J(\\theta_t)\n\\]\nWhere: - \\(H\\) is the Hessian matrix of second derivatives\n\n\n\nApproximates Hessian inverse:\n\\[\n\\begin{aligned}\ns_k &= \\theta_{k+1} - \\theta_k \\\\\ny_k &= \\nabla J(\\theta_{k+1}) - \\nabla J(\\theta_k) \\\\\nB_{k+1} &= B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#implementation-considerations",
    "href": "posts/optimization-algorithms/index.html#implementation-considerations",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Common schedules include:\n\nStep decay: \\[\n\\eta_t = \\eta_0 \\gamma^{\\lfloor t/k \\rfloor}\n\\]\nExponential decay: \\[\n\\eta_t = \\eta_0 e^{-kt}\n\\]\nCosine annealing: \\[\n\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{t\\pi}{T}))\n\\]\n\n\n\n\nThe relationship between batch size and learning rate:\n\\[\n\\eta_{effective} = \\eta \\sqrt{\\frac{b}{b_{base}}}\n\\]\nWhere: - \\(b\\) is the current batch size - \\(b_{base}\\) is the reference batch size\n\n\n\nFor handling exploding gradients:\n\\[\ng_t = \\begin{cases}\ng_t & \\text{if } \\|g_t\\| \\leq c \\\\\nc\\frac{g_t}{\\|g_t\\|} & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#advanced-topics",
    "href": "posts/optimization-algorithms/index.html#advanced-topics",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Update rule using Fisher Information Matrix:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta F^{-1}\\nabla_\\theta J(\\theta_t)\n\\]\nWhere: - \\(F\\) is the Fisher Information Matrix\n\n\n\nFor parallel SGD with K workers:\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{K}\\sum_{k=1}^K \\nabla_\\theta J_k(\\theta_t)\n\\]\n\n\n\nAveraging weights along the trajectory:\n\\[\n\\theta_{SWA} = \\frac{1}{n}\\sum_{i=1}^n \\theta_i\n\\]"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#practical-guidelines",
    "href": "posts/optimization-algorithms/index.html#practical-guidelines",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "First try Adam with default parameters:\n\nLearning rate: \\(10^{-3}\\)\n\\(\\beta_1 = 0.9\\)\n\\(\\beta_2 = 0.999\\)\n\\(\\epsilon = 10^{-8}\\)\n\nIf training is unstable, try:\n\nReducing learning rate\nGradient clipping\nLayer normalization\n\nFor fine-tuning, consider:\n\nSGD with momentum\nCosine annealing\nSWA\n\n\n\n\n\n\nLearning rate search:\n\nStart with logarithmic grid\nUse learning rate finder algorithm\n\nBatch size selection:\n\nStart with power of 2\nConsider memory constraints\nScale learning rate accordingly\n\nMomentum tuning:\n\nDefault: 0.9\nIncrease for noisy gradients\nDecrease for stable training"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#common-issues-and-solutions",
    "href": "posts/optimization-algorithms/index.html#common-issues-and-solutions",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Solutions: 1. Proper initialization: \\[\nW \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}})\n\\]\n\nGradient clipping\nLayer normalization: \\[\n\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\]\n\n\n\n\nSolutions: 1. Add noise to gradients: \\[\ng_t = \\nabla_\\theta J(\\theta_t) + \\mathcal{N}(0, \\sigma^2)\n\\]\n\nUse momentum-based methods\nImplement trust region methods\n\n\n\n\nSolutions: 1. Preconditioning: \\[\n\\theta_{t+1} = \\theta_t - \\eta P^{-1}\\nabla_\\theta J(\\theta_t)\n\\]\n\nAdaptive methods (Adam, RMSprop)\nSecond-order methods when feasible"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#conclusion",
    "href": "posts/optimization-algorithms/index.html#conclusion",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Key takeaways: 1. Understanding optimization fundamentals is crucial 2. Different algorithms suit different problems 3. Practical considerations often outweigh theoretical guarantees 4. Monitoring and debugging optimization is essential"
  },
  {
    "objectID": "posts/optimization-algorithms/index.html#references",
    "href": "posts/optimization-algorithms/index.html#references",
    "title": "Optimization Algorithms in Machine Learning: A Deep Dive",
    "section": "",
    "text": "Mathematical Foundations:\n\n“Convex Optimization” by Boyd and Vandenberghe\n“Optimization Methods for Large-Scale Machine Learning” by Bottou et al.\n\nImplementation Details:\n\n“Deep Learning” by Goodfellow et al.\n“Adaptive Methods for Machine Learning” by Duchi et al.\n\nAdvanced Topics:\n\n“Natural Gradient Works Efficiently in Learning” by Amari\n“On the Convergence of Adam and Beyond” by Reddi et al."
  },
  {
    "objectID": "posts/pac-learning-theory/index.html",
    "href": "posts/pac-learning-theory/index.html",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "PAC learning definition: - Hypothesis class \\(\\mathcal{H}\\) - Instance space \\(\\mathcal{X}\\) - Target concept \\(c: \\mathcal{X} \\to \\{0,1\\}\\) - Distribution \\(\\mathcal{D}\\) over \\(\\mathcal{X}\\)\nPAC requirements:\n\\[\nP_{S \\sim \\mathcal{D}^m}(\\text{error}_\\mathcal{D}(h_S) \\leq \\epsilon) \\geq 1-\\delta\n\\]\nWhere: - \\(\\epsilon\\) is accuracy parameter - \\(\\delta\\) is confidence parameter - \\(m\\) is sample size - \\(h_S\\) is learned hypothesis\n\n\n\nFundamental bound:\n\\[\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{1}{\\delta}\\right)\n\\]\nRealizable case:\n\\[\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nError bound:\n\\[\n\\text{error}_\\mathcal{D}(h) \\leq \\min_{h' \\in \\mathcal{H}}\\text{error}_\\mathcal{D}(h') + \\epsilon\n\\]\nSample complexity:\n\\[\nm \\geq \\frac{2}{\\epsilon^2}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{2}{\\delta}\\right)\n\\]\n\n\n\n\n\n\nGrowth function:\n\\[\n\\Pi_\\mathcal{H}(m) = \\max_{x_1,...,x_m \\in \\mathcal{X}}|\\{(h(x_1),...,h(x_m)): h \\in \\mathcal{H}\\}|\n\\]\nSauer’s Lemma:\n\\[\n\\text{If VC}(\\mathcal{H}) = d, \\text{ then } \\Pi_\\mathcal{H}(m) \\leq \\sum_{i=0}^d \\binom{m}{i}\n\\]\n\n\n\nFundamental theorem:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| &gt; \\epsilon) \\leq 4\\Pi_\\mathcal{H}(2m)\\exp(-\\frac{m\\epsilon^2}{8})\n\\]\nSample complexity:\n\\[\nm = O\\left(\\frac{d}{\\epsilon^2}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon^2}\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nDefinition:\n\\[\n\\mathfrak{R}_S(\\mathcal{H}) = \\mathbb{E}_\\sigma\\left[\\sup_{h \\in \\mathcal{H}}\\frac{1}{m}\\sum_{i=1}^m \\sigma_i h(x_i)\\right]\n\\]\nGeneralization bound:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| \\leq 2\\mathfrak{R}_m(\\mathcal{H}) + \\sqrt{\\frac{2\\ln(2/\\delta)}{m}}) \\geq 1-\\delta\n\\]\n\n\n\n\n\n\nCompression scheme:\n\\[\n\\kappa: \\cup_{m=1}^\\infty (\\mathcal{X} \\times \\{0,1\\})^m \\to \\cup_{i=1}^k (\\mathcal{X} \\times \\{0,1\\})^i\n\\]\nBound:\n\\[\nm \\geq O\\left(\\frac{k}{\\epsilon}\\log\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nWeak learning condition:\n\\[\nP(\\text{error}_\\mathcal{D}(h) \\leq \\frac{1}{2} - \\gamma) \\geq 1-\\delta\n\\]\nStrong learning guarantee:\n\\[\nP(\\text{error}_\\mathcal{D}(H) \\leq \\epsilon) \\geq 1-\\delta\n\\]\n\n\n\nMistake bound:\n\\[\nM \\leq O\\left(\\frac{d}{\\gamma^2}\\log\\frac{1}{\\gamma}\\right)\n\\]\nWhere: - \\(M\\) is number of mistakes - \\(d\\) is VC dimension - \\(\\gamma\\) is margin\n\n\n\n\n\n\nEmpirical Risk Minimization (ERM):\n\\[\nh_S = \\arg\\min_{h \\in \\mathcal{H}}\\widehat{\\text{error}}_S(h)\n\\]\nConsistency condition:\n\\[\n\\lim_{m \\to \\infty}P(\\text{error}_\\mathcal{D}(h_S) &gt; \\inf_{h \\in \\mathcal{H}}\\text{error}_\\mathcal{D}(h) + \\epsilon) = 0\n\\]\n\n\n\nDouble sampling:\n\\[\nP_{S,S'}(|\\widehat{\\text{error}}_S(h) - \\widehat{\\text{error}}_{S'}(h)| &gt; \\epsilon) \\leq \\delta\n\\]\nSymmetrization bound:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| &gt; 2\\epsilon) \\leq 2P(\\sup_{h \\in \\mathcal{H}}|\\widehat{\\text{error}}_S(h) - \\widehat{\\text{error}}_{S'}(h)| &gt; \\epsilon)\n\\]\n\n\n\nNested hypothesis classes:\n\\[\n\\mathcal{H}_1 \\subset \\mathcal{H}_2 \\subset ... \\subset \\mathcal{H}_k\n\\]\nPenalty term:\n\\[\n\\text{pen}(h) = \\sqrt{\\frac{\\text{VC}(\\mathcal{H}(h))\\ln(em/\\text{VC}(\\mathcal{H}(h))) + \\ln(1/\\delta)}{m}}\n\\]\n\n\n\n\n\n\nLocal complexity:\n\\[\n\\mathfrak{R}_S(\\mathcal{H}, r) = \\mathbb{E}_\\sigma\\left[\\sup_{h \\in \\mathcal{H}: \\widehat{\\text{error}}_S(h) \\leq r}\\frac{1}{m}\\sum_{i=1}^m \\sigma_i h(x_i)\\right]\n\\]\nFixed point equation:\n\\[\nr^* = \\inf\\{r &gt; 0: \\mathfrak{R}_S(\\mathcal{H}, r) \\leq r/4\\}\n\\]\n\n\n\nMargin loss:\n\\[\n\\ell_\\gamma(yf(x)) = \\begin{cases}\n1 & \\text{if } yf(x) \\leq 0 \\\\\n1-\\frac{yf(x)}{\\gamma} & \\text{if } 0 &lt; yf(x) \\leq \\gamma \\\\\n0 & \\text{if } yf(x) &gt; \\gamma\n\\end{cases}\n\\]\nMargin bound:\n\\[\nP(\\text{error}_\\mathcal{D}(h) \\leq \\widehat{\\text{error}}_\\gamma(h) + O(\\sqrt{\\frac{d\\ln(1/\\gamma)}{m\\gamma^2}})) \\geq 1-\\delta\n\\]\n\n\n\nUniform stability:\n\\[\n\\sup_{S,z,i}|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\nGeneralization bound:\n\\[\nP(|\\text{error}_\\mathcal{D}(A_S) - \\widehat{\\text{error}}_S(A_S)| \\leq \\epsilon) \\geq 1-2\\exp(-\\frac{m\\epsilon^2}{2\\beta^2})\n\\]\n\n\n\n\n\n\nVC dimension: - Hyperplanes in \\(\\mathbb{R}^d\\): \\(d+1\\) - Homogeneous hyperplanes: \\(d\\)\nSample complexity:\n\\[\nm = O\\left(\\frac{d}{\\epsilon}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon}\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nVC dimension bound:\n\\[\n\\text{VC}(\\text{NN}) \\leq O(WL\\log W)\n\\]\nWhere: - \\(W\\) is number of weights - \\(L\\) is number of layers\n\n\n\nEffective dimension:\n\\[\nd_\\text{eff}(\\lambda) = \\text{tr}(K(K+\\lambda mI)^{-1})\n\\]\nSample complexity:\n\\[\nm = O\\left(\\frac{d_\\text{eff}(\\lambda)}{\\epsilon^2} + \\frac{\\log(1/\\delta)}{\\epsilon^2}\\right)\n\\]\n\n\n\n\n\n\n\nComplexity Control:\n\nVC dimension analysis\nRademacher complexity\nStability measures\n\nRegularization:\n\nTheoretical guarantees\nSample complexity\nGeneralization bounds\n\nValidation:\n\nPAC bounds\nCross-validation theory\nHold-out guarantees\n\n\n\n\n\n\nLearning Rate:\n\nSample complexity analysis\nConvergence guarantees\nStability considerations\n\nArchitecture:\n\nVC dimension bounds\nCapacity control\nExpressivity analysis\n\nOptimization:\n\nGeneralization bounds\nStability analysis\nConvergence rates\n\n\n\n\n\n\n\nTheory:\n\n“A Theory of the Learnable” by Valiant\n“Foundations of Machine Learning” by Mohri et al.\n“Understanding Machine Learning” by Shalev-Shwartz and Ben-David\n\nAdvanced Topics:\n\n“Statistical Learning Theory” by Vapnik\n“The Nature of Statistical Learning Theory” by Vapnik\n“Learning from Data” by Abu-Mostafa et al.\n\nApplications:\n\n“Neural Network Learning” by Anthony and Bartlett\n“Kernel Methods in Machine Learning” by Hofmann et al.\n“Theory of Classification” by Devroye et al."
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#pac-learning-framework",
    "href": "posts/pac-learning-theory/index.html#pac-learning-framework",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "PAC learning definition: - Hypothesis class \\(\\mathcal{H}\\) - Instance space \\(\\mathcal{X}\\) - Target concept \\(c: \\mathcal{X} \\to \\{0,1\\}\\) - Distribution \\(\\mathcal{D}\\) over \\(\\mathcal{X}\\)\nPAC requirements:\n\\[\nP_{S \\sim \\mathcal{D}^m}(\\text{error}_\\mathcal{D}(h_S) \\leq \\epsilon) \\geq 1-\\delta\n\\]\nWhere: - \\(\\epsilon\\) is accuracy parameter - \\(\\delta\\) is confidence parameter - \\(m\\) is sample size - \\(h_S\\) is learned hypothesis\n\n\n\nFundamental bound:\n\\[\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{1}{\\delta}\\right)\n\\]\nRealizable case:\n\\[\nm \\geq \\frac{1}{\\epsilon}\\left(\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nError bound:\n\\[\n\\text{error}_\\mathcal{D}(h) \\leq \\min_{h' \\in \\mathcal{H}}\\text{error}_\\mathcal{D}(h') + \\epsilon\n\\]\nSample complexity:\n\\[\nm \\geq \\frac{2}{\\epsilon^2}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{2}{\\delta}\\right)\n\\]"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#vc-theory",
    "href": "posts/pac-learning-theory/index.html#vc-theory",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "Growth function:\n\\[\n\\Pi_\\mathcal{H}(m) = \\max_{x_1,...,x_m \\in \\mathcal{X}}|\\{(h(x_1),...,h(x_m)): h \\in \\mathcal{H}\\}|\n\\]\nSauer’s Lemma:\n\\[\n\\text{If VC}(\\mathcal{H}) = d, \\text{ then } \\Pi_\\mathcal{H}(m) \\leq \\sum_{i=0}^d \\binom{m}{i}\n\\]\n\n\n\nFundamental theorem:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| &gt; \\epsilon) \\leq 4\\Pi_\\mathcal{H}(2m)\\exp(-\\frac{m\\epsilon^2}{8})\n\\]\nSample complexity:\n\\[\nm = O\\left(\\frac{d}{\\epsilon^2}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon^2}\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nDefinition:\n\\[\n\\mathfrak{R}_S(\\mathcal{H}) = \\mathbb{E}_\\sigma\\left[\\sup_{h \\in \\mathcal{H}}\\frac{1}{m}\\sum_{i=1}^m \\sigma_i h(x_i)\\right]\n\\]\nGeneralization bound:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| \\leq 2\\mathfrak{R}_m(\\mathcal{H}) + \\sqrt{\\frac{2\\ln(2/\\delta)}{m}}) \\geq 1-\\delta\n\\]"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#advanced-pac-concepts",
    "href": "posts/pac-learning-theory/index.html#advanced-pac-concepts",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "Compression scheme:\n\\[\n\\kappa: \\cup_{m=1}^\\infty (\\mathcal{X} \\times \\{0,1\\})^m \\to \\cup_{i=1}^k (\\mathcal{X} \\times \\{0,1\\})^i\n\\]\nBound:\n\\[\nm \\geq O\\left(\\frac{k}{\\epsilon}\\log\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nWeak learning condition:\n\\[\nP(\\text{error}_\\mathcal{D}(h) \\leq \\frac{1}{2} - \\gamma) \\geq 1-\\delta\n\\]\nStrong learning guarantee:\n\\[\nP(\\text{error}_\\mathcal{D}(H) \\leq \\epsilon) \\geq 1-\\delta\n\\]\n\n\n\nMistake bound:\n\\[\nM \\leq O\\left(\\frac{d}{\\gamma^2}\\log\\frac{1}{\\gamma}\\right)\n\\]\nWhere: - \\(M\\) is number of mistakes - \\(d\\) is VC dimension - \\(\\gamma\\) is margin"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#learnability-analysis",
    "href": "posts/pac-learning-theory/index.html#learnability-analysis",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "Empirical Risk Minimization (ERM):\n\\[\nh_S = \\arg\\min_{h \\in \\mathcal{H}}\\widehat{\\text{error}}_S(h)\n\\]\nConsistency condition:\n\\[\n\\lim_{m \\to \\infty}P(\\text{error}_\\mathcal{D}(h_S) &gt; \\inf_{h \\in \\mathcal{H}}\\text{error}_\\mathcal{D}(h) + \\epsilon) = 0\n\\]\n\n\n\nDouble sampling:\n\\[\nP_{S,S'}(|\\widehat{\\text{error}}_S(h) - \\widehat{\\text{error}}_{S'}(h)| &gt; \\epsilon) \\leq \\delta\n\\]\nSymmetrization bound:\n\\[\nP(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| &gt; 2\\epsilon) \\leq 2P(\\sup_{h \\in \\mathcal{H}}|\\widehat{\\text{error}}_S(h) - \\widehat{\\text{error}}_{S'}(h)| &gt; \\epsilon)\n\\]\n\n\n\nNested hypothesis classes:\n\\[\n\\mathcal{H}_1 \\subset \\mathcal{H}_2 \\subset ... \\subset \\mathcal{H}_k\n\\]\nPenalty term:\n\\[\n\\text{pen}(h) = \\sqrt{\\frac{\\text{VC}(\\mathcal{H}(h))\\ln(em/\\text{VC}(\\mathcal{H}(h))) + \\ln(1/\\delta)}{m}}\n\\]"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#advanced-bounds",
    "href": "posts/pac-learning-theory/index.html#advanced-bounds",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "Local complexity:\n\\[\n\\mathfrak{R}_S(\\mathcal{H}, r) = \\mathbb{E}_\\sigma\\left[\\sup_{h \\in \\mathcal{H}: \\widehat{\\text{error}}_S(h) \\leq r}\\frac{1}{m}\\sum_{i=1}^m \\sigma_i h(x_i)\\right]\n\\]\nFixed point equation:\n\\[\nr^* = \\inf\\{r &gt; 0: \\mathfrak{R}_S(\\mathcal{H}, r) \\leq r/4\\}\n\\]\n\n\n\nMargin loss:\n\\[\n\\ell_\\gamma(yf(x)) = \\begin{cases}\n1 & \\text{if } yf(x) \\leq 0 \\\\\n1-\\frac{yf(x)}{\\gamma} & \\text{if } 0 &lt; yf(x) \\leq \\gamma \\\\\n0 & \\text{if } yf(x) &gt; \\gamma\n\\end{cases}\n\\]\nMargin bound:\n\\[\nP(\\text{error}_\\mathcal{D}(h) \\leq \\widehat{\\text{error}}_\\gamma(h) + O(\\sqrt{\\frac{d\\ln(1/\\gamma)}{m\\gamma^2}})) \\geq 1-\\delta\n\\]\n\n\n\nUniform stability:\n\\[\n\\sup_{S,z,i}|\\ell(A_S,z) - \\ell(A_{S^i},z)| \\leq \\beta\n\\]\nGeneralization bound:\n\\[\nP(|\\text{error}_\\mathcal{D}(A_S) - \\widehat{\\text{error}}_S(A_S)| \\leq \\epsilon) \\geq 1-2\\exp(-\\frac{m\\epsilon^2}{2\\beta^2})\n\\]"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#applications",
    "href": "posts/pac-learning-theory/index.html#applications",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "VC dimension: - Hyperplanes in \\(\\mathbb{R}^d\\): \\(d+1\\) - Homogeneous hyperplanes: \\(d\\)\nSample complexity:\n\\[\nm = O\\left(\\frac{d}{\\epsilon}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon}\\ln\\frac{1}{\\delta}\\right)\n\\]\n\n\n\nVC dimension bound:\n\\[\n\\text{VC}(\\text{NN}) \\leq O(WL\\log W)\n\\]\nWhere: - \\(W\\) is number of weights - \\(L\\) is number of layers\n\n\n\nEffective dimension:\n\\[\nd_\\text{eff}(\\lambda) = \\text{tr}(K(K+\\lambda mI)^{-1})\n\\]\nSample complexity:\n\\[\nm = O\\left(\\frac{d_\\text{eff}(\\lambda)}{\\epsilon^2} + \\frac{\\log(1/\\delta)}{\\epsilon^2}\\right)\n\\]"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#best-practices",
    "href": "posts/pac-learning-theory/index.html#best-practices",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "Complexity Control:\n\nVC dimension analysis\nRademacher complexity\nStability measures\n\nRegularization:\n\nTheoretical guarantees\nSample complexity\nGeneralization bounds\n\nValidation:\n\nPAC bounds\nCross-validation theory\nHold-out guarantees\n\n\n\n\n\n\nLearning Rate:\n\nSample complexity analysis\nConvergence guarantees\nStability considerations\n\nArchitecture:\n\nVC dimension bounds\nCapacity control\nExpressivity analysis\n\nOptimization:\n\nGeneralization bounds\nStability analysis\nConvergence rates"
  },
  {
    "objectID": "posts/pac-learning-theory/index.html#references",
    "href": "posts/pac-learning-theory/index.html#references",
    "title": "PAC Learning Theory and VC Dimension",
    "section": "",
    "text": "Theory:\n\n“A Theory of the Learnable” by Valiant\n“Foundations of Machine Learning” by Mohri et al.\n“Understanding Machine Learning” by Shalev-Shwartz and Ben-David\n\nAdvanced Topics:\n\n“Statistical Learning Theory” by Vapnik\n“The Nature of Statistical Learning Theory” by Vapnik\n“Learning from Data” by Abu-Mostafa et al.\n\nApplications:\n\n“Neural Network Learning” by Anthony and Bartlett\n“Kernel Methods in Machine Learning” by Hofmann et al.\n“Theory of Classification” by Devroye et al."
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html",
    "href": "posts/reinforcement-learning-basics/index.html",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Have you ever wondered how animals learn through trial and error? Or how a child learns to ride a bicycle? These are perfect examples of reinforcement learning in nature. Let’s explore this fascinating field of machine learning in a way that’s easy to understand.\n\n\nImagine teaching a dog new tricks. You: 1. Give treats when the dog performs correctly (reward) 2. Don’t give treats when it performs incorrectly (no reward) 3. The dog learns to associate actions with rewards\nThis is exactly how reinforcement learning works! It’s about: - Learning what to do (actions) - How to map situations to actions (strategy) - Maximizing a numerical reward signal\n\n\n\n\n\n\nThis is our learner (like the dog in our example)\nMakes decisions and performs actions\nLearns from experience\nTries to maximize rewards\n\n\n\n\n\nEverything the agent interacts with\nCould be real (physical world) or virtual (computer game)\nProvides feedback to the agent\nChanges in response to agent’s actions\n\n\n\n\nThink of states as “situations” the agent can be in: - Current position in a game - Current balance in a trading account - Current position of a robot\n\n\n\nThese are the choices available to the agent: - Move left/right in a game - Buy/sell in trading - Turn/move forward for a robot\n\n\n\n\nImmediate feedback for actions\nCan be positive or negative\nGuides the learning process\nExamples:\n\nPoints in a game\nProfit/loss in trading\nSuccessfully completing a task\n\n\n\n\n\n\n\n\nImagine teaching an AI to play chess: - States: Position of pieces on the board - Actions: Possible moves - Rewards: * +1 for winning * -1 for losing * 0 for drawing * Small rewards for capturing pieces\n\n\n\nTeaching a robot to navigate a room: - States: Current location, obstacle positions - Actions: Move forward, turn left/right - Rewards: * +1 for reaching goal * -1 for hitting obstacles * Small negative reward for time taken\n\n\n\nOptimizing home energy usage: - States: Time of day, temperature, occupancy - Actions: Adjust heating/cooling - Rewards: * Positive for energy savings * Negative for user discomfort\n\n\n\n\n\n\nThis is like trying a new restaurant vs. going to your favorite one:\nExploration: - Trying new actions - Gathering information - Risk of lower rewards - Necessary for learning\nExploitation: - Using known good actions - Maximizing immediate rewards - Safe but might miss better options\n\n\n\n\nObservation:\n\nAgent observes current state\nLike a chess player analyzing the board\n\nAction Selection:\n\nChoose action based on strategy\nBalance between exploration and exploitation\n\nFeedback:\n\nEnvironment provides reward\nState changes based on action\n\nLearning:\n\nUpdate knowledge based on experience\nImprove future decision making\n\n\n\n\n\nA policy is like a strategy or rulebook: - Maps states to actions - Improves with experience - Aims to maximize long-term rewards\n\n\n\n\n\n\n\nActions might have long-term consequences\nHard to attribute success to specific actions\nExample: Chess moves leading to victory\n\n\n\n\n\nWhich actions were actually responsible for the reward?\nLike trying to identify which study habits led to good grades\n\n\n\n\n\nToo much exploration: slow learning\nToo much exploitation: missed opportunities\nFinding the right balance is crucial\n\n\n\n\n\n\n\n\nLearning new skills\nDeveloping good habits\nCareer progression\n\n\n\n\n\nMarket expansion\nProduct development\nCustomer engagement\n\n\n\n\n\nAdaptive learning systems\nPersonalized curriculum\nStudent engagement\n\n\n\n\n\n\n\n\nStart with simple concepts\nUse analogies from daily life\nFocus on principles before implementation\n\n\n\n\n\nStart with basic scenarios\nClear states and actions\nImmediate rewards\n\n\n\n\n\nMove to more complex problems\nIntroduce delayed rewards\nHandle larger state spaces\n\n\n\n\n\n\nStart Simple\n\nBegin with basic concepts\nUse clear examples\nBuild foundational understanding\n\nUse Analogies\n\nConnect to familiar experiences\nMake abstract concepts concrete\nUse real-world examples\n\nPractice Application\n\nThink about RL in daily life\nIdentify states, actions, rewards\nConsider strategies and policies\n\n\n\n\n\n\nIt’s All About Programming\n\nUnderstanding concepts is more important\nProgramming is just a tool\nFocus on principles first\n\nIt’s Only for Complex Problems\n\nCan be applied to simple tasks\nUseful for everyday decisions\nScalable to various situations\n\nIt Requires Advanced Math\n\nBasic concepts are intuitive\nMath helps but isn’t essential for understanding\nStart with conceptual understanding\n\n\n\n\n\nReinforcement learning is about: 1. Learning from experience 2. Making better decisions 3. Adapting to new situations 4. Maximizing long-term rewards\nRemember: - Start with understanding - Use simple examples - Apply concepts to real life - Build gradually\n\n\n\n\nBooks for Beginners:\n\n“Reinforcement Learning: An Introduction” by Sutton & Barto (first few chapters)\n“Grokking Deep Reinforcement Learning” by Miguel Morales\n\nOnline Resources:\n\nDeepMind’s RL Introduction\nOpenAI’s RL Resources\nStanford’s RL Course (basic concepts)\n\n\nRemember: The key to understanding reinforcement learning is to start with the fundamentals and gradually build up to more complex concepts. Focus on understanding the principles before diving into implementation details."
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#what-is-reinforcement-learning",
    "href": "posts/reinforcement-learning-basics/index.html#what-is-reinforcement-learning",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Imagine teaching a dog new tricks. You: 1. Give treats when the dog performs correctly (reward) 2. Don’t give treats when it performs incorrectly (no reward) 3. The dog learns to associate actions with rewards\nThis is exactly how reinforcement learning works! It’s about: - Learning what to do (actions) - How to map situations to actions (strategy) - Maximizing a numerical reward signal"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#key-components-of-reinforcement-learning",
    "href": "posts/reinforcement-learning-basics/index.html#key-components-of-reinforcement-learning",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "This is our learner (like the dog in our example)\nMakes decisions and performs actions\nLearns from experience\nTries to maximize rewards\n\n\n\n\n\nEverything the agent interacts with\nCould be real (physical world) or virtual (computer game)\nProvides feedback to the agent\nChanges in response to agent’s actions\n\n\n\n\nThink of states as “situations” the agent can be in: - Current position in a game - Current balance in a trading account - Current position of a robot\n\n\n\nThese are the choices available to the agent: - Move left/right in a game - Buy/sell in trading - Turn/move forward for a robot\n\n\n\n\nImmediate feedback for actions\nCan be positive or negative\nGuides the learning process\nExamples:\n\nPoints in a game\nProfit/loss in trading\nSuccessfully completing a task"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#real-world-examples",
    "href": "posts/reinforcement-learning-basics/index.html#real-world-examples",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Imagine teaching an AI to play chess: - States: Position of pieces on the board - Actions: Possible moves - Rewards: * +1 for winning * -1 for losing * 0 for drawing * Small rewards for capturing pieces\n\n\n\nTeaching a robot to navigate a room: - States: Current location, obstacle positions - Actions: Move forward, turn left/right - Rewards: * +1 for reaching goal * -1 for hitting obstacles * Small negative reward for time taken\n\n\n\nOptimizing home energy usage: - States: Time of day, temperature, occupancy - Actions: Adjust heating/cooling - Rewards: * Positive for energy savings * Negative for user discomfort"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#core-concepts",
    "href": "posts/reinforcement-learning-basics/index.html#core-concepts",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "This is like trying a new restaurant vs. going to your favorite one:\nExploration: - Trying new actions - Gathering information - Risk of lower rewards - Necessary for learning\nExploitation: - Using known good actions - Maximizing immediate rewards - Safe but might miss better options\n\n\n\n\nObservation:\n\nAgent observes current state\nLike a chess player analyzing the board\n\nAction Selection:\n\nChoose action based on strategy\nBalance between exploration and exploitation\n\nFeedback:\n\nEnvironment provides reward\nState changes based on action\n\nLearning:\n\nUpdate knowledge based on experience\nImprove future decision making\n\n\n\n\n\nA policy is like a strategy or rulebook: - Maps states to actions - Improves with experience - Aims to maximize long-term rewards"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#common-challenges",
    "href": "posts/reinforcement-learning-basics/index.html#common-challenges",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Actions might have long-term consequences\nHard to attribute success to specific actions\nExample: Chess moves leading to victory\n\n\n\n\n\nWhich actions were actually responsible for the reward?\nLike trying to identify which study habits led to good grades\n\n\n\n\n\nToo much exploration: slow learning\nToo much exploitation: missed opportunities\nFinding the right balance is crucial"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#applications-in-everyday-life",
    "href": "posts/reinforcement-learning-basics/index.html#applications-in-everyday-life",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Learning new skills\nDeveloping good habits\nCareer progression\n\n\n\n\n\nMarket expansion\nProduct development\nCustomer engagement\n\n\n\n\n\nAdaptive learning systems\nPersonalized curriculum\nStudent engagement"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#getting-started-with-reinforcement-learning",
    "href": "posts/reinforcement-learning-basics/index.html#getting-started-with-reinforcement-learning",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Start with simple concepts\nUse analogies from daily life\nFocus on principles before implementation\n\n\n\n\n\nStart with basic scenarios\nClear states and actions\nImmediate rewards\n\n\n\n\n\nMove to more complex problems\nIntroduce delayed rewards\nHandle larger state spaces"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#best-practices-for-learning",
    "href": "posts/reinforcement-learning-basics/index.html#best-practices-for-learning",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Start Simple\n\nBegin with basic concepts\nUse clear examples\nBuild foundational understanding\n\nUse Analogies\n\nConnect to familiar experiences\nMake abstract concepts concrete\nUse real-world examples\n\nPractice Application\n\nThink about RL in daily life\nIdentify states, actions, rewards\nConsider strategies and policies"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#common-misconceptions",
    "href": "posts/reinforcement-learning-basics/index.html#common-misconceptions",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "It’s All About Programming\n\nUnderstanding concepts is more important\nProgramming is just a tool\nFocus on principles first\n\nIt’s Only for Complex Problems\n\nCan be applied to simple tasks\nUseful for everyday decisions\nScalable to various situations\n\nIt Requires Advanced Math\n\nBasic concepts are intuitive\nMath helps but isn’t essential for understanding\nStart with conceptual understanding"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#conclusion",
    "href": "posts/reinforcement-learning-basics/index.html#conclusion",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Reinforcement learning is about: 1. Learning from experience 2. Making better decisions 3. Adapting to new situations 4. Maximizing long-term rewards\nRemember: - Start with understanding - Use simple examples - Apply concepts to real life - Build gradually"
  },
  {
    "objectID": "posts/reinforcement-learning-basics/index.html#additional-resources",
    "href": "posts/reinforcement-learning-basics/index.html#additional-resources",
    "title": "Understanding Reinforcement Learning: A Beginner’s Guide",
    "section": "",
    "text": "Books for Beginners:\n\n“Reinforcement Learning: An Introduction” by Sutton & Barto (first few chapters)\n“Grokking Deep Reinforcement Learning” by Miguel Morales\n\nOnline Resources:\n\nDeepMind’s RL Introduction\nOpenAI’s RL Resources\nStanford’s RL Course (basic concepts)\n\n\nRemember: The key to understanding reinforcement learning is to start with the fundamentals and gradually build up to more complex concepts. Focus on understanding the principles before diving into implementation details."
  }
]