{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Natural Language Processing with Machine Learning\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [machine-learning, nlp, deep-learning, transformers]\n",
        "image: \"nlp_ml.jpg\"\n",
        "description: \"A comprehensive guide to Natural Language Processing with machine learning, covering fundamental concepts, modern architectures, and practical implementations.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Natural Language Processing with Machine Learning\n",
        "\n",
        "Natural Language Processing (NLP) has seen remarkable advances through machine learning, particularly with the advent of transformer models. This post explores key concepts and implementations in NLP.\n",
        "\n",
        "## Setup and Prerequisites\n"
      ],
      "id": "303a3a1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Plotting settings\n",
        "plt.style.use('seaborn')\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "id": "520592e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Text Processing Fundamentals\n",
        "\n",
        "### Text Preprocessing Pipeline\n"
      ],
      "id": "e8dee05a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, min_freq: int = 2, max_vocab_size: int = 10000):\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.vocab = None\n",
        "        self.word2idx = None\n",
        "        self.idx2word = None\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text.\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text into words.\"\"\"\n",
        "        return word_tokenize(text)\n",
        "    \n",
        "    def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
        "        \"\"\"Remove stop words from tokens.\"\"\"\n",
        "        return [token for token in tokens if token not in self.stop_words]\n",
        "    \n",
        "    def build_vocab(self, texts: List[str]) -> None:\n",
        "        \"\"\"Build vocabulary from texts.\"\"\"\n",
        "        # Count word frequencies\n",
        "        word_freq = Counter()\n",
        "        for text in texts:\n",
        "            text = self.clean_text(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            tokens = self.remove_stopwords(tokens)\n",
        "            word_freq.update(tokens)\n",
        "        \n",
        "        # Filter by frequency and vocab size\n",
        "        vocab = [word for word, freq in word_freq.most_common()\n",
        "                if freq >= self.min_freq]\n",
        "        vocab = vocab[:self.max_vocab_size-2]  # Leave room for <PAD> and <UNK>\n",
        "        \n",
        "        # Create vocabulary mappings\n",
        "        self.vocab = ['<PAD>', '<UNK>'] + vocab\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "    \n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert text to sequence of indices.\"\"\"\n",
        "        text = self.clean_text(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "        return [self.word2idx.get(token, self.word2idx['<UNK>'])\n",
        "                for token in tokens]\n",
        "    \n",
        "    def decode(self, indices: List[int]) -> str:\n",
        "        \"\"\"Convert sequence of indices back to text.\"\"\"\n",
        "        return ' '.join([self.idx2word[idx] for idx in indices\n",
        "                        if idx != self.word2idx['<PAD>']])\n",
        "\n",
        "# Example usage\n",
        "sample_texts = [\n",
        "    \"Natural language processing is fascinating!\",\n",
        "    \"Machine learning transforms text understanding.\",\n",
        "    \"Deep neural networks process language effectively.\"\n",
        "]\n",
        "\n",
        "preprocessor = TextPreprocessor()\n",
        "preprocessor.build_vocab(sample_texts)\n",
        "\n",
        "# Demonstrate preprocessing\n",
        "print(\"Vocabulary size:\", len(preprocessor.vocab))\n",
        "print(\"\\nSample encoding:\")\n",
        "for text in sample_texts:\n",
        "    encoded = preprocessor.encode(text)\n",
        "    decoded = preprocessor.decode(encoded)\n",
        "    print(f\"\\nOriginal: {text}\")\n",
        "    print(f\"Encoded: {encoded}\")\n",
        "    print(f\"Decoded: {decoded}\")"
      ],
      "id": "af6934ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Word Embeddings\n",
        "\n",
        "### Word2Vec Implementation\n"
      ],
      "id": "3af883c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embeds = self.embeddings(x)\n",
        "        return self.output(embeds)\n",
        "\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], preprocessor: TextPreprocessor,\n",
        "                 window_size: int = 2):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.window_size = window_size\n",
        "        self.data = self._prepare_data(texts)\n",
        "    \n",
        "    def _prepare_data(self, texts: List[str]) -> List[Tuple[int, int]]:\n",
        "        pairs = []\n",
        "        for text in texts:\n",
        "            indices = self.preprocessor.encode(text)\n",
        "            \n",
        "            # Generate context pairs\n",
        "            for i in range(len(indices)):\n",
        "                for j in range(max(0, i - self.window_size),\n",
        "                             min(len(indices), i + self.window_size + 1)):\n",
        "                    if i != j:\n",
        "                        pairs.append((indices[i], indices[j]))\n",
        "        \n",
        "        return pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "# Train Word2Vec model\n",
        "def train_word2vec(texts: List[str], preprocessor: TextPreprocessor,\n",
        "                   embedding_dim: int = 50, epochs: int = 5):\n",
        "    # Create dataset and dataloader\n",
        "    dataset = Word2VecDataset(texts, preprocessor)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SkipGramModel(len(preprocessor.vocab), embedding_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training loop\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for target, context in dataloader:\n",
        "            target = target.to(device)\n",
        "            context = context.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            output = model(target)\n",
        "            loss = criterion(output, context)\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    return model, losses\n",
        "\n",
        "# Train model\n",
        "model, losses = train_word2vec(sample_texts, preprocessor)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('Word2Vec Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Visualize embeddings\n",
        "def plot_embeddings(model: SkipGramModel, preprocessor: TextPreprocessor,\n",
        "                   num_words: int = 20):\n",
        "    \"\"\"Plot word embeddings using PCA.\"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "    \n",
        "    # Get embeddings\n",
        "    embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
        "    words = list(preprocessor.word2idx.keys())[:num_words]\n",
        "    \n",
        "    # Reduce dimensionality\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_2d = pca.fit_transform(embeddings[:num_words])\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "    \n",
        "    plt.title('Word Embeddings Visualization (PCA)')\n",
        "    plt.show()\n",
        "\n",
        "plot_embeddings(model, preprocessor)"
      ],
      "id": "67807cf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sequence Models\n",
        "\n",
        "### LSTM for Text Classification\n"
      ],
      "id": "d835fc43"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int,\n",
        "                 num_layers: int, num_classes: int, dropout: float = 0.5):\n",
        "        super(TextLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    \n",
        "    def forward(self, x, lengths):\n",
        "        # Embed the input\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        # Pack padded sequence\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        output, (hidden, cell) = self.lstm(packed)\n",
        "        \n",
        "        # Use last hidden state\n",
        "        out = self.dropout(hidden[-1])\n",
        "        return self.fc(out)\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], labels: List[int],\n",
        "                 preprocessor: TextPreprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.encoded_texts = [preprocessor.encode(text) for text in texts]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.tensor(self.encoded_texts[idx]),\n",
        "                torch.tensor(self.labels[idx]))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for padding sequences.\"\"\"\n",
        "    texts, labels = zip(*batch)\n",
        "    lengths = torch.tensor([len(text) for text in texts])\n",
        "    \n",
        "    # Pad sequences\n",
        "    padded_texts = nn.utils.rnn.pad_sequence(\n",
        "        texts, batch_first=True, padding_value=0\n",
        "    )\n",
        "    \n",
        "    return padded_texts, torch.tensor(labels), lengths\n",
        "\n",
        "# Example usage\n",
        "# Create synthetic dataset\n",
        "texts = [\n",
        "    \"This movie is great!\",\n",
        "    \"Terrible waste of time.\",\n",
        "    \"I loved this film.\",\n",
        "    \"Do not recommend.\",\n",
        "    \"Amazing performance!\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Prepare data\n",
        "preprocessor.build_vocab(texts)\n",
        "dataset = TextClassificationDataset(texts, labels, preprocessor)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True,\n",
        "                       collate_fn=collate_fn)\n",
        "\n",
        "# Initialize model\n",
        "model = TextLSTM(\n",
        "    vocab_size=len(preprocessor.vocab),\n",
        "    embedding_dim=50,\n",
        "    hidden_dim=64,\n",
        "    num_layers=2,\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "# Training loop\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for texts, labels, lengths in dataloader:\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(texts, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "id": "783dcc3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformer Architecture\n",
        "\n",
        "### Simple Transformer Implementation\n"
      ],
      "id": "bf477cca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attention, V)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        \n",
        "        # Linear projections and split into heads\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        \n",
        "        # Transpose for attention computation\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Apply attention\n",
        "        scores = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # Concatenate heads and apply final linear layer\n",
        "        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_o(scores)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attention))\n",
        "        \n",
        "        # Feed forward\n",
        "        ff = self.feed_forward(x)\n",
        "        return self.norm2(x + self.dropout(ff))\n",
        "\n",
        "# Example usage\n",
        "seq_length = 10\n",
        "batch_size = 4\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "d_ff = 256\n",
        "\n",
        "# Create random input\n",
        "x = torch.randn(batch_size, seq_length, d_model).to(device)\n",
        "\n",
        "# Initialize transformer block\n",
        "transformer = TransformerBlock(d_model, num_heads, d_ff).to(device)\n",
        "\n",
        "# Forward pass\n",
        "output = transformer(x)\n",
        "print(\"Transformer output shape:\", output.shape)\n",
        "\n",
        "# Visualize attention patterns\n",
        "def visualize_attention(model, input_seq):\n",
        "    \"\"\"Visualize attention patterns in the transformer.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Get attention scores\n",
        "        Q = model.attention.W_q(input_seq)\n",
        "        K = model.attention.W_k(input_seq)\n",
        "        \n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(model.attention.d_k)\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Plot attention matrix\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attention[0].cpu().numpy(), cmap='viridis')\n",
        "        plt.title('Attention Pattern')\n",
        "        plt.xlabel('Key Position')\n",
        "        plt.ylabel('Query Position')\n",
        "        plt.show()\n",
        "\n",
        "visualize_attention(transformer, x)"
      ],
      "id": "4b5f11a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices in NLP\n",
        "\n",
        "1. **Text Preprocessing**\n",
        "   - Proper tokenization\n",
        "   - Handle special characters\n",
        "   - Consider domain-specific preprocessing\n",
        "   - Implement proper cleaning\n",
        "\n",
        "2. **Model Architecture**\n",
        "   - Choose appropriate embeddings\n",
        "   - Consider sequence length\n",
        "   - Handle out-of-vocabulary words\n",
        "   - Implement proper regularization\n",
        "\n",
        "3. **Training Strategy**\n",
        "   - Proper batch size selection\n",
        "   - Learning rate scheduling\n",
        "   - Handle class imbalance\n",
        "   - Implement early stopping\n",
        "\n",
        "4. **Deployment Considerations**\n",
        "   - Model optimization\n",
        "   - Inference speed\n",
        "   - Memory constraints\n",
        "   - Batch processing\n",
        "\n",
        "## Common Challenges and Solutions\n",
        "\n",
        "1. **Data Quality**\n",
        "   - Noisy text\n",
        "   - Multiple languages\n",
        "   - Domain-specific terms\n",
        "   - Informal language\n",
        "\n",
        "2. **Model Performance**\n",
        "   - Long sequences\n",
        "   - Memory constraints\n",
        "   - Training time\n",
        "   - Model complexity\n",
        "\n",
        "3. **Deployment Issues**\n",
        "   - Model size\n",
        "   - Inference latency\n",
        "   - Scalability\n",
        "   - Resource constraints\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "NLP with ML requires:\n",
        "\n",
        "1. Strong preprocessing pipeline\n",
        "2. Appropriate model selection\n",
        "3. Effective training strategies\n",
        "4. Consideration of deployment constraints\n",
        "\n",
        "In the next post, we'll explore reinforcement learning concepts and applications.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "1. Books:\n",
        "   - \"Natural Language Processing with Transformers\" by Lewis Tunstall et al.\n",
        "   - \"Speech and Language Processing\" by Jurafsky and Martin\n",
        "   - \"Natural Language Processing with PyTorch\" by Rao and McMahan\n",
        "\n",
        "2. Online Resources:\n",
        "   - Hugging Face Documentation\n",
        "   - Stanford CS224N Course\n",
        "   - Fast.ai NLP Course\n",
        "\n",
        "Remember: NLP is a rapidly evolving field with new architectures and techniques emerging regularly. Stay updated with the latest research while maintaining a strong understanding of the fundamentals."
      ],
      "id": "3d804e1a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}