---
title: "Getting Started with PyTorch"
author: "Ram Polisetti"
date: "2024-03-19"
categories: [deep-learning, python, tutorial, machine-learning, data-science]
image: "pytorch_logo.png"
description: "An introduction to deep learning with PyTorch, covering basic concepts and building your first neural network."
---

# Getting Started with Deep Learning using PyTorch

In this post, we'll dive into the basics of deep learning using PyTorch. We'll build a simple neural network and understand the fundamental concepts of deep learning.

## Setup

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Set plot style
plt.style.use('default')
sns.set_theme()

# Generate synthetic data
n_samples = 1000
X = np.random.uniform(-5, 5, (n_samples, 1))
y = 2 * np.sin(X) + np.random.normal(0, 0.2, (n_samples, 1))

# Convert to PyTorch tensors
X_tensor = torch.FloatTensor(X)
y_tensor = torch.FloatTensor(y)
```

## Building the Neural Network

```{python}
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(1, 64)
        self.layer2 = nn.Linear(64, 64)
        self.layer3 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# Create the model
model = SimpleNN()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
```

## Training the Model

```{python}
# Training loop
epochs = 100
losses = []

for epoch in range(epochs):
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    losses.append(loss.item())
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Plot training loss
plt.figure(figsize=(10, 5))
plt.plot(losses)
plt.title('Training Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.yscale('log')
plt.show()
```

## Visualizing the Results

```{python}
# Generate predictions
model.eval()
with torch.no_grad():
    X_test = torch.FloatTensor(np.linspace(-6, 6, 200).reshape(-1, 1))
    y_pred = model(X_test)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.5, label='Data')
plt.plot(X_test, y_pred, 'r', label='Model Prediction')
plt.plot(X_test, 2 * np.sin(X_test), 'g--', label='True Function')
plt.legend()
plt.title('Neural Network Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.show()
```

## Understanding the Model

Let's analyze what our neural network has learned:

1. The network architecture consists of:
   - Input layer (1 neuron)
   - Two hidden layers (64 neurons each)
   - Output layer (1 neuron)
   - ReLU activation functions

2. Training process:
   - Used Mean Squared Error loss
   - Adam optimizer
   - 100 epochs of training

3. Results:
   - The model successfully learned the underlying sinusoidal pattern
   - Handles noise in the data well
   - Generalizes to unseen data points

## Key Takeaways

In this tutorial, we learned:

1. How to create a neural network using PyTorch
2. Basic concepts of:
   - Network architecture
   - Forward and backward propagation
   - Loss functions and optimization
3. Visualization of training progress and results

Next Steps:
- Exploring more complex architectures
- Understanding different activation functions
- Working with real-world datasets
- Implementing different types of neural networks (CNNs, RNNs)
