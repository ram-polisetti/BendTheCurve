{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PAC Learning Theory and VC Dimension\"\n",
        "author: \"Ram Polisetti\"\n",
        "date: \"2024-03-19\"\n",
        "categories: [machine-learning, theory, mathematics, statistics]\n",
        "tags: [machine-learning-theory, computational-learning, statistical-learning, optimization]\n",
        "image: \"pac_learning.jpg\"\n",
        "description: \"A beginner-friendly guide to PAC learning theory and VC dimension with interactive visualizations and practical examples.\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# PAC Learning Theory and VC Dimension\n",
        "\n",
        ":::{.callout-note}\n",
        "## Learning Objectives\n",
        "By the end of this article, you will:\n",
        "1. Understand PAC learning intuitively and mathematically\n",
        "\n",
        "2. Visualize VC dimension in practice\n",
        "\n",
        "3. Calculate sample complexity for real problems\n",
        "\n",
        "4. Implement PAC learning algorithms\n",
        "\n",
        "5. Apply VC theory to model selection\n",
        ":::\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Imagine you're teaching a robot to recognize apples . How can you be \"probably approximately correct\" about its ability to recognize any apple? PAC learning theory gives us the mathematical framework to answer such questions.\n"
      ],
      "id": "f76b4a05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification, make_circles\n",
        "from sklearn.model_selection import learning_curve\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "id": "8ee8f6f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PAC Learning Visualization\n",
        "\n",
        "Let's visualize what \"probably approximately correct\" means:\n"
      ],
      "id": "d3cf9034"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "def visualize_pac_learning(n_samples=100, noise_level=0.1):\n",
        "    # Generate synthetic dataset\n",
        "    X, y = make_circles(n_samples=n_samples, noise=noise_level, factor=0.3)\n",
        "    \n",
        "    # Train models with different sample sizes\n",
        "    sample_sizes = [10, 30, 50, n_samples]\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, size in enumerate(sample_sizes):\n",
        "        # Train model on subset\n",
        "        model = SVC(kernel='rbf')\n",
        "        idx = np.random.choice(n_samples, size=size, replace=False)\n",
        "        model.fit(X[idx], y[idx])\n",
        "        \n",
        "        # Create grid for decision boundary\n",
        "        xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100),\n",
        "                            np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100))\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        # Plot\n",
        "        axes[i].contourf(xx, yy, Z, alpha=0.4)\n",
        "        axes[i].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
        "        axes[i].set_title(f'Training samples: {size}')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_pac_learning()"
      ],
      "id": "78b4b88e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-tip}\n",
        "## Key Insight\n",
        "Notice how the decision boundary becomes more stable and accurate as we increase the sample size. This is PAC learning in action!\n",
        ":::\n",
        "\n",
        "## 2. VC Dimension Explorer\n",
        "\n",
        "Let's create an interactive tool to explore VC dimension:\n"
      ],
      "id": "50c9ef34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "def explore_vc_dimension(n_points=100):\n",
        "    def generate_points(n):\n",
        "        return np.random.rand(n, 2)\n",
        "    \n",
        "    def plot_linear_classifier(ax, points, labels):\n",
        "        if len(points) >= 2:\n",
        "            model = SVC(kernel='linear')\n",
        "            try:\n",
        "                model.fit(points, labels)\n",
        "                \n",
        "                # Plot decision boundary\n",
        "                xx, yy = np.meshgrid(np.linspace(0, 1, 100),\n",
        "                                   np.linspace(0, 1, 100))\n",
        "                Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "                Z = Z.reshape(xx.shape)\n",
        "                ax.contourf(xx, yy, Z, alpha=0.4)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Plot points\n",
        "        colors = ['red' if l == 0 else 'blue' for l in labels]\n",
        "        ax.scatter(points[:, 0], points[:, 1], c=colors)\n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "    \n",
        "    # Generate different labelings\n",
        "    points = generate_points(3)  # Try with 3 points\n",
        "    all_labels = [[int(i) for i in format(j, f'0{3}b')] \n",
        "                 for j in range(2**3)]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, labels in enumerate(all_labels):\n",
        "        plot_linear_classifier(axes[i], points, labels)\n",
        "        axes[i].set_title(f'Labeling {i+1}')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "explore_vc_dimension()"
      ],
      "id": "ad6ad534",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-note}\n",
        "## Understanding VC Dimension\n",
        "The plots above show different possible labelings of 3 points. A linear classifier (VC dimension = 3) can shatter these points in most, but not all configurations.\n",
        ":::\n",
        "\n",
        "## Theoretical Foundations\n",
        "\n",
        "### 1. PAC Learning Framework\n",
        "\n",
        "The PAC (Probably Approximately Correct) learning framework provides theoretical guarantees for learning algorithms:\n",
        "\n",
        "$$\n",
        "P_{S \\sim \\mathcal{D}^m}(\\text{error}_\\mathcal{D}(h_S) \\leq \\epsilon) \\geq 1-\\delta\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\epsilon$ is the accuracy parameter (how close to perfect)\n",
        "\n",
        "- $\\delta$ is the confidence parameter (how sure we are)\n",
        "\n",
        "- $m$ is the sample size\n",
        "\n",
        "- $h_S$ is the learned hypothesis\n",
        "\n",
        "### 2. Sample Complexity\n",
        "\n",
        "The fundamental bound for sample complexity:\n",
        "\n",
        "$$\n",
        "m \\geq \\frac{1}{\\epsilon}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{1}{\\delta}\\right)\n",
        "$$\n",
        "\n",
        "For the realizable case (when perfect classification is possible):\n",
        "\n",
        "$$\n",
        "m \\geq \\frac{1}{\\epsilon}\\left(\\ln\\frac{1}{\\delta}\\right)\n",
        "$$\n",
        "\n",
        "### 3. VC Dimension Theory\n",
        "\n",
        "The VC dimension of a hypothesis class $\\mathcal{H}$ is the largest number of points that can be shattered (assigned any possible labeling) by $\\mathcal{H}$.\n",
        "\n",
        "Growth function:\n",
        "\n",
        "$$\n",
        "\\Pi_\\mathcal{H}(m) = \\max_{x_1,...,x_m \\in \\mathcal{X}}|\\{(h(x_1),...,h(x_m)): h \\in \\mathcal{H}\\}|\n",
        "$$\n",
        "\n",
        "Sauer's Lemma:\n",
        "\n",
        "$$\n",
        "\\text{If VC}(\\mathcal{H}) = d, \\text{ then } \\Pi_\\mathcal{H}(m) \\leq \\sum_{i=0}^d \\binom{m}{i}\n",
        "$$\n",
        "\n",
        "### 4. Generalization Bounds\n",
        "\n",
        "The fundamental theorem of learning theory:\n",
        "\n",
        "$$\n",
        "P(\\sup_{h \\in \\mathcal{H}}|\\text{error}_\\mathcal{D}(h) - \\widehat{\\text{error}}_S(h)| > \\epsilon) \\leq 4\\Pi_\\mathcal{H}(2m)\\exp(-\\frac{m\\epsilon^2}{8})\n",
        "$$\n",
        "\n",
        "Sample complexity in terms of VC dimension:\n",
        "\n",
        "$$\n",
        "m = O\\left(\\frac{d}{\\epsilon^2}\\ln\\frac{1}{\\epsilon} + \\frac{1}{\\epsilon^2}\\ln\\frac{1}{\\delta}\\right)\n",
        "$$\n",
        "\n",
        ":::{.callout-note}\n",
        "## Key Insight\n",
        "The VC dimension ($d$) appears in the sample complexity bound, showing how model complexity affects learning guarantees.\n",
        ":::\n",
        "\n",
        "## PAC Learning Framework\n",
        "\n",
        "### 1. Basic Definitions\n",
        "\n",
        "Let's make PAC learning concrete with an example:\n"
      ],
      "id": "b5fe8677"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "class PACLearner:\n",
        "    def __init__(self, epsilon=0.1, delta=0.05):\n",
        "        self.epsilon = epsilon  # accuracy parameter\n",
        "        self.delta = delta    # confidence parameter\n",
        "        self.model = None\n",
        "    \n",
        "    def required_samples(self, vc_dim):\n",
        "        \"\"\"Calculate required sample size using VC bound\"\"\"\n",
        "        return int(np.ceil((8/self.epsilon) * \n",
        "                         (2*vc_dim * np.log2(16/self.epsilon) + \n",
        "                          np.log2(2/self.delta))))\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train model with PAC guarantees\"\"\"\n",
        "        n_samples = len(X)\n",
        "        required = self.required_samples(vc_dim=3)  # for linear classifier\n",
        "        \n",
        "        if n_samples < required:\n",
        "            print(f\"Warning: Need at least {required} samples for PAC guarantees\")\n",
        "        \n",
        "        self.model = SVC(kernel='linear')\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# Example usage\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
        "                          n_informative=2, random_state=42)\n",
        "learner = PACLearner(epsilon=0.1, delta=0.05)\n",
        "print(f\"Required samples: {learner.required_samples(vc_dim=3)}\")\n",
        "learner.fit(X, y)"
      ],
      "id": "a6cae521",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        ":::{.callout-tip}\n",
        "## Practical PAC Learning\n",
        "1. Choose your desired accuracy (ε) and confidence (δ)\n",
        "2. Calculate required sample size using VC dimension\n",
        "3. Collect enough samples to meet PAC guarantees\n",
        "4. Train your model on the collected samples\n",
        ":::\n",
        "\n",
        "## Advanced Topics\n",
        "\n",
        "### 1. Rademacher Complexity\n",
        "\n",
        "Rademacher complexity measures the richness of a hypothesis class:\n",
        "\n",
        "$$\n",
        "\\mathfrak{R}_S(\\mathcal{H}) = \\mathbb{E}_\\sigma\\left[\\sup_{h \\in \\mathcal{H}}\\frac{1}{m}\\sum_{i=1}^m \\sigma_i h(x_i)\\right]\n",
        "$$\n",
        "\n",
        "### 2. Agnostic PAC Learning\n",
        "\n",
        "For the non-realizable case:\n",
        "\n",
        "$$\n",
        "\\text{error}_\\mathcal{D}(h) \\leq \\min_{h' \\in \\mathcal{H}}\\text{error}_\\mathcal{D}(h') + \\epsilon\n",
        "$$\n",
        "\n",
        "Sample complexity:\n",
        "\n",
        "$$\n",
        "m \\geq \\frac{2}{\\epsilon^2}\\left(\\ln|\\mathcal{H}| + \\ln\\frac{2}{\\delta}\\right)\n",
        "$$\n",
        "\n",
        "### 3. Structural Risk Minimization\n",
        "\n",
        "For nested hypothesis classes $\\mathcal{H}_1 \\subset \\mathcal{H}_2 \\subset ... \\subset \\mathcal{H}_k$:\n",
        "\n",
        "$$\n",
        "\\text{pen}(h) = \\sqrt{\\frac{\\text{VC}(\\mathcal{H}(h))\\ln(em/\\text{VC}(\\mathcal{H}(h))) + \\ln(1/\\delta)}{m}}\n",
        "$$\n",
        "\n",
        ":::{.callout-tip}\n",
        "## Practical Application\n",
        "Use structural risk minimization to automatically select model complexity based on your dataset size.\n",
        ":::\n",
        "\n",
        "## Practical Implementation\n",
        "\n",
        "Here's a complete example of PAC learning in practice:\n",
        "\n",
        "\n",
        "#| code-fold: false\n",
        "class MemoryEfficientLearner:\n",
        "    def __init__(self, max_memory=1000):\n",
        "        self.max_memory = max_memory\n",
        "        self.model = None\n",
        "    \n",
        "    def fit_with_memory_constraint(self, X, y):\n",
        "        n_samples = len(X)\n",
        "        batch_size = min(self.max_memory, n_samples)\n",
        "        \n",
        "        # Simulate streaming learning\n",
        "        times = []\n",
        "        memories = []\n",
        "        accuracies = []\n",
        "        \n",
        "        for batch_end in range(batch_size, n_samples + batch_size, batch_size):\n",
        "            batch_start = batch_end - batch_size\n",
        "            X_batch = X[batch_start:batch_end]\n",
        "            y_batch = y[batch_start:batch_end]\n",
        "            \n",
        "            start_time = time.time()\n",
        "            if self.model is None:\n",
        "                self.model = SVC(kernel='linear')\n",
        "            self.model.fit(X_batch, y_batch)\n",
        "            \n",
        "            times.append(time.time() - start_time)\n",
        "            memories.append(batch_size * X.shape[1] * 8)  # Approximate memory in bytes\n",
        "            accuracies.append(self.model.score(X_batch, y_batch))\n",
        "        \n",
        "        return times, memories, accuracies\n",
        "\n",
        "# Generate synthetic dataset with proper parameters\n",
        "X, y = make_classification(\n",
        "    n_samples=2000,\n",
        "    n_features=2,\n",
        "    n_informative=2,    # All features are informative\n",
        "    n_redundant=0,      # No redundant features\n",
        "    n_repeated=0,       # No repeated features\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the features for better SVM performance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Create and train the memory-efficient learner\n",
        "learner = MemoryEfficientLearner(max_memory=500)\n",
        "times, memories, accuracies = learner.fit_with_memory_constraint(X, y)\n",
        "\n",
        "# Create subplots with proper spacing\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Space-Time Tradeoff\n",
        "ax1.plot(memories, times, 'go-', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Memory usage (bytes)')\n",
        "ax1.set_ylabel('Training time (seconds)')\n",
        "ax1.set_title('Space-Time Tradeoff')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Memory vs Accuracy\n",
        "ax2.plot(memories, accuracies, 'bo-', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Memory usage (bytes)')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Memory-Accuracy Tradeoff')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Improve plot aesthetics\n",
        "plt.tight_layout()\n",
        "for ax in [ax1, ax2]:\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    \n",
        "plt.show()\n",
        "\n",
        "::: {.column-screen-inset}\n",
        "# You May Also Like {.unnumbered}\n",
        "\n",
        ":::{#related-posts}\n",
        "---"
      ],
      "id": "f3aa6baf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "listing:\n",
        "  contents: \"../**/index.qmd\"\n",
        "  type: default\n",
        "  fields: [title, description, date, author]\n",
        "  sort: \"date desc\"\n",
        "  max-items: 2\n",
        "  filter-ui: false\n",
        "  categories: false\n",
        "  include-in-header: false\n",
        "  feed: true\n",
        "  date-format: \"MMMM D, YYYY\""
      ],
      "id": "eda190f9"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.related-posts-section}\n",
        "## Continue Your Learning Journey\n",
        "\n",
        ":::{#related-posts}\n",
        "---"
      ],
      "id": "8e938082"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "listing:\n",
        "  contents: \"../**/index.qmd\"\n",
        "  type: default\n",
        "  fields: [title, description, date, author]\n",
        "  sort: \"date desc\"\n",
        "  max-items: 2\n",
        "  filter-ui: false\n",
        "  categories: false\n",
        "  include-in-header: false\n",
        "  feed: true\n",
        "  date-format: \"MMMM D, YYYY\""
      ],
      "id": "2f369c7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        ":::\n",
        ":::"
      ],
      "id": "b8f35577"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/quarto-blog/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}