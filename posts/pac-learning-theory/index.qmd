---
title: "PAC Learning Theory and VC Dimension"
author: "Ram Polisetti"
date: "2024-03-19"
categories: [machine-learning, theory, mathematics, statistics]
tags: [machine-learning-theory, computational-learning, statistical-learning, optimization]
image: "pac_learning.jpg"
description: "A beginner-friendly guide to PAC learning theory and VC dimension with interactive visualizations and practical examples."
jupyter: python3
---

# PAC Learning Theory and VC Dimension

:::{.callout-note}
## Learning Objectives
By the end of this article, you will:
1. Understand PAC learning intuitively and mathematically

2. Visualize VC dimension in practice

3. Calculate sample complexity for real problems

4. Implement PAC learning algorithms

5. Apply VC theory to model selection
:::

## Introduction

Imagine you're teaching a robot to recognize apples . How can you be "probably approximately correct" about its ability to recognize any apple? PAC learning theory gives us the mathematical framework to answer such questions.

```{python}
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.datasets import make_classification, make_circles
from sklearn.model_selection import learning_curve
import time

# Set random seed for reproducibility
np.random.seed(42)
```

## 1. PAC Learning Visualization

Let's visualize what "probably approximately correct" means:

```{python}
#| code-fold: false
def visualize_pac_learning(n_samples=100, noise_level=0.1):
    # Generate synthetic dataset
    X, y = make_circles(n_samples=n_samples, noise=noise_level, factor=0.3)
    
    # Train models with different sample sizes
    sample_sizes = [10, 30, 50, n_samples]
    fig, axes = plt.subplots(2, 2, figsize=(12, 12))
    axes = axes.ravel()
    
    for i, size in enumerate(sample_sizes):
        # Train model on subset
        model = SVC(kernel='rbf')
        idx = np.random.choice(n_samples, size=size, replace=False)
        model.fit(X[idx], y[idx])
        
        # Create grid for decision boundary
        xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100),
                            np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100))
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        # Plot
        axes[i].contourf(xx, yy, Z, alpha=0.4)
        axes[i].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
        axes[i].set_title(f'Training samples: {size}')
    
    plt.tight_layout()
    plt.show()

visualize_pac_learning()
```

:::{.callout-tip}
## Key Insight
Notice how the decision boundary becomes more stable and accurate as we increase the sample size. This is PAC learning in action!
:::

## 2. VC Dimension Explorer

Let's create an interactive tool to explore VC dimension:

```{python}
#| code-fold: false
def explore_vc_dimension(n_points=100):
    def generate_points(n):
        return np.random.rand(n, 2)
    
    def plot_linear_classifier(ax, points, labels):
        if len(points) >= 2:
            model = SVC(kernel='linear')
            try:
                model.fit(points, labels)
                
                # Plot decision boundary
                xx, yy = np.meshgrid(np.linspace(0, 1, 100),
                                   np.linspace(0, 1, 100))
                Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
                Z = Z.reshape(xx.shape)
                ax.contourf(xx, yy, Z, alpha=0.4)
            except:
                pass
        
        # Plot points
        colors = ['red' if l == 0 else 'blue' for l in labels]
        ax.scatter(points[:, 0], points[:, 1], c=colors)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
    
    # Generate different labelings
    points = generate_points(3)  # Try with 3 points
    all_labels = [[int(i) for i in format(j, f'0{3}b')] 
                 for j in range(2**3)]
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.ravel()
    
    for i, labels in enumerate(all_labels):
        plot_linear_classifier(axes[i], points, labels)
        axes[i].set_title(f'Labeling {i+1}')
    
    plt.tight_layout()
    plt.show()

explore_vc_dimension()
```

:::{.callout-note}
## Understanding VC Dimension
The plots above show different possible labelings of 3 points. A linear classifier (VC dimension = 3) can shatter these points in most, but not all configurations.
:::

## Theoretical Foundations

### 1. PAC Learning Framework

The PAC (Probably Approximately Correct) learning framework provides theoretical guarantees for learning algorithms:

$$
P_{S \sim \mathcal{D}^m}(\text{error}_\mathcal{D}(h_S) \leq \epsilon) \geq 1-\delta
$$

Where:
- $\epsilon$ is the accuracy parameter (how close to perfect)

- $\delta$ is the confidence parameter (how sure we are)

- $m$ is the sample size

- $h_S$ is the learned hypothesis

### 2. Sample Complexity

The fundamental bound for sample complexity:

$$
m \geq \frac{1}{\epsilon}\left(\ln|\mathcal{H}| + \ln\frac{1}{\delta}\right)
$$

For the realizable case (when perfect classification is possible):

$$
m \geq \frac{1}{\epsilon}\left(\ln\frac{1}{\delta}\right)
$$

### 3. VC Dimension Theory

The VC dimension of a hypothesis class $\mathcal{H}$ is the largest number of points that can be shattered (assigned any possible labeling) by $\mathcal{H}$.

Growth function:

$$
\Pi_\mathcal{H}(m) = \max_{x_1,...,x_m \in \mathcal{X}}|\{(h(x_1),...,h(x_m)): h \in \mathcal{H}\}|
$$

Sauer's Lemma:

$$
\text{If VC}(\mathcal{H}) = d, \text{ then } \Pi_\mathcal{H}(m) \leq \sum_{i=0}^d \binom{m}{i}
$$

### 4. Generalization Bounds

The fundamental theorem of learning theory:

$$
P(\sup_{h \in \mathcal{H}}|\text{error}_\mathcal{D}(h) - \widehat{\text{error}}_S(h)| > \epsilon) \leq 4\Pi_\mathcal{H}(2m)\exp(-\frac{m\epsilon^2}{8})
$$

Sample complexity in terms of VC dimension:

$$
m = O\left(\frac{d}{\epsilon^2}\ln\frac{1}{\epsilon} + \frac{1}{\epsilon^2}\ln\frac{1}{\delta}\right)
$$

:::{.callout-note}
## Key Insight
The VC dimension ($d$) appears in the sample complexity bound, showing how model complexity affects learning guarantees.
:::

## PAC Learning Framework

### 1. Basic Definitions

Let's make PAC learning concrete with an example:

```{python}
#| code-fold: false
class PACLearner:
    def __init__(self, epsilon=0.1, delta=0.05):
        self.epsilon = epsilon  # accuracy parameter
        self.delta = delta    # confidence parameter
        self.model = None
    
    def required_samples(self, vc_dim):
        """Calculate required sample size using VC bound"""
        return int(np.ceil((8/self.epsilon) * 
                         (2*vc_dim * np.log2(16/self.epsilon) + 
                          np.log2(2/self.delta))))
    
    def fit(self, X, y):
        """Train model with PAC guarantees"""
        n_samples = len(X)
        required = self.required_samples(vc_dim=3)  # for linear classifier
        
        if n_samples < required:
            print(f"Warning: Need at least {required} samples for PAC guarantees")
        
        self.model = SVC(kernel='linear')
        self.model.fit(X, y)
        return self
    
    def predict(self, X):
        return self.model.predict(X)

# Example usage
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42)
learner = PACLearner(epsilon=0.1, delta=0.05)
print(f"Required samples: {learner.required_samples(vc_dim=3)}")
learner.fit(X, y)
```

:::{.callout-tip}
## Practical PAC Learning
1. Choose your desired accuracy (ε) and confidence (δ)
2. Calculate required sample size using VC dimension
3. Collect enough samples to meet PAC guarantees
4. Train your model on the collected samples
:::

## Advanced Topics

### 1. Rademacher Complexity

Rademacher complexity measures the richness of a hypothesis class:

$$
\mathfrak{R}_S(\mathcal{H}) = \mathbb{E}_\sigma\left[\sup_{h \in \mathcal{H}}\frac{1}{m}\sum_{i=1}^m \sigma_i h(x_i)\right]
$$

### 2. Agnostic PAC Learning

For the non-realizable case:

$$
\text{error}_\mathcal{D}(h) \leq \min_{h' \in \mathcal{H}}\text{error}_\mathcal{D}(h') + \epsilon
$$

Sample complexity:

$$
m \geq \frac{2}{\epsilon^2}\left(\ln|\mathcal{H}| + \ln\frac{2}{\delta}\right)
$$

### 3. Structural Risk Minimization

For nested hypothesis classes $\mathcal{H}_1 \subset \mathcal{H}_2 \subset ... \subset \mathcal{H}_k$:

$$
\text{pen}(h) = \sqrt{\frac{\text{VC}(\mathcal{H}(h))\ln(em/\text{VC}(\mathcal{H}(h))) + \ln(1/\delta)}{m}}
$$

:::{.callout-tip}
## Practical Application
Use structural risk minimization to automatically select model complexity based on your dataset size.
:::

## Practical Implementation

Here's a complete example of PAC learning in practice:

```{python}
#| code-fold: false
class MemoryEfficientLearner:
    def __init__(self, max_memory=1000):
        self.max_memory = max_memory
        self.model = None
    
    def fit_with_memory_constraint(self, X, y):
        n_samples = len(X)
        batch_size = min(self.max_memory, n_samples)
        
        # Simulate streaming learning
        times = []
        memories = []
        accuracies = []
        
        for batch_end in range(batch_size, n_samples + batch_size, batch_size):
            batch_start = batch_end - batch_size
            X_batch = X[batch_start:batch_end]
            y_batch = y[batch_start:batch_end]
            
            start_time = time.time()
            if self.model is None:
                self.model = SVC(kernel='linear')
            self.model.fit(X_batch, y_batch)
            
            times.append(time.time() - start_time)
            memories.append(batch_size * X.shape[1] * 8)  # Approximate memory in bytes
            accuracies.append(self.model.score(X_batch, y_batch))
        
        return times, memories, accuracies

# Generate synthetic dataset with proper parameters
X, y = make_classification(
    n_samples=2000,
    n_features=2,
    n_informative=2,    # All features are informative
    n_redundant=0,      # No redundant features
    n_repeated=0,       # No repeated features
    n_classes=2,
    random_state=42
)

# Scale the features for better SVM performance
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Create and train the memory-efficient learner
learner = MemoryEfficientLearner(max_memory=500)
times, memories, accuracies = learner.fit_with_memory_constraint(X, y)

# Create subplots with proper spacing
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot 1: Space-Time Tradeoff
ax1.plot(memories, times, 'go-', linewidth=2, markersize=8)
ax1.set_xlabel('Memory usage (bytes)')
ax1.set_ylabel('Training time (seconds)')
ax1.set_title('Space-Time Tradeoff')
ax1.grid(True, alpha=0.3)

# Plot 2: Memory vs Accuracy
ax2.plot(memories, accuracies, 'bo-', linewidth=2, markersize=8)
ax2.set_xlabel('Memory usage (bytes)')
ax2.set_ylabel('Accuracy')
ax2.set_title('Memory-Accuracy Tradeoff')
ax2.grid(True, alpha=0.3)

# Improve plot aesthetics
plt.tight_layout()
for ax in [ax1, ax2]:
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
plt.show()

# Related Posts {.unnumbered}

::: {.grid}

::: {.g-col-6}
### [Computational Learning Theory](/posts/computational-learning-theory/)
Deep dive into the theoretical foundations of machine learning algorithms
:::

::: {.g-col-6}
### [Statistical Learning Theory](/posts/statistical-learning-theory/)
Understanding the statistical principles behind machine learning
:::

::: {.g-col-6}
### [Machine Learning Theory Foundations](/posts/ml-theory-foundations/)
Essential theoretical concepts for understanding machine learning
:::

::: {.g-col-6}
### [Optimization Algorithms](/posts/optimization-algorithms/)
Understanding key optimization algorithms used in machine learning
:::

:::